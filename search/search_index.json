{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Apache Kafka 2.8.0 \u00b6 Welcome to The Internals of Apache Kafka online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Kafka as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Apache Kafka \ud83d\udd25 Last update: 2021-09-16","title":"Home"},{"location":"#the-internals-of-apache-kafka-280","text":"Welcome to The Internals of Apache Kafka online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Kafka as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Apache Kafka \ud83d\udd25 Last update: 2021-09-16","title":"The Internals of Apache Kafka 2.8.0"},{"location":"AbstractConfig/","text":"AbstractConfig \u00b6 AbstractConfig is...FIXME","title":"AbstractConfig"},{"location":"AbstractConfig/#abstractconfig","text":"AbstractConfig is...FIXME","title":"AbstractConfig"},{"location":"Configurable/","text":"Configurable \u00b6 Configurable is...FIXME","title":"Configurable"},{"location":"Configurable/#configurable","text":"Configurable is...FIXME","title":"Configurable"},{"location":"InterBrokerSendThread/","text":"InterBrokerSendThread \u00b6","title":"InterBrokerSendThread"},{"location":"InterBrokerSendThread/#interbrokersendthread","text":"","title":"InterBrokerSendThread"},{"location":"Utils/","text":"Utils \u00b6 murmur2 \u00b6 int murmur2 ( byte [] data ) murmur2 generates a 32-bit murmur2 hash for the given byte array. murmur2 is used when: DefaultPartitioner is requested to compute a partition for a record Demo \u00b6 import org.apache.kafka.common.utils.Utils val keyBytes = \"hello\".getBytes val hash = Utils.murmur2(keyBytes) println(hash) toPositive \u00b6 int toPositive ( int number ) toPositive converts a number to a positive value.","title":"Utils"},{"location":"Utils/#utils","text":"","title":"Utils"},{"location":"Utils/#murmur2","text":"int murmur2 ( byte [] data ) murmur2 generates a 32-bit murmur2 hash for the given byte array. murmur2 is used when: DefaultPartitioner is requested to compute a partition for a record","title":" murmur2"},{"location":"Utils/#demo","text":"import org.apache.kafka.common.utils.Utils val keyBytes = \"hello\".getBytes val hash = Utils.murmur2(keyBytes) println(hash)","title":" Demo"},{"location":"Utils/#topositive","text":"int toPositive ( int number ) toPositive converts a number to a positive value.","title":" toPositive"},{"location":"building-from-sources/","text":"Building from Sources \u00b6 Based on README.md : KAFKA_VERSION=2.8.0 ./gradlew clean releaseTarGz install && \\ tar -zxvf core/build/distributions/kafka_2.13-$KAFKA_VERSION.tgz) cd kafka_2.13-$KAFKA_VERSION $ ./bin/kafka-server-start.sh --version [2021-09-12 19:00:12,467] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$) 2.8.0 (Commit:ebb1d6e21cc92130)","title":"Building from Sources"},{"location":"building-from-sources/#building-from-sources","text":"Based on README.md : KAFKA_VERSION=2.8.0 ./gradlew clean releaseTarGz install && \\ tar -zxvf core/build/distributions/kafka_2.13-$KAFKA_VERSION.tgz) cd kafka_2.13-$KAFKA_VERSION $ ./bin/kafka-server-start.sh --version [2021-09-12 19:00:12,467] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$) 2.8.0 (Commit:ebb1d6e21cc92130)","title":"Building from Sources"},{"location":"logging/","text":"Logging \u00b6 Brokers \u00b6 Kafka brokers use Apache Log4j 2 for logging and use config/log4j.properties by default. The default logging level is INFO with stdout appender. log4j.rootLogger=INFO, stdout, kafkaAppender log4j.logger.kafka=INFO log4j.logger.org.apache.kafka=INFO Clients \u00b6 build.sbt \u00b6 libraryDependencies += \"org.apache.kafka\" % \"kafka-clients\" % \"2.8.0\" val slf4j = \"1.7.32\" libraryDependencies += \"org.slf4j\" % \"slf4j-api\" % slf4j libraryDependencies += \"org.slf4j\" % \"slf4j-log4j12\" % slf4j log4j.properties \u00b6 In src/main/resources/log4j.properties use the following: log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL","title":"Logging"},{"location":"logging/#logging","text":"","title":"Logging"},{"location":"logging/#brokers","text":"Kafka brokers use Apache Log4j 2 for logging and use config/log4j.properties by default. The default logging level is INFO with stdout appender. log4j.rootLogger=INFO, stdout, kafkaAppender log4j.logger.kafka=INFO log4j.logger.org.apache.kafka=INFO","title":"Brokers"},{"location":"logging/#clients","text":"","title":"Clients"},{"location":"logging/#buildsbt","text":"libraryDependencies += \"org.apache.kafka\" % \"kafka-clients\" % \"2.8.0\" val slf4j = \"1.7.32\" libraryDependencies += \"org.slf4j\" % \"slf4j-api\" % slf4j libraryDependencies += \"org.slf4j\" % \"slf4j-log4j12\" % slf4j","title":"build.sbt"},{"location":"logging/#log4jproperties","text":"In src/main/resources/log4j.properties use the following: log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL","title":"log4j.properties"},{"location":"overview/","text":"Apache Kafka \u00b6 Apache Kafka is an open source project for a distributed publish-subscribe messaging system rethought as a distributed commit log . Messages \u00b6 Messages ( records , events ) are byte arrays (String, JSON, and Avro are among the most common formats). If a message has a key, Kafka (uses Partitioner ) to make sure that all messages of the same key are in the same partition. Topics \u00b6 Kafka stores messages in topics that are partitioned and replicated across multiple brokers in a cluster. Kafka Clients \u00b6 Producers send messages to topics from which consumers read. Language Agnostic \u00b6 Kafka clients use binary protocol to talk to a Kafka cluster. Consumer Groups \u00b6 Consumers may be grouped in a consumer group with multiple consumers. Each consumer in a consumer group will read messages from a unique subset of partitions in each topic they subscribe to. Each message is delivered to one consumer in the group, and all messages with the same key arrive to the same consumer. Durability \u00b6 Kafka does not track which messages were read by consumers. Kafka keeps all messages for a finite amount of time, and it is consumers' responsibility to track their location per topic (offsets).","title":"Overview"},{"location":"overview/#apache-kafka","text":"Apache Kafka is an open source project for a distributed publish-subscribe messaging system rethought as a distributed commit log .","title":"Apache Kafka"},{"location":"overview/#messages","text":"Messages ( records , events ) are byte arrays (String, JSON, and Avro are among the most common formats). If a message has a key, Kafka (uses Partitioner ) to make sure that all messages of the same key are in the same partition.","title":"Messages"},{"location":"overview/#topics","text":"Kafka stores messages in topics that are partitioned and replicated across multiple brokers in a cluster.","title":"Topics"},{"location":"overview/#kafka-clients","text":"Producers send messages to topics from which consumers read.","title":"Kafka Clients"},{"location":"overview/#language-agnostic","text":"Kafka clients use binary protocol to talk to a Kafka cluster.","title":"Language Agnostic"},{"location":"overview/#consumer-groups","text":"Consumers may be grouped in a consumer group with multiple consumers. Each consumer in a consumer group will read messages from a unique subset of partitions in each topic they subscribe to. Each message is delivered to one consumer in the group, and all messages with the same key arrive to the same consumer.","title":"Consumer Groups"},{"location":"overview/#durability","text":"Kafka does not track which messages were read by consumers. Kafka keeps all messages for a finite amount of time, and it is consumers' responsibility to track their location per topic (offsets).","title":"Durability"},{"location":"clients/","text":"Kafka Clients \u00b6","title":"Kafka Clients"},{"location":"clients/#kafka-clients","text":"","title":"Kafka Clients"},{"location":"clients/CommonClientConfigs/","text":"CommonClientConfigs \u00b6 client.id \u00b6 retries \u00b6","title":"CommonClientConfigs"},{"location":"clients/CommonClientConfigs/#commonclientconfigs","text":"","title":"CommonClientConfigs"},{"location":"clients/CommonClientConfigs/#clientid","text":"","title":" client.id"},{"location":"clients/CommonClientConfigs/#retries","text":"","title":" retries"},{"location":"clients/KafkaClient/","text":"KafkaClient \u00b6 KafkaClient is an interface to NetworkClient . Contract \u00b6 inFlightRequestCount \u00b6 int inFlightRequestCount () Used when: FIXME leastLoadedNode \u00b6 Node leastLoadedNode ( long now ) Used when: FIXME newClientRequest \u00b6 ClientRequest newClientRequest ( String nodeId , AbstractRequest . Builder <?> requestBuilder , long createdTimeMs , boolean expectResponse , int requestTimeoutMs , RequestCompletionHandler callback ) Used when: FIXME poll \u00b6 List < ClientResponse > poll ( long timeout , long now ) Used when: FIXME pollDelayMs \u00b6 long pollDelayMs ( Node node , long now ) Used when: FIXME Is Node Ready and Connected \u00b6 boolean ready ( Node node , long now ); Used when: AdminClientRunnable is requested to sendEligibleCalls ConsumerNetworkClient is requested to tryConnect and trySend InterBrokerSendThread is requested to sendRequests NetworkClientUtils is requested to awaitReady Sender is requested to sendProducerData send \u00b6 void send ( ClientRequest request , long now ) Used when: FIXME wakeup \u00b6 void wakeup () Used when: FIXME Implementations \u00b6 NetworkClient Closeable \u00b6 KafkaClient is a Closeable ( Java ).","title":"KafkaClient"},{"location":"clients/KafkaClient/#kafkaclient","text":"KafkaClient is an interface to NetworkClient .","title":"KafkaClient"},{"location":"clients/KafkaClient/#contract","text":"","title":"Contract"},{"location":"clients/KafkaClient/#inflightrequestcount","text":"int inFlightRequestCount () Used when: FIXME","title":" inFlightRequestCount"},{"location":"clients/KafkaClient/#leastloadednode","text":"Node leastLoadedNode ( long now ) Used when: FIXME","title":" leastLoadedNode"},{"location":"clients/KafkaClient/#newclientrequest","text":"ClientRequest newClientRequest ( String nodeId , AbstractRequest . Builder <?> requestBuilder , long createdTimeMs , boolean expectResponse , int requestTimeoutMs , RequestCompletionHandler callback ) Used when: FIXME","title":" newClientRequest"},{"location":"clients/KafkaClient/#poll","text":"List < ClientResponse > poll ( long timeout , long now ) Used when: FIXME","title":" poll"},{"location":"clients/KafkaClient/#polldelayms","text":"long pollDelayMs ( Node node , long now ) Used when: FIXME","title":" pollDelayMs"},{"location":"clients/KafkaClient/#is-node-ready-and-connected","text":"boolean ready ( Node node , long now ); Used when: AdminClientRunnable is requested to sendEligibleCalls ConsumerNetworkClient is requested to tryConnect and trySend InterBrokerSendThread is requested to sendRequests NetworkClientUtils is requested to awaitReady Sender is requested to sendProducerData","title":" Is Node Ready and Connected"},{"location":"clients/KafkaClient/#send","text":"void send ( ClientRequest request , long now ) Used when: FIXME","title":" send"},{"location":"clients/KafkaClient/#wakeup","text":"void wakeup () Used when: FIXME","title":" wakeup"},{"location":"clients/KafkaClient/#implementations","text":"NetworkClient","title":"Implementations"},{"location":"clients/KafkaClient/#closeable","text":"KafkaClient is a Closeable ( Java ).","title":" Closeable"},{"location":"clients/NetworkClient/","text":"NetworkClient \u00b6 NetworkClient is a KafkaClient .","title":"NetworkClient"},{"location":"clients/NetworkClient/#networkclient","text":"NetworkClient is a KafkaClient .","title":"NetworkClient"},{"location":"clients/NetworkClientUtils/","text":"NetworkClientUtils \u00b6","title":"NetworkClientUtils"},{"location":"clients/NetworkClientUtils/#networkclientutils","text":"","title":"NetworkClientUtils"},{"location":"clients/admin/","text":"Kafka Admin \u00b6","title":"Kafka Admin"},{"location":"clients/admin/#kafka-admin","text":"","title":"Kafka Admin"},{"location":"clients/admin/AdminClientRunnable/","text":"AdminClientRunnable \u00b6","title":"AdminClientRunnable"},{"location":"clients/admin/AdminClientRunnable/#adminclientrunnable","text":"","title":"AdminClientRunnable"},{"location":"clients/consumer/","text":"Kafka Consumers \u00b6","title":"Kafka Consumers"},{"location":"clients/consumer/#kafka-consumers","text":"","title":"Kafka Consumers"},{"location":"clients/consumer/ConsumerNetworkClient/","text":"ConsumerNetworkClient \u00b6","title":"ConsumerNetworkClient"},{"location":"clients/consumer/ConsumerNetworkClient/#consumernetworkclient","text":"","title":"ConsumerNetworkClient"},{"location":"clients/producer/","text":"Kafka Producers \u00b6","title":"Kafka Producers"},{"location":"clients/producer/#kafka-producers","text":"","title":"Kafka Producers"},{"location":"clients/producer/BufferPool/","text":"BufferPool \u00b6 BufferPool is...FIXME","title":"BufferPool"},{"location":"clients/producer/BufferPool/#bufferpool","text":"BufferPool is...FIXME","title":"BufferPool"},{"location":"clients/producer/DefaultPartitioner/","text":"DefaultPartitioner \u00b6 DefaultPartitioner is a Partitioner . Demo \u00b6 import org.apache.kafka.clients.producer.internals.DefaultPartitioner val partitioner = new DefaultPartitioner val keyBytes = \"hello\".getBytes val numPartitions = 3 val p = partitioner.partition(null, null, keyBytes, null, null, null, numPartitions) println(p) The following snippet should generate the same partition value (since it is exactly how DefaultPartitioner does it). import org.apache.kafka.common.utils.Utils val keyBytes = \"hello\".getBytes val numPartitions = 3 val p = Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions println(p)","title":"DefaultPartitioner"},{"location":"clients/producer/DefaultPartitioner/#defaultpartitioner","text":"DefaultPartitioner is a Partitioner .","title":"DefaultPartitioner"},{"location":"clients/producer/DefaultPartitioner/#demo","text":"import org.apache.kafka.clients.producer.internals.DefaultPartitioner val partitioner = new DefaultPartitioner val keyBytes = \"hello\".getBytes val numPartitions = 3 val p = partitioner.partition(null, null, keyBytes, null, null, null, numPartitions) println(p) The following snippet should generate the same partition value (since it is exactly how DefaultPartitioner does it). import org.apache.kafka.common.utils.Utils val keyBytes = \"hello\".getBytes val numPartitions = 3 val p = Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions println(p)","title":"Demo"},{"location":"clients/producer/KafkaProducer/","text":"KafkaProducer \u00b6 KafkaProducer<K, V> is a concrete Producer . Creating Instance \u00b6 KafkaProducer takes the following to be created: ProducerConfig Key Serializer<K> Value Serializer<V> ProducerMetadata KafkaClient ProducerInterceptor<K, V> s Time configureTransactionState \u00b6 TransactionManager configureTransactionState ( ProducerConfig config , LogContext logContext ) configureTransactionState ...FIXME newSender \u00b6 Sender newSender ( LogContext logContext , KafkaClient kafkaClient , ProducerMetadata metadata ) newSender ...FIXME configureInflightRequests \u00b6 int configureInflightRequests ( ProducerConfig config ) configureInflightRequests gives the value of the max.in.flight.requests.per.connection (in the given ProducerConfig ). configureInflightRequests throws a ConfigException when the idempotence is enabled and the value of the max.in.flight.requests.per.connection is above 5: Must set max.in.flight.requests.per.connection to at most 5 to use the idempotent producer. configureAcks \u00b6 short configureAcks ( ProducerConfig config , Logger log ) configureAcks ...FIXME Sender Thread \u00b6 KafkaProducer creates a Sender when created . Sender is immediately started as a daemon thread with the following name (using the clientId ): kafka-producer-network-thread | [clientId] KafkaProducer is actually considered open (and usable) as long as the Sender is running . KafkaProducer simply requests the Sender to wake up for the following: initTransactions sendOffsetsToTransaction commitTransaction abortTransaction doSend waitOnMetadata flush RecordAccumulator \u00b6 KafkaProducer creates a RecordAccumulator when created . This RecordAccumulator is used for the following: Create a Sender append when doSend beginFlush when flush Aborting Incomplete Transaction \u00b6 void abortTransaction () abortTransaction prints out the following INFO message to the logs: Aborting incomplete transaction abortTransaction ...FIXME abortTransaction is part of the Producer abstraction. Sending Record \u00b6 Future < RecordMetadata > send ( ProducerRecord < K , V > record , Callback callback ) send ...FIXME send is part of the Producer abstraction. doSend \u00b6 Future < RecordMetadata > doSend ( ProducerRecord < K , V > record , Callback callback ) doSend ...FIXME partition \u00b6 int partition ( ProducerRecord < K , V > record , byte [] serializedKey , byte [] serializedValue , Cluster cluster ) partition is the partition (of the given ProducerRecord ) if defined or requests the Partitioner for the partition . Flushing \u00b6 void flush () flush requests the RecordAccumulator to beginFlush . flush requests the Sender to wakeup . flush requests the RecordAccumulator to awaitFlushCompletion . flush is part of the Producer abstraction. Demo \u00b6 // Necessary imports import org.apache.kafka.clients.producer.KafkaProducer import org.apache.kafka.clients.producer.ProducerConfig import org.apache.kafka.common.serialization.StringSerializer // Creating a KafkaProducer import java.util.Properties val props = new Properties() props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \":9092\") val producer = new KafkaProducer[String, String](props) // Creating a record to be sent import org.apache.kafka.clients.producer.ProducerRecord val r = new ProducerRecord[String, String](\"0\", \"this is a message\") // Sending the record (with no Callback) import java.util.concurrent.Future import org.apache.kafka.clients.producer.RecordMetadata val metadataF: Future[RecordMetadata] = producer.send(r) Logging \u00b6 Enable ALL logging level for org.apache.kafka.clients.producer.KafkaProducer logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL Refer to Logging .","title":"KafkaProducer"},{"location":"clients/producer/KafkaProducer/#kafkaproducer","text":"KafkaProducer<K, V> is a concrete Producer .","title":"KafkaProducer"},{"location":"clients/producer/KafkaProducer/#creating-instance","text":"KafkaProducer takes the following to be created: ProducerConfig Key Serializer<K> Value Serializer<V> ProducerMetadata KafkaClient ProducerInterceptor<K, V> s Time","title":"Creating Instance"},{"location":"clients/producer/KafkaProducer/#configuretransactionstate","text":"TransactionManager configureTransactionState ( ProducerConfig config , LogContext logContext ) configureTransactionState ...FIXME","title":" configureTransactionState"},{"location":"clients/producer/KafkaProducer/#newsender","text":"Sender newSender ( LogContext logContext , KafkaClient kafkaClient , ProducerMetadata metadata ) newSender ...FIXME","title":" newSender"},{"location":"clients/producer/KafkaProducer/#configureinflightrequests","text":"int configureInflightRequests ( ProducerConfig config ) configureInflightRequests gives the value of the max.in.flight.requests.per.connection (in the given ProducerConfig ). configureInflightRequests throws a ConfigException when the idempotence is enabled and the value of the max.in.flight.requests.per.connection is above 5: Must set max.in.flight.requests.per.connection to at most 5 to use the idempotent producer.","title":" configureInflightRequests"},{"location":"clients/producer/KafkaProducer/#configureacks","text":"short configureAcks ( ProducerConfig config , Logger log ) configureAcks ...FIXME","title":" configureAcks"},{"location":"clients/producer/KafkaProducer/#sender-thread","text":"KafkaProducer creates a Sender when created . Sender is immediately started as a daemon thread with the following name (using the clientId ): kafka-producer-network-thread | [clientId] KafkaProducer is actually considered open (and usable) as long as the Sender is running . KafkaProducer simply requests the Sender to wake up for the following: initTransactions sendOffsetsToTransaction commitTransaction abortTransaction doSend waitOnMetadata flush","title":" Sender Thread"},{"location":"clients/producer/KafkaProducer/#recordaccumulator","text":"KafkaProducer creates a RecordAccumulator when created . This RecordAccumulator is used for the following: Create a Sender append when doSend beginFlush when flush","title":" RecordAccumulator"},{"location":"clients/producer/KafkaProducer/#aborting-incomplete-transaction","text":"void abortTransaction () abortTransaction prints out the following INFO message to the logs: Aborting incomplete transaction abortTransaction ...FIXME abortTransaction is part of the Producer abstraction.","title":" Aborting Incomplete Transaction"},{"location":"clients/producer/KafkaProducer/#sending-record","text":"Future < RecordMetadata > send ( ProducerRecord < K , V > record , Callback callback ) send ...FIXME send is part of the Producer abstraction.","title":" Sending Record"},{"location":"clients/producer/KafkaProducer/#dosend","text":"Future < RecordMetadata > doSend ( ProducerRecord < K , V > record , Callback callback ) doSend ...FIXME","title":" doSend"},{"location":"clients/producer/KafkaProducer/#partition","text":"int partition ( ProducerRecord < K , V > record , byte [] serializedKey , byte [] serializedValue , Cluster cluster ) partition is the partition (of the given ProducerRecord ) if defined or requests the Partitioner for the partition .","title":" partition"},{"location":"clients/producer/KafkaProducer/#flushing","text":"void flush () flush requests the RecordAccumulator to beginFlush . flush requests the Sender to wakeup . flush requests the RecordAccumulator to awaitFlushCompletion . flush is part of the Producer abstraction.","title":" Flushing"},{"location":"clients/producer/KafkaProducer/#demo","text":"// Necessary imports import org.apache.kafka.clients.producer.KafkaProducer import org.apache.kafka.clients.producer.ProducerConfig import org.apache.kafka.common.serialization.StringSerializer // Creating a KafkaProducer import java.util.Properties val props = new Properties() props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \":9092\") val producer = new KafkaProducer[String, String](props) // Creating a record to be sent import org.apache.kafka.clients.producer.ProducerRecord val r = new ProducerRecord[String, String](\"0\", \"this is a message\") // Sending the record (with no Callback) import java.util.concurrent.Future import org.apache.kafka.clients.producer.RecordMetadata val metadataF: Future[RecordMetadata] = producer.send(r)","title":"Demo"},{"location":"clients/producer/KafkaProducer/#logging","text":"Enable ALL logging level for org.apache.kafka.clients.producer.KafkaProducer logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL Refer to Logging .","title":"Logging"},{"location":"clients/producer/Partitioner/","text":"Partitioner \u00b6 Partitioner is an abstraction of partitioners for a KafkaProducer to determine the partition of records (to be sent out ). Configurable \u00b6 Partitioner is a Configurable . Closeable \u00b6 Partitioner is a Closeable ( Java ). Contract \u00b6 onNewBatch \u00b6 void onNewBatch ( String topic , Cluster cluster , int prevPartition ) Used when: KafkaProducer is requested to send a record (and doSend ) Computing Partition \u00b6 int partition ( String topic , Object key , byte [] keyBytes , Object value , byte [] valueBytes , Cluster cluster ) Used when: KafkaProducer is requested to send a record (and determines the partition ) Implementations \u00b6 DefaultPartitioner UniformStickyPartitioner RoundRobinPartitioner","title":"Partitioner"},{"location":"clients/producer/Partitioner/#partitioner","text":"Partitioner is an abstraction of partitioners for a KafkaProducer to determine the partition of records (to be sent out ).","title":"Partitioner"},{"location":"clients/producer/Partitioner/#configurable","text":"Partitioner is a Configurable .","title":" Configurable"},{"location":"clients/producer/Partitioner/#closeable","text":"Partitioner is a Closeable ( Java ).","title":" Closeable"},{"location":"clients/producer/Partitioner/#contract","text":"","title":"Contract"},{"location":"clients/producer/Partitioner/#onnewbatch","text":"void onNewBatch ( String topic , Cluster cluster , int prevPartition ) Used when: KafkaProducer is requested to send a record (and doSend )","title":" onNewBatch"},{"location":"clients/producer/Partitioner/#computing-partition","text":"int partition ( String topic , Object key , byte [] keyBytes , Object value , byte [] valueBytes , Cluster cluster ) Used when: KafkaProducer is requested to send a record (and determines the partition )","title":" Computing Partition"},{"location":"clients/producer/Partitioner/#implementations","text":"DefaultPartitioner UniformStickyPartitioner RoundRobinPartitioner","title":"Implementations"},{"location":"clients/producer/Producer/","text":"Producer \u00b6 Producer<K, V> is an interface to KafkaProducer for Kafka developers to use to send messages (with K keys and V values) to a Kafka cluster. Contract (Subset) \u00b6 abortTransaction \u00b6 void abortTransaction () beginTransaction \u00b6 void beginTransaction () Used when: FIXME commitTransaction \u00b6 void commitTransaction () Used when: FIXME initTransactions \u00b6 void initTransactions () Used when: FIXME sendOffsetsToTransaction \u00b6 void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , String consumerGroupId ) void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , ConsumerGroupMetadata groupMetadata ) Used when: FIXME","title":"Producer"},{"location":"clients/producer/Producer/#producer","text":"Producer<K, V> is an interface to KafkaProducer for Kafka developers to use to send messages (with K keys and V values) to a Kafka cluster.","title":"Producer"},{"location":"clients/producer/Producer/#contract-subset","text":"","title":"Contract (Subset)"},{"location":"clients/producer/Producer/#aborttransaction","text":"void abortTransaction ()","title":" abortTransaction"},{"location":"clients/producer/Producer/#begintransaction","text":"void beginTransaction () Used when: FIXME","title":" beginTransaction"},{"location":"clients/producer/Producer/#committransaction","text":"void commitTransaction () Used when: FIXME","title":" commitTransaction"},{"location":"clients/producer/Producer/#inittransactions","text":"void initTransactions () Used when: FIXME","title":" initTransactions"},{"location":"clients/producer/Producer/#sendoffsetstotransaction","text":"void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , String consumerGroupId ) void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , ConsumerGroupMetadata groupMetadata ) Used when: FIXME","title":" sendOffsetsToTransaction"},{"location":"clients/producer/ProducerBatch/","text":"ProducerBatch \u00b6 Creating Instance \u00b6 ProducerBatch takes the following to be created: TopicPartition MemoryRecordsBuilder createdMs isSplitBatch flag (default: false ) ProducerBatch is created when: ProducerBatch is requested to createBatchOffAccumulatorForRecord RecordAccumulator is requested to append a record tryAppend \u00b6 FutureRecordMetadata tryAppend ( long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long now ) tryAppend ...FIXME tryAppend is used when: RecordAccumulator is requested to append a record","title":"ProducerBatch"},{"location":"clients/producer/ProducerBatch/#producerbatch","text":"","title":"ProducerBatch"},{"location":"clients/producer/ProducerBatch/#creating-instance","text":"ProducerBatch takes the following to be created: TopicPartition MemoryRecordsBuilder createdMs isSplitBatch flag (default: false ) ProducerBatch is created when: ProducerBatch is requested to createBatchOffAccumulatorForRecord RecordAccumulator is requested to append a record","title":"Creating Instance"},{"location":"clients/producer/ProducerBatch/#tryappend","text":"FutureRecordMetadata tryAppend ( long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long now ) tryAppend ...FIXME tryAppend is used when: RecordAccumulator is requested to append a record","title":" tryAppend"},{"location":"clients/producer/ProducerConfig/","text":"ProducerConfig \u00b6 batch.size \u00b6 The buffer size allocated for a partition. When records are received (which are smaller than this size) KafkaProducer will attempt to optimistically group them together until this size is reached. Default: 16384 Must be at least 0 Related to: linger.ms max-partition-memory-bytes ( ConsoleProducer ) Used when: KafkaProducer is created (to create a RecordAccumulator and an accompanying BufferPool ) KafkaLog4jAppender is requested to activateOptions enable.idempotence \u00b6 Default: false Used when: KafkaProducer is requested to configureTransactionState ProducerConfig is requested to maybeOverrideEnableIdempotence and idempotenceEnabled linger.ms \u00b6 max.in.flight.requests.per.connection \u00b6 The maximum number of unacknowledged requests the client will send on a single connection before blocking. Note that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled). Default: 5 Must be at least 1 Related to: enable.idempotence retries Used when: KafkaProducer is requested to configureInflightRequests partitioner.class \u00b6 The class of the Partitioner for a KafkaProducer Default: DefaultPartitioner retries \u00b6 transactional.id \u00b6 The TransactionalId to use for transactional delivery. This enables reliability semantics which span multiple producer sessions since it allows the client to guarantee that transactions using the same TransactionalId have been completed prior to starting any new transactions. If no TransactionalId is provided, then the producer is limited to idempotent delivery. If a TransactionalId is configured, enable.idempotence is implied. Default: (empty) (transactions cannot be used) Note that, by default, transactions require a cluster of at least three brokers which is the recommended setting for production; for development you can change this, by adjusting broker setting transaction.state.log.replication.factor . transaction.state.log.replication.factor \u00b6 idempotenceEnabled \u00b6 boolean idempotenceEnabled () idempotenceEnabled ...FIXME idempotenceEnabled is used when: KafkaProducer is created (and requested to configureTransactionState , configureInflightRequests , configureAcks ) ProducerConfig is requested to maybeOverrideAcksAndRetries postProcessParsedConfig \u00b6 Map < String , Object > postProcessParsedConfig ( Map < String , Object > parsedValues ) postProcessParsedConfig maybeOverrideEnableIdempotence . postProcessParsedConfig maybeOverrideClientId . postProcessParsedConfig maybeOverrideAcksAndRetries . postProcessParsedConfig is part of the AbstractConfig abstraction. maybeOverrideClientId \u00b6 maybeOverrideAcksAndRetries uses transactional.id (if defined) or the next available ID for an ID with producer- prefix for client.id unless already defined. maybeOverrideAcksAndRetries \u00b6 void maybeOverrideAcksAndRetries ( Map < String , Object > configs ) maybeOverrideAcksAndRetries ...FIXME maybeOverrideEnableIdempotence \u00b6 void maybeOverrideEnableIdempotence ( Map < String , Object > configs ) maybeOverrideEnableIdempotence ...FIXME","title":"ProducerConfig"},{"location":"clients/producer/ProducerConfig/#producerconfig","text":"","title":"ProducerConfig"},{"location":"clients/producer/ProducerConfig/#batchsize","text":"The buffer size allocated for a partition. When records are received (which are smaller than this size) KafkaProducer will attempt to optimistically group them together until this size is reached. Default: 16384 Must be at least 0 Related to: linger.ms max-partition-memory-bytes ( ConsoleProducer ) Used when: KafkaProducer is created (to create a RecordAccumulator and an accompanying BufferPool ) KafkaLog4jAppender is requested to activateOptions","title":" batch.size"},{"location":"clients/producer/ProducerConfig/#enableidempotence","text":"Default: false Used when: KafkaProducer is requested to configureTransactionState ProducerConfig is requested to maybeOverrideEnableIdempotence and idempotenceEnabled","title":" enable.idempotence"},{"location":"clients/producer/ProducerConfig/#lingerms","text":"","title":" linger.ms"},{"location":"clients/producer/ProducerConfig/#maxinflightrequestsperconnection","text":"The maximum number of unacknowledged requests the client will send on a single connection before blocking. Note that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled). Default: 5 Must be at least 1 Related to: enable.idempotence retries Used when: KafkaProducer is requested to configureInflightRequests","title":" max.in.flight.requests.per.connection"},{"location":"clients/producer/ProducerConfig/#partitionerclass","text":"The class of the Partitioner for a KafkaProducer Default: DefaultPartitioner","title":" partitioner.class"},{"location":"clients/producer/ProducerConfig/#retries","text":"","title":" retries"},{"location":"clients/producer/ProducerConfig/#transactionalid","text":"The TransactionalId to use for transactional delivery. This enables reliability semantics which span multiple producer sessions since it allows the client to guarantee that transactions using the same TransactionalId have been completed prior to starting any new transactions. If no TransactionalId is provided, then the producer is limited to idempotent delivery. If a TransactionalId is configured, enable.idempotence is implied. Default: (empty) (transactions cannot be used) Note that, by default, transactions require a cluster of at least three brokers which is the recommended setting for production; for development you can change this, by adjusting broker setting transaction.state.log.replication.factor .","title":" transactional.id"},{"location":"clients/producer/ProducerConfig/#transactionstatelogreplicationfactor","text":"","title":" transaction.state.log.replication.factor"},{"location":"clients/producer/ProducerConfig/#idempotenceenabled","text":"boolean idempotenceEnabled () idempotenceEnabled ...FIXME idempotenceEnabled is used when: KafkaProducer is created (and requested to configureTransactionState , configureInflightRequests , configureAcks ) ProducerConfig is requested to maybeOverrideAcksAndRetries","title":" idempotenceEnabled"},{"location":"clients/producer/ProducerConfig/#postprocessparsedconfig","text":"Map < String , Object > postProcessParsedConfig ( Map < String , Object > parsedValues ) postProcessParsedConfig maybeOverrideEnableIdempotence . postProcessParsedConfig maybeOverrideClientId . postProcessParsedConfig maybeOverrideAcksAndRetries . postProcessParsedConfig is part of the AbstractConfig abstraction.","title":" postProcessParsedConfig"},{"location":"clients/producer/ProducerConfig/#maybeoverrideclientid","text":"maybeOverrideAcksAndRetries uses transactional.id (if defined) or the next available ID for an ID with producer- prefix for client.id unless already defined.","title":" maybeOverrideClientId"},{"location":"clients/producer/ProducerConfig/#maybeoverrideacksandretries","text":"void maybeOverrideAcksAndRetries ( Map < String , Object > configs ) maybeOverrideAcksAndRetries ...FIXME","title":" maybeOverrideAcksAndRetries"},{"location":"clients/producer/ProducerConfig/#maybeoverrideenableidempotence","text":"void maybeOverrideEnableIdempotence ( Map < String , Object > configs ) maybeOverrideEnableIdempotence ...FIXME","title":" maybeOverrideEnableIdempotence"},{"location":"clients/producer/RecordAccumulator/","text":"RecordAccumulator \u00b6 Creating Instance \u00b6 RecordAccumulator takes the following to be created: LogContext Batch size CompressionType Linger retryBackoffMs deliveryTimeoutMs Metrics Name of the Metrics Group Time ApiVersions TransactionManager BufferPool RecordAccumulator is created for KafkaProducer . appendsInProgress Counter \u00b6 RecordAccumulator creates an AtomicInteger ( Java ) for appendsInProgress internal counter when created . appendsInProgress simply marks a single execution of append (and is incremented at the beginning and decremented right at the end). appendsInProgress is used when flushInProgress . flushInProgress \u00b6 boolean appendsInProgress () appendsInProgress indicates if the appendsInProgress counter is above 0 . appendsInProgress is used when abortIncompleteBatches . flushesInProgress Counter \u00b6 RecordAccumulator creates an AtomicInteger ( Java ) for flushesInProgress internal counter when created . flushesInProgress is incremented when beginFlush and decremented when awaitFlushCompletion . flushesInProgress is used when flushInProgress . flushInProgress \u00b6 boolean flushInProgress () flushInProgress indicates if the flushesInProgress counter is above 0 . flushInProgress is used when: RecordAccumulator is requested to ready Sender is requested to maybeSendAndPollTransactionalRequest Appending Record \u00b6 RecordAppendResult append ( TopicPartition tp , long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long maxTimeToBlock , boolean abortOnNewBatch , long nowMs ) append ...FIXME append is used when: KafkaProducer is requested to send a record (and doSend ) tryAppend \u00b6 RecordAppendResult tryAppend ( long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , Deque < ProducerBatch > deque , long nowMs ) tryAppend ...FIXME ready \u00b6 ReadyCheckResult ready ( Cluster cluster , long nowMs ) ready is a list of partitions with data ready to send. ready ...FIXME ready is used when: Sender is requested to sendProducerData beginFlush \u00b6 void beginFlush () beginFlush atomically increments the flushesInProgress counter. beginFlush is used when: KafkaProducer is requested to flush Sender is requested to maybeSendAndPollTransactionalRequest awaitFlushCompletion \u00b6 void awaitFlushCompletion () awaitFlushCompletion ...FIXME awaitFlushCompletion is used when: KafkaProducer is requested to flush splitAndReenqueue \u00b6 int splitAndReenqueue ( ProducerBatch bigBatch ) splitAndReenqueue ...FIXME splitAndReenqueue is used when: Sender is requested to completeBatch deallocate \u00b6 void deallocate ( ProducerBatch batch ) deallocate ...FIXME deallocate is used when: RecordAccumulator is requested to abortBatches and abortUndrainedBatches Sender is requested to maybeRemoveAndDeallocateBatch abortBatches \u00b6 void abortBatches () // (1) void abortBatches ( RuntimeException reason ) Uses a KafkaException abortBatches ...FIXME abortBatches is used when: RecordAccumulator is requested to abortIncompleteBatches Sender is requested to maybeAbortBatches abortIncompleteBatches \u00b6 void abortIncompleteBatches () abortIncompleteBatches abortBatches as long as there are appendsInProgress . abortIncompleteBatches abortBatches one last time (after no thread was appending in case there was a new batch appended by the last appending thread). In the end, abortIncompleteBatches clears the batches registry. abortIncompleteBatches is used when: Sender is requested to run (and forceClose ) abortUndrainedBatches \u00b6 void abortUndrainedBatches ( RuntimeException reason ) abortUndrainedBatches ...FIXME abortUndrainedBatches is used when: Sender is requested to maybeSendAndPollTransactionalRequest Incomplete (Pending) Batches \u00b6 RecordAccumulator creates an IncompleteBatches for incomplete internal registry of pending batches when created . RecordAccumulator uses the IncompleteBatches when: append (to add a new ProducerBatch ) splitAndReenqueue (to add a new ProducerBatch ) deallocate (to remove a ProducerBatch ) awaitFlushCompletion , abortBatches and abortUndrainedBatches (to copy all ProducerBatch s) hasIncomplete \u00b6 boolean hasIncomplete () hasIncomplete is true when the incomplete registry is not empty. hasIncomplete is used when: Sender is requested to maybeSendAndPollTransactionalRequest and maybeAbortBatches In-Progress Batches \u00b6 ConcurrentMap < TopicPartition , Deque < ProducerBatch >> batches RecordAccumulator creates a ConcurrentMap ( Java ) for the batches internal registry of in-progress ProducerBatch es (per TopicPartition ). RecordAccumulator adds a new ArrayDeque ( Java ) when getOrCreateDeque . batches is used when: expiredBatches ready hasUndrained getDeque batches abortIncompleteBatches getOrCreateDeque \u00b6 Deque < ProducerBatch > getOrCreateDeque ( TopicPartition tp ) getOrCreateDeque ...FIXME getOrCreateDeque is used when: RecordAccumulator is requested to append , reenqueue , splitAndReenqueue","title":"RecordAccumulator"},{"location":"clients/producer/RecordAccumulator/#recordaccumulator","text":"","title":"RecordAccumulator"},{"location":"clients/producer/RecordAccumulator/#creating-instance","text":"RecordAccumulator takes the following to be created: LogContext Batch size CompressionType Linger retryBackoffMs deliveryTimeoutMs Metrics Name of the Metrics Group Time ApiVersions TransactionManager BufferPool RecordAccumulator is created for KafkaProducer .","title":"Creating Instance"},{"location":"clients/producer/RecordAccumulator/#appendsinprogress-counter","text":"RecordAccumulator creates an AtomicInteger ( Java ) for appendsInProgress internal counter when created . appendsInProgress simply marks a single execution of append (and is incremented at the beginning and decremented right at the end). appendsInProgress is used when flushInProgress .","title":" appendsInProgress Counter"},{"location":"clients/producer/RecordAccumulator/#flushinprogress","text":"boolean appendsInProgress () appendsInProgress indicates if the appendsInProgress counter is above 0 . appendsInProgress is used when abortIncompleteBatches .","title":"flushInProgress"},{"location":"clients/producer/RecordAccumulator/#flushesinprogress-counter","text":"RecordAccumulator creates an AtomicInteger ( Java ) for flushesInProgress internal counter when created . flushesInProgress is incremented when beginFlush and decremented when awaitFlushCompletion . flushesInProgress is used when flushInProgress .","title":" flushesInProgress Counter"},{"location":"clients/producer/RecordAccumulator/#flushinprogress_1","text":"boolean flushInProgress () flushInProgress indicates if the flushesInProgress counter is above 0 . flushInProgress is used when: RecordAccumulator is requested to ready Sender is requested to maybeSendAndPollTransactionalRequest","title":" flushInProgress"},{"location":"clients/producer/RecordAccumulator/#appending-record","text":"RecordAppendResult append ( TopicPartition tp , long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long maxTimeToBlock , boolean abortOnNewBatch , long nowMs ) append ...FIXME append is used when: KafkaProducer is requested to send a record (and doSend )","title":" Appending Record"},{"location":"clients/producer/RecordAccumulator/#tryappend","text":"RecordAppendResult tryAppend ( long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , Deque < ProducerBatch > deque , long nowMs ) tryAppend ...FIXME","title":" tryAppend"},{"location":"clients/producer/RecordAccumulator/#ready","text":"ReadyCheckResult ready ( Cluster cluster , long nowMs ) ready is a list of partitions with data ready to send. ready ...FIXME ready is used when: Sender is requested to sendProducerData","title":" ready"},{"location":"clients/producer/RecordAccumulator/#beginflush","text":"void beginFlush () beginFlush atomically increments the flushesInProgress counter. beginFlush is used when: KafkaProducer is requested to flush Sender is requested to maybeSendAndPollTransactionalRequest","title":" beginFlush"},{"location":"clients/producer/RecordAccumulator/#awaitflushcompletion","text":"void awaitFlushCompletion () awaitFlushCompletion ...FIXME awaitFlushCompletion is used when: KafkaProducer is requested to flush","title":" awaitFlushCompletion"},{"location":"clients/producer/RecordAccumulator/#splitandreenqueue","text":"int splitAndReenqueue ( ProducerBatch bigBatch ) splitAndReenqueue ...FIXME splitAndReenqueue is used when: Sender is requested to completeBatch","title":" splitAndReenqueue"},{"location":"clients/producer/RecordAccumulator/#deallocate","text":"void deallocate ( ProducerBatch batch ) deallocate ...FIXME deallocate is used when: RecordAccumulator is requested to abortBatches and abortUndrainedBatches Sender is requested to maybeRemoveAndDeallocateBatch","title":" deallocate"},{"location":"clients/producer/RecordAccumulator/#abortbatches","text":"void abortBatches () // (1) void abortBatches ( RuntimeException reason ) Uses a KafkaException abortBatches ...FIXME abortBatches is used when: RecordAccumulator is requested to abortIncompleteBatches Sender is requested to maybeAbortBatches","title":" abortBatches"},{"location":"clients/producer/RecordAccumulator/#abortincompletebatches","text":"void abortIncompleteBatches () abortIncompleteBatches abortBatches as long as there are appendsInProgress . abortIncompleteBatches abortBatches one last time (after no thread was appending in case there was a new batch appended by the last appending thread). In the end, abortIncompleteBatches clears the batches registry. abortIncompleteBatches is used when: Sender is requested to run (and forceClose )","title":" abortIncompleteBatches"},{"location":"clients/producer/RecordAccumulator/#abortundrainedbatches","text":"void abortUndrainedBatches ( RuntimeException reason ) abortUndrainedBatches ...FIXME abortUndrainedBatches is used when: Sender is requested to maybeSendAndPollTransactionalRequest","title":" abortUndrainedBatches"},{"location":"clients/producer/RecordAccumulator/#incomplete-pending-batches","text":"RecordAccumulator creates an IncompleteBatches for incomplete internal registry of pending batches when created . RecordAccumulator uses the IncompleteBatches when: append (to add a new ProducerBatch ) splitAndReenqueue (to add a new ProducerBatch ) deallocate (to remove a ProducerBatch ) awaitFlushCompletion , abortBatches and abortUndrainedBatches (to copy all ProducerBatch s)","title":" Incomplete (Pending) Batches"},{"location":"clients/producer/RecordAccumulator/#hasincomplete","text":"boolean hasIncomplete () hasIncomplete is true when the incomplete registry is not empty. hasIncomplete is used when: Sender is requested to maybeSendAndPollTransactionalRequest and maybeAbortBatches","title":" hasIncomplete"},{"location":"clients/producer/RecordAccumulator/#in-progress-batches","text":"ConcurrentMap < TopicPartition , Deque < ProducerBatch >> batches RecordAccumulator creates a ConcurrentMap ( Java ) for the batches internal registry of in-progress ProducerBatch es (per TopicPartition ). RecordAccumulator adds a new ArrayDeque ( Java ) when getOrCreateDeque . batches is used when: expiredBatches ready hasUndrained getDeque batches abortIncompleteBatches","title":" In-Progress Batches"},{"location":"clients/producer/RecordAccumulator/#getorcreatedeque","text":"Deque < ProducerBatch > getOrCreateDeque ( TopicPartition tp ) getOrCreateDeque ...FIXME getOrCreateDeque is used when: RecordAccumulator is requested to append , reenqueue , splitAndReenqueue","title":" getOrCreateDeque"},{"location":"clients/producer/Sender/","text":"Sender \u00b6 Sender is a Runnable ( Java ) (and is intended to be executed by a thread). Creating Instance \u00b6 Sender takes the following to be created: LogContext KafkaClient ProducerMetadata RecordAccumulator guaranteeMessageOrder flag maxRequestSize acks retries SenderMetricsRegistry Time requestTimeoutMs retryBackoffMs TransactionManager ApiVersions Sender is created along with a KafkaProducer . KafkaClient \u00b6 Sender is given a KafkaClient when created . Waking Up \u00b6 void wakeup () wakeup requests the KafkaClient to wakeup . wakeup is used when: KafkaProducer is requested to initTransactions , sendOffsetsToTransaction , commitTransaction , abortTransaction , doSend , waitOnMetadata , flush Sender is requested to initiateClose","title":"Sender"},{"location":"clients/producer/Sender/#sender","text":"Sender is a Runnable ( Java ) (and is intended to be executed by a thread).","title":"Sender"},{"location":"clients/producer/Sender/#creating-instance","text":"Sender takes the following to be created: LogContext KafkaClient ProducerMetadata RecordAccumulator guaranteeMessageOrder flag maxRequestSize acks retries SenderMetricsRegistry Time requestTimeoutMs retryBackoffMs TransactionManager ApiVersions Sender is created along with a KafkaProducer .","title":"Creating Instance"},{"location":"clients/producer/Sender/#kafkaclient","text":"Sender is given a KafkaClient when created .","title":" KafkaClient"},{"location":"clients/producer/Sender/#waking-up","text":"void wakeup () wakeup requests the KafkaClient to wakeup . wakeup is used when: KafkaProducer is requested to initTransactions , sendOffsetsToTransaction , commitTransaction , abortTransaction , doSend , waitOnMetadata , flush Sender is requested to initiateClose","title":" Waking Up"},{"location":"clients/producer/TransactionManager/","text":"TransactionManager \u00b6 TransactionManager is...FIXME","title":"TransactionManager"},{"location":"clients/producer/TransactionManager/#transactionmanager","text":"TransactionManager is...FIXME","title":"TransactionManager"}]}