{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"The Internals of Apache Kafka 3.6.0","text":"<p>Welcome to The Internals of Apache Kafka online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, a Freelance Data Engineer specializing in Apache Spark (incl. Spark SQL and Spark Structured Streaming), Delta Lake, Databricks, and Apache Kafka (incl. Kafka Streams) with brief forays into a wider data engineering space (e.g., Trino, Dask and dbt, mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Kafka as much as I have.</p> <p>Flannery O'Connor</p> <p>I write to discover what I know.</p> <p>\"The Internals Of\" series</p> <p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p> <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into Apache Kafka \ud83d\udd25</p> <p>Last update: 2023-12-02</p>"},{"location":"AbstractConfig/","title":"AbstractConfig","text":"<p><code>AbstractConfig</code> is...FIXME</p>"},{"location":"AbstractRequest/","title":"AbstractRequest","text":"<p><code>AbstractRequest</code> is...FIXME</p>"},{"location":"ApiRequestHandler/","title":"ApiRequestHandler","text":"<p><code>ApiRequestHandler</code> is...FIXME</p>"},{"location":"AutoTopicCreationManager/","title":"AutoTopicCreationManager","text":"<p><code>AutoTopicCreationManager</code> is an abstraction of managers that can create topics.</p>"},{"location":"AutoTopicCreationManager/#contract","title":"Contract","text":""},{"location":"AutoTopicCreationManager/#createtopics","title":"createTopics <pre><code>createTopics(\n  topicNames: Set[String],\n  controllerMutationQuota: ControllerMutationQuota): Seq[MetadataResponseTopic]\n</code></pre> <p>Used when:</p> <ul> <li><code>KafkaApis</code> is requested to getTopicMetadata and handleFindCoordinatorRequest</li> </ul>","text":""},{"location":"AutoTopicCreationManager/#shutdown","title":"shutdown <pre><code>shutdown(): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>BrokerServer</code> is requested to shutdown</li> <li><code>KafkaServer</code> is requested to shutdown</li> </ul>","text":""},{"location":"AutoTopicCreationManager/#start","title":"start <pre><code>start(): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>BrokerServer</code> is requested to startup</li> <li><code>KafkaServer</code> is requested to startup</li> </ul>","text":""},{"location":"AutoTopicCreationManager/#implementations","title":"Implementations","text":"<ul> <li>DefaultAutoTopicCreationManager</li> </ul>"},{"location":"AutoTopicCreationManager/#creating-autotopiccreationmanager","title":"Creating AutoTopicCreationManager <pre><code>apply(\n  config: KafkaConfig,\n  metadataCache: MetadataCache,\n  time: Time,\n  metrics: Metrics,\n  threadNamePrefix: Option[String],\n  adminManager: Option[ZkAdminManager],\n  controller: Option[KafkaController],\n  groupCoordinator: GroupCoordinator,\n  txnCoordinator: TransactionCoordinator,\n  enableForwarding: Boolean): AutoTopicCreationManager\n</code></pre> <p><code>apply</code> creates a DefaultAutoTopicCreationManager.</p> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>KafkaServer</code> is requested to startup</li> </ul>","text":""},{"location":"BrokerMetadataCheckpoint/","title":"BrokerMetadataCheckpoint","text":"<p><code>BrokerMetadataCheckpoint</code> is...FIXME</p>"},{"location":"Configurable/","title":"Configurable","text":"<p><code>Configurable</code> is...FIXME</p>"},{"location":"DefaultAutoTopicCreationManager/","title":"DefaultAutoTopicCreationManager","text":"<p><code>DefaultAutoTopicCreationManager</code> is an AutoTopicCreationManager.</p>"},{"location":"DefaultAutoTopicCreationManager/#creating-instance","title":"Creating Instance","text":"<p><code>DefaultAutoTopicCreationManager</code> takes the following to be created:</p> <ul> <li> KafkaConfig <li> Optional <code>BrokerToControllerChannelManager</code> <li> Optional ZkAdminManager <li> Optional KafkaController <li> GroupCoordinator <li>TransactionCoordinator</li> <p><code>DefaultAutoTopicCreationManager</code> is created\u00a0when:</p> <ul> <li><code>AutoTopicCreationManager</code> utility is used to create an AutoTopicCreationManager</li> <li><code>BrokerServer</code> is requested to startup</li> </ul>"},{"location":"DefaultAutoTopicCreationManager/#transactioncoordinator","title":"TransactionCoordinator <p><code>DefaultAutoTopicCreationManager</code> is given a TransactionCoordinator when created.</p> <p><code>DefaultAutoTopicCreationManager</code> uses the <code>TransactionCoordinator</code> for the transactionTopicConfigs when requested to creatableTopic for __transaction_state topic.</p>","text":""},{"location":"DefaultAutoTopicCreationManager/#createtopics","title":"createTopics <pre><code>createTopics(\n  topics: Set[String],\n  controllerMutationQuota: ControllerMutationQuota,\n  metadataRequestContext: Option[RequestContext]): Seq[MetadataResponseTopic]\n</code></pre> <p><code>createTopics</code> is part of the AutoTopicCreationManager abstraction.</p>  <p><code>createTopics</code> filterCreatableTopics the given <code>topics</code>.</p> <p><code>createTopics</code> sendCreateTopicRequest when the controller is undefined or is not active and the channelManager is defined. Otherwise, <code>createTopics</code> createTopicsInZk.</p>","text":""},{"location":"DefaultAutoTopicCreationManager/#filtercreatabletopics","title":"filterCreatableTopics <pre><code>filterCreatableTopics(\n  topics: Set[String]): (Map[String, CreatableTopic], Seq[MetadataResponseTopic])\n</code></pre> <p><code>filterCreatableTopics</code> splits the <code>topics</code> into CreatableTopics and with errors (e.g. invalid names or being created).</p>","text":""},{"location":"DefaultAutoTopicCreationManager/#creatabletopic","title":"creatableTopic <pre><code>creatableTopic(\n  topic: String): CreatableTopic\n</code></pre> <p><code>creatableTopic</code> creates a <code>CreatableTopic</code> with the given <code>topic</code> name.</p>    Topic Name Number of Partitions Replication Factor Configs     <code>__consumer_offsets</code> offsets.topic.num.partitions offsets.topic.replication.factor offsetsTopicConfigs   <code>__transaction_state</code> transaction.state.log.num.partitions transaction.state.log.replication.factor transactionTopicConfigs   other topics num.partitions default.replication.factor","text":""},{"location":"DefaultAutoTopicCreationManager/#createtopicsinzk","title":"createTopicsInZk <pre><code>createTopicsInZk(\n  creatableTopics: Map[String, CreatableTopic],\n  controllerMutationQuota: ControllerMutationQuota): Seq[MetadataResponseTopic]\n</code></pre> <p><code>createTopicsInZk</code> requests the ZkAdminManager to create the topics.</p>","text":""},{"location":"FetchResponseData/","title":"FetchResponseData","text":"<p><code>FetchResponseData</code> is a <code>ApiMessage</code>.</p>"},{"location":"InterBrokerSendThread/","title":"InterBrokerSendThread","text":""},{"location":"Kafka/","title":"Kafka Utility","text":"<p><code>kafka.Kafka</code> is a command-line application that is executed using <code>kafka-server-start</code> shell script.</p>"},{"location":"Kafka/#launching-kafka-server","title":"Launching Kafka Server <pre><code>main(\n  args: Array[String]): Unit\n</code></pre> <p><code>main</code>...FIXME</p>","text":""},{"location":"Kafka/#buildserver","title":"buildServer <pre><code>buildServer(\n  props: Properties): Server\n</code></pre> <p><code>buildServer</code> creates a KafkaConfig (from the given <code>Properties</code>).</p> <p><code>buildServer</code> creates a Server based on process.roles configuration property:</p> <ul> <li>KafkaServer if empty</li> <li>KafkaRaftServer, otherwise</li> </ul>","text":""},{"location":"KafkaApis/","title":"KafkaApis","text":"<p><code>KafkaApis</code> is responsible to handle API requests to a Kafka broker (by means of handlers).</p> <p></p> <p>Some requests are meant for the controller broker and simply do nothing (no-ops) when received by a regular non-controller broker.</p>"},{"location":"KafkaApis/#creating-instance","title":"Creating Instance","text":"<p><code>KafkaApis</code> takes the following to be created:</p> <ul> <li> <code>RequestChannel</code> <li> <code>MetadataSupport</code> <li> ReplicaManager <li>GroupCoordinator</li> <li>TransactionCoordinator</li> <li> AutoTopicCreationManager <li> Broker ID <li> KafkaConfig <li> <code>ConfigRepository</code> <li> MetadataCache <li> Metrics <li>Authorizer</li> <li> <code>QuotaManagers</code> <li> <code>FetchManager</code> <li> <code>BrokerTopicStats</code> <li> Cluster ID <li> <code>Time</code> <li> <code>DelegationTokenManager</code> <li> <code>ApiVersionManager</code> <p><code>KafkaApis</code> is created\u00a0when:</p> <ul> <li><code>BrokerServer</code> is requested to start up (for the dataPlaneRequestProcessor and the controlPlaneRequestProcessor)</li> <li><code>KafkaServer</code> is requested to start up (for the dataPlaneRequestProcessor and the controlPlaneRequestProcessor)</li> </ul>"},{"location":"KafkaApis/#authorizer","title":"Authorizer <p><code>KafkaApis</code> can be given an Authorizer when created. The <code>Authorizer</code> instance is given right from the creator (based on authorizer.class.name configuration property):</p> <ul> <li>BrokerServer</li> <li>KafkaServer</li> </ul> <p>The <code>Authorizer</code> is used only to create the following:</p> <ul> <li>AuthHelper</li> <li>AclApis</li> </ul>","text":""},{"location":"KafkaApis/#authhelper","title":"AuthHelper <p><code>KafkaApis</code> creates an AuthHelper when created.</p> <p>The <code>AuthHelper</code> is given the optional Authorizer.</p> <p>The <code>AuthHelper</code> is used to create the AclApis and authorize operations.</p>","text":""},{"location":"KafkaApis/#groupcoordinator","title":"GroupCoordinator <p><code>KafkaApis</code> is given a GroupCoordinator when created.</p> <p>The <code>GroupCoordinator</code> is used for the following:</p> <ul> <li>handleAddOffsetsToTxnRequest</li> <li>handleDeleteGroupsRequest</li> <li>handleDescribeGroupRequest</li> <li>handleFindCoordinatorRequest</li> <li>handleHeartbeatRequest</li> <li>handleJoinGroupRequest</li> <li>handleLeaderAndIsrRequest</li> <li>handleLeaveGroupRequest</li> <li>handleListGroupsRequest</li> <li>handleOffsetCommitRequest</li> <li>handleOffsetDeleteRequest</li> <li>handleOffsetFetchRequest</li> <li>handleStopReplicaRequest</li> <li>handleSyncGroupRequest</li> <li>handleTxnOffsetCommitRequest</li> <li>handleUpdateMetadataRequest</li> <li>handleWriteTxnMarkersRequest</li> </ul>","text":""},{"location":"KafkaApis/#transactioncoordinator","title":"TransactionCoordinator <p><code>KafkaApis</code> is given a TransactionCoordinator when created.</p> <p>The <code>TransactionCoordinator</code> is used for the following:</p> <ul> <li>handleAddOffsetsToTxnRequest</li> <li>handleAddPartitionToTxnRequest</li> <li>handleEndTxnRequest</li> <li>handleFindCoordinatorRequest</li> <li>handleInitProducerIdRequest</li> <li>handleLeaderAndIsrRequest</li> <li>handleStopReplicaRequest</li> </ul>","text":""},{"location":"KafkaApis/#handlefetchrequest","title":"handleFetchRequest <pre><code>handleFetchRequest(\n  request: RequestChannel.Request): Unit\n</code></pre> <p><code>handleFetchRequest</code> assumes that the given <code>RequestChannel.Request</code> is an <code>FetchRequest</code>.</p> <p><code>handleFetchRequest</code> authorizes the request.</p> <p>In the end, <code>handleFetchRequest</code> requests the ReplicaManager to fetchMessages.</p> <p><code>handleFetchRequest</code>\u00a0is used when:</p> <ul> <li><code>KafkaApis</code> is requested to handle a FETCH request</li> </ul>","text":""},{"location":"KafkaApis/#handlefindcoordinatorrequest","title":"handleFindCoordinatorRequest <pre><code>handleFindCoordinatorRequest(\n  request: RequestChannel.Request): Unit\n</code></pre> <p><code>handleFindCoordinatorRequest</code> converts the given <code>RequestChannel.Request</code> to an <code>FindCoordinatorRequest</code>.</p> <p><code>handleFindCoordinatorRequest</code> finds the group or transaction coordinator.</p> <p>In the end, <code>handleFindCoordinatorRequest</code> prints out the following TRACE message to the logs:</p> <pre><code>Sending FindCoordinator response [response] for correlation id [correlationId] to client [clientId].\n</code></pre>  <p><code>handleFindCoordinatorRequest</code>\u00a0is used when:</p> <ul> <li><code>KafkaApis</code> is requested to handle a FIND_COORDINATOR request</li> </ul>","text":""},{"location":"KafkaApis/#finding-group-or-transaction-coordinator","title":"Finding Group or Transaction Coordinator <pre><code>getCoordinator(\n  request: RequestChannel.Request,\n  keyType: Byte,\n  key: String): (Errors, Node)\n</code></pre>","text":""},{"location":"KafkaApis/#authorization","title":"Authorization <p>For <code>GROUP</code> coordinator (by the <code>keyType</code>), <code>getCoordinator</code> requests the AuthHelper to authorize:</p> <ul> <li><code>DESCRIBE</code> operation</li> <li><code>GROUP</code> resource type</li> <li><code>key</code> resource name</li> </ul> <p>For <code>TRANSACTION</code> coordinator (by the <code>keyType</code>), <code>getCoordinator</code> requests the AuthHelper to authorize:</p> <ul> <li><code>DESCRIBE</code> operation</li> <li><code>TRANSACTIONAL_ID</code> resource type</li> <li><code>key</code> resource name</li> </ul> <p>If either fails, <code>getCoordinator</code> returns the <code>Errors</code>.</p>","text":""},{"location":"KafkaApis/#partition","title":"Partition <p>For <code>GROUP</code> coordinator (by the <code>keyType</code>), <code>getCoordinator</code> requests the GroupCoordinator for the partition of the <code>key</code> group.</p> <p>For <code>TRANSACTION</code> coordinator (by the <code>keyType</code>), <code>getCoordinator</code> requests the TransactionCoordinator for the partition of the <code>key</code> transactional ID.</p>","text":""},{"location":"KafkaApis/#topic-metadata","title":"Topic Metadata <p><code>getCoordinator</code> requests the MetadataCache to getTopicMetadata with the name of the internal topic and then to getAliveBrokerNode.</p> <p>Possible errors are <code>COORDINATOR_NOT_AVAILABLE</code>s.</p>","text":""},{"location":"KafkaApis/#handleinitproduceridrequest","title":"handleInitProducerIdRequest <pre><code>handleInitProducerIdRequest(\n  request: RequestChannel.Request): Unit\n</code></pre> <p><code>handleInitProducerIdRequest</code> assumes that the given <code>RequestChannel.Request</code> is an <code>InitProducerIdRequest</code>.</p> <p><code>handleInitProducerIdRequest</code> authorizes the request.</p> <p>With <code>producerId</code> and <code>producerEpoch</code> set either to <code>-1</code>s (<code>NO_PRODUCER_ID</code> and <code>NO_PRODUCER_EPOCH</code>) or some non-<code>-1</code> values, <code>handleInitProducerIdRequest</code> requests the TransactionCoordinator to handleInitProducerId.</p> <p>Otherwise, <code>handleInitProducerIdRequest</code> sends an error back.</p> <p><code>handleInitProducerIdRequest</code>\u00a0is used when:</p> <ul> <li><code>KafkaApis</code> is requested to handle a INIT_PRODUCER_ID request</li> </ul>","text":""},{"location":"KafkaApis/#handleleaderandisrrequest","title":"handleLeaderAndIsrRequest <pre><code>handleLeaderAndIsrRequest(\n  request: RequestChannel.Request): Unit\n</code></pre> <p>In summary, <code>handleLeaderAndIsrRequest</code> requests the ReplicaManager to become the leader or a follower (of partitions).</p>  <p><code>handleLeaderAndIsrRequest</code> expects the given <code>RequestChannel.Request</code> to be an LeaderAndIsrRequest.</p> <p><code>handleLeaderAndIsrRequest</code> requests the AuthHelper to authorize <code>CLUSTER_ACTION</code> operation.</p> <p>In the end, <code>handleLeaderAndIsrRequest</code> requests the ReplicaManager to become the leader or a follower (of partitions) (with a <code>correlationId</code> and onLeadershipChange handler).</p>  <p><code>handleLeaderAndIsrRequest</code>\u00a0is used when:</p> <ul> <li><code>KafkaApis</code> is requested to handle a LEADER_AND_ISR request</li> </ul>","text":""},{"location":"KafkaApis/#handleincrementalalterconfigsrequest","title":"handleIncrementalAlterConfigsRequest <pre><code>handleIncrementalAlterConfigsRequest(\n  request: RequestChannel.Request): Unit\n</code></pre> <p><code>handleIncrementalAlterConfigsRequest</code>...FIXME</p>  <p><code>handleIncrementalAlterConfigsRequest</code> is used when:</p> <ul> <li><code>KafkaApis</code> is requested to handle a INCREMENTAL_ALTER_CONFIGS request</li> </ul>","text":""},{"location":"KafkaApis/#processincrementalalterconfigsrequest","title":"processIncrementalAlterConfigsRequest <pre><code>processIncrementalAlterConfigsRequest(\n  originalRequest: RequestChannel.Request,\n  data: IncrementalAlterConfigsRequestData): IncrementalAlterConfigsResponseData\n</code></pre> <p><code>processIncrementalAlterConfigsRequest</code>...FIXME</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"KafkaApis/#handling-api-request","title":"Handling API Request <pre><code>handle(\n  request: RequestChannel.Request,\n  requestLocal: RequestLocal): Unit\n</code></pre> <p><code>handle</code> is part of the ApiRequestHandler abstraction.</p>  <p><code>handle</code> prints out the following TRACE message to the logs:</p> <pre><code>Handling request:[request] from connection [id];securityProtocol:[protocol],principal:[principal]\n</code></pre> <p><code>handle</code> handles the given <code>RequestChannel.Request</code> (based on the <code>apiKey</code> in the header) using the corresponding handler.</p>    API Key Handler     LeaderAndIsr handleLeaderAndIsrRequest   <code>INCREMENTAL_ALTER_CONFIGS</code> handleIncrementalAlterConfigsRequest   others","text":""},{"location":"KafkaApis/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>kafka.server.KafkaApis</code> logger to see what happens inside.</p> <p>Add the following line to <code>config/log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.server.KafkaApis=ALL\n</code></pre> <p>Refer to Logging.</p>  <p>Please note that Kafka comes with a preconfigured <code>kafka.server.KafkaApis</code> logger in <code>config/log4j.properties</code>:</p> <pre><code>log4j.appender.requestAppender=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.requestAppender.DatePattern='.'yyyy-MM-dd-HH\nlog4j.appender.requestAppender.File=${kafka.logs.dir}/kafka-request.log\nlog4j.appender.requestAppender.layout=org.apache.log4j.PatternLayout\nlog4j.appender.requestAppender.layout.ConversionPattern=[%d] %p %m (%c)%n\n\nlog4j.logger.kafka.server.KafkaApis=TRACE, requestAppender\nlog4j.additivity.kafka.server.KafkaApis=false\n</code></pre> <p>That means that the logs of <code>KafkaApis</code> go to <code>logs/kafka-request.log</code> file at <code>TRACE</code> logging level and are not added to the main logs (per <code>log4j.additivity</code> being off).</p>","text":""},{"location":"KafkaConfig/","title":"KafkaConfig","text":"<p><code>KafkaConfig</code> is configuration properties of a Kafka broker.</p> <p><code>KafkaConfig</code> is a AbstractConfig.</p>"},{"location":"KafkaConfig/#accessing-kafkaconfig","title":"Accessing KafkaConfig","text":"<pre><code>import kafka.server.KafkaConfig\nimport java.util.Properties\nval props = new Properties()\nprops.put(\"zookeeper.connect\", \":2181\") // a required property\nval config = KafkaConfig.fromProps(props, doLog = true)\nassert(config.uncleanLeaderElectionEnable == false)\n</code></pre>"},{"location":"KafkaConfig/#authorizerclassname","title":"authorizer.class.name <p>The fully-qualified name of a class that implements Authorizer interface, which is used by the broker for request authorization.</p> <p>Default: empty</p> <p>Use <code>KafkaConfig.authorizerClassName</code> to access the current value.</p> <p>Used when:</p> <ul> <li><code>BrokerServer</code> is requested to start up</li> <li><code>ControllerServer</code> is requested to start up</li> <li><code>KafkaServer</code> is requested to start up</li> </ul>","text":""},{"location":"KafkaConfig/#brokerid","title":"broker.id <p>The broker ID of this Kafka server.</p> <p>Default: <code>-1</code></p> <p>If unset or negative, a unique broker id will be generated (when <code>KafkaServer</code> is requested to start up).</p> <p>To avoid conflicts between zookeeper generated broker id's and user configured broker id's, generated broker ids start from reserved.broker.max.id + 1</p> <p>Use <code>KafkaConfig.brokerId</code> to access the current value.</p>  <pre><code>import kafka.server.KafkaConfig\n// For some reason zookeeper.connect is required?!\nval m = Map(\n  \"zookeeper.connect\" -&gt; \"xxx\"\n)\nval props = new java.util.Properties()\nimport scala.jdk.CollectionConverters._\nprops.putAll(m.asJava)\nval config = KafkaConfig.fromProps(props)\nassert(config.brokerId == -1)\n</code></pre>","text":""},{"location":"KafkaConfig/#brokeridgenerationenable","title":"broker.id.generation.enable <p>Enables broker id generation on a server. When enabled, reserved.broker.max.id should be reviewed.</p> <p>Default: <code>true</code></p> <p>Use <code>brokerIdGenerationEnable</code> to access the current value.</p>","text":""},{"location":"KafkaConfig/#defaultreplicationfactor","title":"default.replication.factor","text":""},{"location":"KafkaConfig/#interbrokerprotocolversion","title":"inter.broker.protocol.version <p>Specify which version of the inter-broker protocol to use. Typically bumped up after all brokers were upgraded to a new version.</p> <p>Default: The latest version of <code>ApiVersion</code> of the broker</p>","text":""},{"location":"KafkaConfig/#logcleanerenable","title":"log.cleaner.enable <p>Enables LogCleaner</p> <p>Default: <code>true</code></p> <ul> <li>Should be enabled if using any topics with a cleanup.policy=compact including the internal offsets topic</li> <li>If disabled those topics will not be compacted and continually grow in size.</li> </ul> <p>Used when:</p> <ul> <li><code>CleanerConfig</code> is created</li> </ul>","text":""},{"location":"KafkaConfig/#logcleanerthreads","title":"log.cleaner.threads <p>The number of background threads to use for log cleaning (by LogCleaner)</p> <p>Default: <code>1</code></p> <p>Must be at least <code>0</code></p> <p>Reconfigurable Config</p> <p>Used when:</p> <ul> <li><code>CleanerConfig</code> is created</li> </ul>","text":""},{"location":"KafkaConfig/#logcleanerbackoffms","title":"log.cleaner.backoff.ms <p>How long to pause a CleanerThread (until next log cleaning attempt) when there are no logs to clean</p> <p>Default: <code>15 * 1000</code></p> <p>Must be at least <code>0</code></p> <p>Reconfigurable Config</p> <p>Used when:</p> <ul> <li><code>CleanerConfig</code> is created</li> </ul>","text":""},{"location":"KafkaConfig/#metadataLogDir","title":"metadata.log.dir <p>The directory of the metadata log of a Kafka cluster in KRaft mode.</p> <p>Unless specified, the metadata log is placed in the first log directory from log.dirs.</p> <p>Default: (unspecified)</p> <p>Available as metadataLogDir</p>","text":""},{"location":"KafkaConfig/#numpartitions","title":"num.partitions","text":""},{"location":"KafkaConfig/#offsetstopicnumpartitions","title":"offsets.topic.num.partitions <p>The number of partitions for <code>__consumer_offsets</code> offset commit topic (should not change after deployment)</p> <p>For every partition there is a <code>GroupCoordinator</code> elected to handle consumer groups that are \"assigned\" to this partition.</p> <p>Default: <code>50</code></p> <p>Must be at least 1</p> <p>Use <code>KafkaConfig.offsetsTopicPartitions</code> to access the current value.</p> <p>Used when:</p> <ul> <li><code>GroupCoordinator</code> is requested to create an OffsetConfig</li> <li><code>DefaultAutoTopicCreationManager</code> is requested to creatableTopic</li> <li><code>KafkaServer</code> is requested to start up (and starts up the GroupCoordinator)</li> <li><code>BrokerMetadataPublisher</code> is requested to <code>initializeManagers</code> (and starts up the <code>GroupCoordinator</code>)</li> </ul>","text":""},{"location":"KafkaConfig/#offsetstopicreplicationfactor","title":"offsets.topic.replication.factor","text":""},{"location":"KafkaConfig/#controller.quorum.voters","title":"controller.quorum.voters <p>controller.quorum.voters</p>","text":""},{"location":"KafkaConfig/#process.roles","title":"process.roles <p>A comma-separated list of the roles that this Kafka server plays in a Kafka cluster:</p> <p>Supported values:</p> <ul> <li><code>broker</code></li> <li><code>controller</code></li> <li><code>broker,controller</code></li> </ul> <p>Default: (empty)</p> <ol> <li>When empty, the process requires Zookeeper (runs with Zookeeper).</li> <li>Only applicable for clusters in KRaft (Kafka Raft) mode</li> <li>If used, controller.quorum.voters must contain a parseable set of voters</li> <li>advertised.listeners config must not contain KRaft controller listeners from controller.listener.names when <code>process.roles</code> contains <code>broker</code> role because Kafka clients that send requests via advertised listeners do not send requests to KRaft controllers -- they only send requests to KRaft brokers</li> <li>If <code>process.roles</code> contains <code>controller</code> role, the node.id must be included in the set of voters controller.quorum.voters</li> <li>If <code>process.roles</code> contains just the <code>broker</code> role, the node.id must not be included in the set of voters controller.quorum.voters</li> <li>If controller.listener.names has multiple entries; only the first will be used when <code>process.roles</code> is <code>broker</code></li> <li>The advertised listeners (advertised.listeners or listeners) config must only contain KRaft controller listeners from controller.listener.names when <code>process.roles</code> is <code>controller</code></li> </ol>","text":""},{"location":"KafkaConfig/#requesttimeoutms","title":"request.timeout.ms <p>request.timeout.ms</p>","text":""},{"location":"KafkaConfig/#transactionaborttimedouttransactioncleanupintervalms","title":"transaction.abort.timed.out.transaction.cleanup.interval.ms","text":""},{"location":"KafkaConfig/#transactionalidexpirationms","title":"transactional.id.expiration.ms","text":""},{"location":"KafkaConfig/#transactionmaxtimeoutms","title":"transaction.max.timeout.ms","text":""},{"location":"KafkaConfig/#transactionremoveexpiredtransactioncleanupintervalms","title":"transaction.remove.expired.transaction.cleanup.interval.ms","text":""},{"location":"KafkaConfig/#transactionstatelognumpartitions","title":"transaction.state.log.num.partitions <p>The number of partitions for the transaction topic</p> <p>Default: 50</p> <p>Must be at least 1</p>","text":""},{"location":"KafkaConfig/#transactionstatelogreplicationfactor","title":"transaction.state.log.replication.factor","text":""},{"location":"KafkaConfig/#transactionstatelogsegmentbytes","title":"transaction.state.log.segment.bytes","text":""},{"location":"KafkaConfig/#transactionstatelogloadbuffersize","title":"transaction.state.log.load.buffer.size","text":""},{"location":"KafkaConfig/#transactionstatelogminisr","title":"transaction.state.log.min.isr","text":""},{"location":"KafkaConfig/#uncleanleaderelectionenable","title":"unclean.leader.election.enable <p>Enables replicas not in the ISR to be elected as leaders as a last resort, even though it is not guaranteed to have every committed message (and may even result in data loss).</p> <p>It is to support use cases where uptime and availability are preferable over consistency and allow non-in-sync replicas to become partition leaders.</p> <p>Default: <code>false</code> (disabled)</p> <p>Unclean leader election is automatically enabled by the controller when this config is dynamically updated by using per-topic config override.</p> <p>Use <code>KafkaConfig.uncleanLeaderElectionEnable</code> to access the current value.</p> <p>Per-topic configuration: unclean.leader.election.enable</p> <p>Used when:</p> <ul> <li><code>TopicConfigHandler</code> is requested to processConfigChanges (to enableTopicUncleanLeaderElection on an active controller)</li> </ul>","text":""},{"location":"KafkaConfig/#utilities","title":"Utilities","text":""},{"location":"KafkaConfig/#dynamicbrokerconfig","title":"DynamicBrokerConfig <pre><code>dynamicConfig: DynamicBrokerConfig\n</code></pre> <p><code>KafkaConfig</code> initializes <code>dynamicConfig</code> when created (based on the optionaldynamicConfigOverride).</p> <p>The <code>DynamicBrokerConfig</code> is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"KafkaConfig/#interbrokerprotocolversion_1","title":"interBrokerProtocolVersion <pre><code>interBrokerProtocolVersion: ApiVersion\n</code></pre> <p><code>interBrokerProtocolVersion</code> creates a <code>ApiVersion</code> for the inter.broker.protocol.version.</p>","text":""},{"location":"KafkaConfig/#requireszookeeper","title":"requiresZookeeper <pre><code>requiresZookeeper: Boolean\n</code></pre> <p><code>requiresZookeeper</code> is <code>true</code> when process.roles is empty.</p>","text":""},{"location":"KafkaConfig/#creating-instance","title":"Creating Instance <p><code>KafkaConfig</code> takes the following to be created:</p> <ul> <li> <code>doLog</code> flag <li> Properties <li>DynamicBrokerConfig</li>  <p><code>KafkaConfig</code> is created when:</p> <ul> <li><code>DynamicBrokerConfig</code> is requested to initialize, reloadUpdatedFilesWithoutConfigChange, processReconfiguration</li> <li><code>KafkaConfig</code> is requested to fromProps, apply</li> </ul>","text":""},{"location":"KafkaConfig/#dynamicconfigoverride","title":"dynamicConfigOverride <p><code>KafkaConfig</code> can be given a DynamicBrokerConfig when created.</p>  <p>Note</p> <p><code>DynamicBrokerConfig</code> seems never be given.</p>  <p><code>KafkaConfig</code> creates a new <code>DynamicBrokerConfig</code> for dynamicConfig unless given.</p>","text":""},{"location":"KafkaConfig/#creating-kafkaconfig-instance","title":"Creating KafkaConfig Instance","text":""},{"location":"KafkaConfig/#fromprops","title":"fromProps <pre><code>fromProps(\n  props: Properties): KafkaConfig\nfromProps(\n  props: Properties,\n  doLog: Boolean): KafkaConfig\nfromProps(\n  defaults: Properties,\n  overrides: Properties): KafkaConfig\nfromProps(\n  defaults: Properties,\n  overrides: Properties,\n  doLog: Boolean): KafkaConfig\n</code></pre> <p><code>fromProps</code>...FIXME</p>  <p><code>fromProps</code> is used when:</p> <ul> <li><code>Kafka</code> is requested to build a Server</li> <li><code>AclAuthorizer</code> is requested to configure</li> </ul>","text":""},{"location":"KafkaConfig/#apply","title":"apply <pre><code>apply(\n  props: Map[_, _],\n  doLog: Boolean = true): KafkaConfig\n</code></pre> <p><code>apply</code>...FIXME</p>  <p><code>apply</code> seems to be used for testing only.</p>","text":""},{"location":"KafkaConfig/#metadataLogDir","title":"metadataLogDir <pre><code>metadataLogDir: String\n</code></pre> <p><code>metadataLogDir</code> is the value of metadata.log.dir, if defined, or the first directory from logDirs.</p>  <p><code>metadataLogDir</code> is used when:</p> <ul> <li><code>KafkaRaftManager</code> is created and requested to createDataDir</li> <li><code>KafkaRaftServer</code> is requested to initializeLogDirs</li> <li><code>StorageTool</code> is requested to configToLogDirectories</li> </ul>","text":""},{"location":"Log/","title":"Log","text":""},{"location":"Log/#creating-instance","title":"Creating Instance","text":"<p><code>Log</code> takes the following to be created:</p> <ul> <li> Directory <li> <code>LogConfig</code> <li> <code>LogSegments</code> <li> logStartOffset <li> recoveryPoint <li> <code>LogOffsetMetadata</code> <li> <code>Scheduler</code> <li> <code>BrokerTopicStats</code> <li> <code>Time</code> <li> producerIdExpirationCheckIntervalMs <li> <code>TopicPartition</code> <li> Optional <code>LeaderEpochFileCache</code> <li> <code>ProducerStateManager</code> <li> <code>LogDirFailureChannel</code> <li> Optional Topic ID <li> keepPartitionMetadataFile <p><code>Log</code> is created\u00a0using apply utility.</p>"},{"location":"Log/#creating-log","title":"Creating Log <pre><code>apply(\n  dir: File,\n  config: LogConfig,\n  logStartOffset: Long,\n  recoveryPoint: Long,\n  scheduler: Scheduler,\n  brokerTopicStats: BrokerTopicStats,\n  time: Time = Time.SYSTEM,\n  maxProducerIdExpirationMs: Int,\n  producerIdExpirationCheckIntervalMs: Int,\n  logDirFailureChannel: LogDirFailureChannel,\n  lastShutdownClean: Boolean = true,\n  topicId: Option[Uuid],\n  keepPartitionMetadataFile: Boolean): Log\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>LogManager</code> is requested to <code>loadLog</code> and <code>getOrCreateLog</code></li> <li><code>KafkaMetadataLog</code> is requested to <code>apply</code></li> </ul>","text":""},{"location":"Log/#read","title":"Reading Messages <pre><code>read(\n  startOffset: Long,\n  maxLength: Int,\n  isolation: FetchIsolation,\n  minOneMessage: Boolean): FetchDataInfo\n</code></pre> <p><code>read</code> prints out the following TRACE message to the logs:</p> <pre><code>Reading maximum [maxLength] bytes at offset [startOffset] from log with total length [size] bytes\n</code></pre> <p><code>read</code>...FIXME</p> <p><code>read</code> requests the <code>LogSegment</code> to read messages.</p> <p><code>read</code>...FIXME</p>  <p><code>read</code>\u00a0is used when:</p> <ul> <li><code>Partition</code> is requested to readRecords</li> <li><code>GroupMetadataManager</code> is requested to <code>doLoadGroupsAndOffsets</code></li> <li><code>TransactionStateManager</code> is requested to <code>loadTransactionMetadata</code></li> <li><code>Log</code> is requested to convertToOffsetMetadataOrThrow</li> <li><code>KafkaMetadataLog</code> is requested to <code>read</code></li> </ul>","text":""},{"location":"Log/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>kafka.log.Log</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.log.Log=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"MetadataCache/","title":"MetadataCache","text":"<p><code>MetadataCache</code> is...FIXME</p>"},{"location":"Partition/","title":"Partition","text":""},{"location":"Partition/#readrecords","title":"readRecords <pre><code>readRecords(\n  lastFetchedEpoch: Optional[Integer],\n  fetchOffset: Long,\n  currentLeaderEpoch: Optional[Integer],\n  maxBytes: Int,\n  fetchIsolation: FetchIsolation,\n  fetchOnlyFromLeader: Boolean,\n  minOneMessage: Boolean): LogReadInfo\n</code></pre> <p><code>readRecords</code>...FIXME</p> <p>In the end, <code>readRecords</code> requests the <code>Log</code> to read messages.</p>  <p><code>readRecords</code>\u00a0is used when:</p> <ul> <li><code>ReplicaManager</code> is requested to readFromLocalLog</li> </ul>","text":""},{"location":"Partition/#makeleader","title":"makeLeader <pre><code>makeLeader(\n  partitionState: LeaderAndIsrPartitionState,\n  highWatermarkCheckpoints: OffsetCheckpoints,\n  topicId: Option[Uuid]): Boolean\n</code></pre> <p><code>makeLeader</code>...FIXME</p>  <p><code>makeLeader</code> is used when:</p> <ul> <li><code>ReplicaManager</code> is requested to makeLeaders, applyLocalLeadersDelta</li> </ul>","text":""},{"location":"Partition/#createlogifnotexists","title":"createLogIfNotExists <pre><code>createLogIfNotExists(\n  isNew: Boolean,\n  isFutureReplica: Boolean,\n  offsetCheckpoints: OffsetCheckpoints,\n  topicId: Option[Uuid]): Unit\n</code></pre> <p><code>createLogIfNotExists</code>...FIXME</p>  <p><code>createLogIfNotExists</code>\u00a0is used when:</p> <ul> <li><code>Partition</code> is requested to maybeCreateFutureReplica, makeLeader, makeFollower</li> <li><code>ReplicaManager</code> is requested to maybeAddLogDirFetchers, makeFollowers, applyLocalFollowersDelta</li> </ul>","text":""},{"location":"Partition/#createlog","title":"createLog <pre><code>createLog(\n  isNew: Boolean,\n  isFutureReplica: Boolean,\n  offsetCheckpoints: OffsetCheckpoints,\n  topicId: Option[Uuid]): UnifiedLog\n</code></pre> <p><code>createLog</code>...FIXME</p>","text":""},{"location":"PartitionData/","title":"PartitionData","text":"<p><code>PartitionData</code> is a <code>Message</code> of FetchResponseData.</p>"},{"location":"PartitionData/#abortedtransactions","title":"abortedTransactions <pre><code>List&lt;AbortedTransaction&gt; abortedTransactions\nList&lt;AbortedTransaction&gt; abortedTransactions()\n</code></pre> <p><code>abortedTransactions</code>...FIXME</p> <p><code>abortedTransactions</code>\u00a0is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"PartitionData/#preferred-read-replica","title":"Preferred Read Replica <p>Default: <code>-1</code></p> <p><code>preferredReadReplica</code>\u00a0is used when:</p> <ul> <li><code>Fetcher</code> is requested to initializeCompletedFetch</li> </ul>","text":""},{"location":"ReplicaAlterLogDirsThread/","title":"ReplicaAlterLogDirsThread","text":"<p><code>ReplicaAlterLogDirsThread</code> is...FIXME</p>"},{"location":"ReplicaManager/","title":"ReplicaManager","text":""},{"location":"ReplicaManager/#fetchmessages","title":"fetchMessages <pre><code>fetchMessages(\n  timeout: Long,\n  replicaId: Int,\n  fetchMinBytes: Int,\n  fetchMaxBytes: Int,\n  hardMaxBytesLimit: Boolean,\n  fetchInfos: Seq[(TopicPartition, PartitionData)],\n  quota: ReplicaQuota,\n  responseCallback: Seq[(TopicPartition, FetchPartitionData)] =&gt; Unit,\n  isolationLevel: IsolationLevel,\n  clientMetadata: Option[ClientMetadata]): Unit\n</code></pre> <p><code>fetchMessages</code> determines whether the request comes from a follower or a consumer (based on the given <code>replicaId</code>).</p> <p><code>fetchMessages</code> determines <code>FetchIsolation</code>:</p> <ul> <li><code>FetchLogEnd</code> if the request comes from a follower</li> <li><code>FetchTxnCommitted</code> if the request comes from a consumer with <code>READ_COMMITTED</code> isolation level</li> <li><code>FetchHighWatermark</code> otherwise</li> </ul> <p><code>fetchMessages</code> readFromLocalLog (passing in the <code>FetchIsolation</code> among the others).</p> <p><code>fetchMessages</code>...FIXME</p> <p><code>fetchMessages</code>\u00a0is used when:</p> <ul> <li><code>KafkaApis</code> is requested to handle a Fetch request</li> <li><code>ReplicaAlterLogDirsThread</code> is requested to fetchFromLeader</li> </ul>","text":""},{"location":"ReplicaManager/#readfromlocallog","title":"readFromLocalLog <pre><code>readFromLocalLog(\n  replicaId: Int,\n  fetchOnlyFromLeader: Boolean,\n  fetchIsolation: FetchIsolation,\n  fetchMaxBytes: Int,\n  hardMaxBytesLimit: Boolean,\n  readPartitionInfo: Seq[(TopicPartition, PartitionData)],\n  quota: ReplicaQuota,\n  clientMetadata: Option[ClientMetadata]): Seq[(TopicPartition, LogReadResult)]\n</code></pre> <p><code>readFromLocalLog</code>...FIXME</p> <p><code>readFromLocalLog</code> finds the <code>Partition</code> and requests it to readRecords.</p> <p><code>readFromLocalLog</code>...FIXME</p> <p><code>readFromLocalLog</code>\u00a0is used when:</p> <ul> <li><code>DelayedFetch</code> is requested to <code>onComplete</code></li> <li><code>ReplicaManager</code> is requested to fetchMessages</li> </ul>","text":""},{"location":"ReplicaManager/#becomeleaderorfollower","title":"becomeLeaderOrFollower <pre><code>becomeLeaderOrFollower(\n  correlationId: Int,\n  leaderAndIsrRequest: LeaderAndIsrRequest,\n  onLeadershipChange: (Iterable[Partition], Iterable[Partition]) =&gt; Unit): LeaderAndIsrResponse\n</code></pre> <p><code>becomeLeaderOrFollower</code>...FIXME</p>  <p><code>becomeLeaderOrFollower</code> is used when:</p> <ul> <li><code>KafkaApis</code> is requested to handleLeaderAndIsrRequest</li> </ul>","text":""},{"location":"ReplicaManager/#makeleaders","title":"makeLeaders <pre><code>makeLeaders(\n  controllerId: Int,\n  controllerEpoch: Int,\n  partitionStates: Map[Partition, LeaderAndIsrPartitionState],\n  correlationId: Int,\n  responseMap: mutable.Map[TopicPartition, Errors],\n  highWatermarkCheckpoints: OffsetCheckpoints,\n  topicIds: String =&gt; Option[Uuid]): Set[Partition]\n</code></pre> <p><code>makeLeaders</code>...FIXME</p>","text":""},{"location":"RequestHandlerHelper/","title":"RequestHandlerHelper","text":""},{"location":"RequestHandlerHelper/#creating-instance","title":"Creating Instance","text":"<p><code>RequestHandlerHelper</code> takes the following to be created:</p> <ul> <li> <code>RequestChannel</code> <li> <code>QuotaManagers</code> <li> <code>Time</code> <li> Log Prefix <p><code>RequestHandlerHelper</code> is created\u00a0when:</p> <ul> <li><code>ControllerApis</code> is created (<code>requestHelper</code>)</li> <li><code>KafkaApis</code> is created</li> </ul>"},{"location":"RequestHandlerHelper/#onleadershipchange","title":"onLeadershipChange <pre><code>onLeadershipChange(\n  groupCoordinator: GroupCoordinator,\n  txnCoordinator: TransactionCoordinator,\n  updatedLeaders: Iterable[Partition],\n  updatedFollowers: Iterable[Partition]): Unit\n</code></pre> <p><code>onLeadershipChange</code>...FIXME</p> <p><code>onLeadershipChange</code>\u00a0is used when:</p> <ul> <li><code>BrokerServer</code> is requested to start up</li> <li><code>KafkaApis</code> is requested to handleLeaderAndIsrRequest</li> <li><code>BrokerMetadataListener</code> is requested to <code>handleCommits</code> and <code>execCommits</code></li> </ul>","text":""},{"location":"Serde/","title":"Serde","text":"<p><code>Serde</code> is an abstraction of wrappers with Serializers and Deserializers.</p> <p>Note</p> <p><code>Serde</code> seems to be of more use in libraries like Kafka Streams or Kafka Connect (that in the Kafka Core).</p>"},{"location":"Serde/#contract","title":"Contract","text":""},{"location":"Serde/#configure","title":"configure <pre><code>void configure(\n  Map&lt;String, ?&gt; configs,\n  boolean isKey)\n</code></pre>","text":""},{"location":"Serde/#deserializer","title":"deserializer <pre><code>Deserializer&lt;T&gt; deserializer()\n</code></pre>","text":""},{"location":"Serde/#serializer","title":"serializer <pre><code>Serializer&lt;T&gt; serializer()\n</code></pre>","text":""},{"location":"Serde/#implementations","title":"Implementations","text":"<ul> <li>WrapperSerde</li> </ul>"},{"location":"Serdes/","title":"Serdes Utility","text":"<p><code>Serdes</code> is a utility with the serializers and deserializers for many built-in types in Java and allows defining new ones.</p> <pre><code>import org.apache.kafka.common.serialization.Serdes\n\nval longSerde = Serdes.Long\nscala&gt; :type longSerde\norg.apache.kafka.common.serialization.Serde[Long]\n\nscala&gt; :type longSerde.serializer\norg.apache.kafka.common.serialization.Serializer[Long]\n\nscala&gt; :type longSerde.deserializer\norg.apache.kafka.common.serialization.Deserializer[Long]\n</code></pre>"},{"location":"Serdes/#wrapperserde","title":"WrapperSerde <p><code>Serdes</code> defines a <code>static public class WrapperSerde&lt;T&gt;</code> that is a Serde from and to <code>T</code> values.</p> <ul> <li><code>ShortSerde</code></li> <li><code>BytesSerde</code></li> <li><code>IntegerSerde</code></li> <li><code>ListSerde</code></li> <li><code>UUIDSerde</code></li> <li><code>FloatSerde</code></li> <li><code>FullTimeWindowedSerde</code> (Kafka Streams)</li> <li><code>VoidSerde</code></li> <li><code>LongSerde</code></li> <li><code>DoubleSerde</code></li> <li><code>ByteArraySerde</code></li> <li><code>TimeWindowedSerde</code> (Kafka Streams)</li> <li><code>SessionWindowedSerde</code> (Kafka Streams)</li> <li><code>ByteBufferSerde</code></li> <li><code>StringSerde</code></li> </ul>","text":""},{"location":"Serdes/#serdefrom","title":"serdeFrom <pre><code>Serde&lt;T&gt; serdeFrom(\n  Class&lt;T&gt; type)\nSerde&lt;T&gt; serdeFrom(\n  Serializer&lt;T&gt; serializer,\n  Deserializer&lt;T&gt; deserializer)\n</code></pre> <p><code>serdeFrom</code> looks up the Serde for a given type (if supported) or creates a new one based on the given pair of <code>Serializer</code> and <code>Deserializer</code>.</p>","text":""},{"location":"Server/","title":"Server","text":"<p><code>Server</code> is an abstraction of Kafka servers (brokers) for Kafka utility.</p>"},{"location":"Server/#contract","title":"Contract","text":""},{"location":"Server/#awaitshutdown","title":"awaitShutdown <pre><code>awaitShutdown(): Unit\n</code></pre> <p>Awaits termination signal (and keeps this server alive)</p> <p>Used when:</p> <ul> <li><code>Kafka</code> utility is executed (on command line)</li> </ul>","text":""},{"location":"Server/#starting-up","title":"Starting Up <pre><code>startup(): Unit\n</code></pre> <p>Starts up this server</p> <p>Used when:</p> <ul> <li><code>Kafka</code> utility is executed (on command line)</li> </ul>","text":""},{"location":"Server/#shutdown","title":"shutdown <pre><code>shutdown(): Unit\n</code></pre> <p>Shuts down this server (as part of <code>kafka-shutdown-hook</code> shutdown hook)</p> <p>Used when:</p> <ul> <li><code>Kafka</code> utility is executed (on command line and handles termination signals)</li> </ul>","text":""},{"location":"Server/#implementations","title":"Implementations","text":"<ul> <li>KafkaRaftServer</li> <li>KafkaServer</li> </ul>"},{"location":"TopicConfig/","title":"TopicConfig","text":"<p><code>TopicConfig</code> is configuration properties of Kafka topics. In other words, <code>TopicConfig</code> is a topic-specific configuration properties (while KafkaConfig is broker-wide).</p> <p>Tip</p> <p>While reviewing the source code it can get tricky to find broker-wide properties (e.g. <code>log.cleanup.policy</code>). The reason is that broker-wide properties as split into <code>log.</code> prefix and a corresponding topic-specific property.</p> <p>A solution is to search for a topic-specific property removing <code>log.</code> prefix.</p>"},{"location":"TopicConfig/#cleanuppolicy","title":"cleanup.policy <p>A comma-separated list of cleanup policies:</p> <ul> <li><code>compact</code></li> <li><code>delete</code></li> </ul> <p>Default: <code>delete</code></p> <p>Broker-wide configuration: log.cleanup.policy</p> <p>Used when:</p> <ul> <li><code>GroupCoordinator</code> is requested for the offsetsTopicConfigs</li> <li><code>LogConfig</code> is requested to compact, delete, TopicConfigSynonyms and extractLogConfigMap</li> <li><code>TopicBasedRemoteLogMetadataManager</code> is requested to <code>createRemoteLogMetadataTopicRequest</code></li> <li><code>TransactionStateManager</code> is requested for the transactionTopicConfigs</li> </ul>","text":""},{"location":"TopicConfig/#uncleanleaderelectionenable","title":"unclean.leader.election.enable <p>LogConfig</p>","text":""},{"location":"Utils/","title":"Utils","text":""},{"location":"Utils/#murmur2","title":"murmur2 <pre><code>int murmur2(\n  byte[] data)\n</code></pre> <p><code>murmur2</code> generates a 32-bit murmur2 hash for the given byte array.</p> <p><code>murmur2</code>\u00a0is used when:</p> <ul> <li><code>DefaultPartitioner</code> is requested to compute a partition for a record</li> </ul>","text":""},{"location":"Utils/#demo","title":"Demo <pre><code>import org.apache.kafka.common.utils.Utils\n\nval keyBytes = \"hello\".getBytes\nval hash = Utils.murmur2(keyBytes)\n\nprintln(hash)\n</code></pre>","text":""},{"location":"Utils/#topositive","title":"toPositive <pre><code>int toPositive(\n  int number)\n</code></pre> <p><code>toPositive</code> converts a number to a positive value.</p>","text":""},{"location":"ZkAdminManager/","title":"ZkAdminManager","text":""},{"location":"ZkAdminManager/#incrementalalterconfigs","title":"incrementalAlterConfigs <pre><code>incrementalAlterConfigs(\n  configs: Map[ConfigResource,\n  Seq[AlterConfigOp]],\n  validateOnly: Boolean): Map[ConfigResource, ApiError]\n</code></pre> <p><code>incrementalAlterConfigs</code>...FIXME</p>  <p><code>incrementalAlterConfigs</code> is used when:</p> <ul> <li><code>KafkaApis</code> is requested to processIncrementalAlterConfigsRequest</li> </ul>","text":""},{"location":"ZkAdminManager/#alterconfigs","title":"alterConfigs <pre><code>alterConfigs(\n  configs: Map[ConfigResource,\n  AlterConfigsRequest.Config],\n  validateOnly: Boolean): Map[ConfigResource, ApiError]\n</code></pre> <p><code>alterConfigs</code>...FIXME</p>  <p><code>alterConfigs</code> is used when:</p> <ul> <li><code>KafkaApis</code> is requested to processLegacyAlterConfigsRequest</li> </ul>","text":""},{"location":"ZkAdminManager/#altertopicconfigs","title":"alterTopicConfigs <pre><code>alterTopicConfigs(\n  resource: ConfigResource,\n  validateOnly: Boolean,\n  configProps: Properties,\n  configEntriesMap: Map[String, String]): (ConfigResource, ApiError)\n</code></pre> <p><code>alterTopicConfigs</code>...FIXME</p>  <p><code>alterTopicConfigs</code> is used when:</p> <ul> <li><code>ZkAdminManager</code> is requested to alterConfigs (of a topic), incrementalAlterConfigs (of a topic)</li> </ul>","text":""},{"location":"ZkAdminManager/#creating-topics","title":"Creating Topics <pre><code>createTopics(\n  timeout: Int,\n  validateOnly: Boolean,\n  toCreate: Map[String, CreatableTopic],\n  includeConfigsAndMetadata: Map[String, CreatableTopicResult],\n  controllerMutationQuota: ControllerMutationQuota,\n  responseCallback: Map[String, ApiError] =&gt; Unit): Unit\n</code></pre> <p><code>createTopics</code>...FIXME</p>  <p><code>createTopics</code> is used when:</p> <ul> <li><code>DefaultAutoTopicCreationManager</code> is requested to createTopicsInZk</li> <li><code>KafkaApis</code> is requested to handleCreateTopicsRequest</li> </ul>","text":""},{"location":"authentication/","title":"Authentication","text":"<p>Kafka Authentication is based on the following:</p> <ul> <li>security.protocol configuration property</li> <li>ssl.client.auth configuration property</li> </ul> <p>For SSL with client authentication enabled, <code>TransportLayer#handshake()</code> performs authentication. For SASL, authentication is performed by <code>Authenticator#authenticate()</code>.</p> <p>For SSL authentication,  the principal will be derived using the rules defined by ssl.principal.mapping.rules applied on the distinguished name from the client certificate if one is provided. Otherwise, if client authentication is not required, the principal name will be <code>ANONYMOUS</code>.</p> <p>For <code>PLAINTEXT</code> listeners or when client authentication is not required, the principal will always be <code>ANONYMOUS</code>.</p>"},{"location":"building-from-sources/","title":"Building from Sources","text":"<p>Based on README.md:</p> <pre><code>KAFKA_VERSION=3.6.0\nSCALA_VERSION=2.13\n</code></pre> <pre><code>$ java -version\nopenjdk version \"11.0.12\" 2021-07-20\nOpenJDK Runtime Environment Temurin-11.0.12+7 (build 11.0.12+7)\nOpenJDK 64-Bit Server VM Temurin-11.0.12+7 (build 11.0.12+7, mixed mode)\n</code></pre> <pre><code>./gradlew clean releaseTarGz install -PskipSigning=true &amp;&amp; \\\ntar -zxvf core/build/distributions/kafka_$SCALA_VERSION-$KAFKA_VERSION.tgz\n</code></pre> <pre><code>cd kafka_$SCALA_VERSION-$KAFKA_VERSION\n</code></pre> <pre><code>$ ./bin/kafka-server-start.sh --version | tail -1\n3.0.0 (Commit:8cb0a5e9d3441962)\n</code></pre>"},{"location":"logging/","title":"Logging","text":""},{"location":"logging/#brokers","title":"Brokers","text":"<p>Kafka brokers use Apache Log4j 2 for logging and use <code>config/log4j.properties</code> by default.</p> <p>The default logging level is <code>INFO</code> with <code>stdout</code> appender.</p> <pre><code>log4j.rootLogger=INFO, stdout, kafkaAppender\n\nlog4j.logger.kafka=INFO\nlog4j.logger.org.apache.kafka=INFO\n</code></pre>"},{"location":"logging/#tools","title":"Tools","text":"<p>Kafka tools (e.g. <code>kafka-console-producer</code>) use <code>config/tools-log4j.properties</code> as the logging configuration file.</p>"},{"location":"logging/#clients","title":"Clients","text":""},{"location":"logging/#buildsbt","title":"build.sbt","text":"<pre><code>libraryDependencies += \"org.apache.kafka\" % \"kafka-clients\" % \"2.8.0\"\n\nval slf4j = \"1.7.32\"\nlibraryDependencies += \"org.slf4j\" % \"slf4j-api\" % slf4j\nlibraryDependencies += \"org.slf4j\" % \"slf4j-log4j12\" % slf4j\n</code></pre>"},{"location":"logging/#log4jproperties","title":"log4j.properties","text":"<p>In <code>src/main/resources/log4j.properties</code> use the following:</p> <pre><code>log4j.rootLogger=INFO, stdout\n\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n\n\nlog4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL\n</code></pre>"},{"location":"overview/","title":"Apache Kafka","text":"<p>Apache Kafka is an open source project for a distributed publish-subscribe messaging system rethought as a distributed commit log.</p>"},{"location":"overview/#messages","title":"Messages","text":"<p>Messages (records, events) are byte arrays (String, JSON, and Avro are among the most common formats). If a message has a key, Kafka (uses Partitioner) to make sure that all messages of the same key are in the same partition.</p>"},{"location":"overview/#topics","title":"Topics","text":"<p>Kafka stores messages in topics that are partitioned and replicated across multiple brokers in a cluster.</p>"},{"location":"overview/#kafka-clients","title":"Kafka Clients","text":"<p>Producers send messages to topics from which consumers read.</p>"},{"location":"overview/#language-agnostic","title":"Language Agnostic","text":"<p>Kafka clients use binary protocol to talk to a Kafka cluster.</p>"},{"location":"overview/#consumer-groups","title":"Consumer Groups","text":"<p>Consumers may be grouped in a consumer group with multiple consumers. Each consumer in a consumer group will read messages from a unique subset of partitions in each topic they subscribe to. Each message is delivered to one consumer in the group, and all messages with the same key arrive to the same consumer.</p>"},{"location":"overview/#durability","title":"Durability","text":"<p>Kafka does not track which messages were read by consumers. Kafka keeps all messages for a finite amount of time, and it is consumers' responsibility to track their location per topic (offsets).</p>"},{"location":"authorization/","title":"Authorization","text":"<p>Kafka Authorization is based on the following:</p> <ul> <li>kafka-acls.sh utility for ACL management</li> <li>authorizer.class.name configuration property</li> <li>Authorizer (and AclAuthorizer)</li> </ul>"},{"location":"authorization/#resource-types","title":"Resource Types","text":"Type Name Resource ANY Any resource TOPIC A topic GROUP Consumer group CLUSTER Kafka cluster as a whole TRANSACTIONAL_ID Transactional ID DELEGATION_TOKEN A delegation token"},{"location":"authorization/AclApis/","title":"AclApis","text":"<p><code>AclApis</code> is...FIXME</p>"},{"location":"authorization/AclAuthorizer/","title":"AclAuthorizer","text":"<p><code>AclAuthorizer</code> (<code>kafka.security.authorizer.AclAuthorizer</code>) is an Authorizer that uses Apache Zookeeper to persist ACLs.</p> <p>Note</p> <p><code>AclAuthorizer</code> is available since Apache Kafka 2.4.0 (and KIP-504 - Add new Java Authorizer Interface).</p>"},{"location":"authorization/AclAuthorizer/#demo","title":"Demo","text":"<p>Demo: ACL Authorization</p>"},{"location":"authorization/AclAuthorizer/#configuration-properties","title":"Configuration Properties","text":""},{"location":"authorization/AclAuthorizer/#alloweveryoneifnoaclfound","title":"allow.everyone.if.no.acl.found <p>Controls whether or not the authorizer allows access to everyone when no acls are found for a resource.</p> <p>Default: <code>false</code></p> <p>Used when:</p> <ul> <li>authorizeByResourceType</li> <li>authorizeAction</li> </ul>","text":""},{"location":"authorization/AclAuthorizer/#authorizerzookeeperconnectiontimeoutms","title":"authorizer.zookeeper.connection.timeout.ms <p>Default: zkConnectionTimeoutMs</p>","text":""},{"location":"authorization/AclAuthorizer/#authorizerzookeepermaxinflightrequests","title":"authorizer.zookeeper.max.in.flight.requests <p>Default: zookeeper.max.in.flight.requests</p>","text":""},{"location":"authorization/AclAuthorizer/#authorizerzookeepersessiontimeoutms","title":"authorizer.zookeeper.session.timeout.ms <p>Default: zookeeper.session.timeout.ms</p>","text":""},{"location":"authorization/AclAuthorizer/#authorizerzookeeperurl","title":"authorizer.zookeeper.url <p>URL of the dedicated Zookeeper to store ACLs</p> <p>Default: zookeeper.connect</p>","text":""},{"location":"authorization/AclAuthorizer/#superusers","title":"super.users <p>A <code>;</code>-separated list of <code>KafkaPrincipal</code>s (in format <code>type:name</code>) of super users who are allowed to execute operations without checking ACLs (e.g., have access to all the resources for all actions from all hosts).</p> <p>Default: (empty)</p>","text":""},{"location":"authorization/AclAuthorizer/#kafkazkclient","title":"KafkaZkClient <p><code>AclAuthorizer</code> creates a KafkaZkClient in configure and immediately requests it to createAclPaths.</p> <p>This <code>KafkaZkClient</code> can use its own dedicated Zookeeper to store ACLs based on the configuration properties.</p> <p>The <code>KafkaZkClient</code> is used when:</p> <ul> <li>loadCache</li> <li>startZkChangeListeners</li> <li>updateResourceAcls</li> <li>getAclsFromZk</li> <li>updateAclChangedFlag</li> </ul>","text":""},{"location":"authorization/AclAuthorizer/#configure","title":"configure <pre><code>configure(\n  javaConfigs: ju.Map[String, _]): Unit\n</code></pre> <p><code>configure</code> is part of the Configurable abstraction.</p>  <p><code>configure</code> sets up superUsers.</p> <p><code>configure</code> reads the authorizer-specific configuration properties and the following:</p> <ul> <li>zookeeper.set.acl</li> </ul> <p><code>configure</code> zkClientConfigFromKafkaConfigAndMap for a <code>ZKClientConfig</code>.</p> <p><code>configure</code> creates a KafkaZkClient (with the properties) and the following:</p> <ul> <li><code>ACL authorizer</code> name</li> <li><code>kafka.security</code> metric group</li> <li><code>AclAuthorizer</code> metric type</li> </ul> <p></p> <p><code>configure</code> requests the <code>KafkaZkClient</code> to createAclPaths.</p> <p><code>configure</code> sets up extendedAclSupport flag.</p> <p>In the end, <code>configure</code> startZkChangeListeners and loadCache.</p>","text":""},{"location":"authorization/AclAuthorizer/#aclchangesubscription-listeners","title":"AclChangeSubscription Listeners <p><code>AclAuthorizer</code> initializes <code>AclChangeSubscription</code> listeners when startZkChangeListeners.</p>  <p>Note</p> <p>The list of <code>AclChangeSubscription</code> listeners is fixed.</p>","text":""},{"location":"authorization/AclAuthorizer/#extendedaclsupport","title":"extendedAclSupport <p><code>AclAuthorizer</code> uses <code>extendedAclSupport</code> flag to...FIXME</p>","text":""},{"location":"authorization/AclAuthorizer/#issuperuser","title":"isSuperUser <pre><code>isSuperUser(\n  principal: KafkaPrincipal): Boolean\n</code></pre> <p><code>isSuperUser</code> checks whether or not the <code>KafkaPrincipal</code> is a superuser.</p> <p>If so, <code>isSuperUser</code> prints out the following DEBUG message to the logs and returns <code>true</code>.</p> <pre><code>principal = [principal] is a super user, allowing operation without checking acls.\n</code></pre> <p>Otherwise, <code>isSuperUser</code> returns <code>false</code>.</p>  <p><code>isSuperUser</code> is used when:</p> <ul> <li><code>AclAuthorizer</code> is requested to authorizeByResourceType and authorizeAction</li> </ul>","text":""},{"location":"authorization/AclAuthorizer/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>kafka.authorizer.logger</code> logger to see what happens inside.</p> <p>Add the following line to <code>confing/log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.authorizer.logger=ALL\n</code></pre> <p>Refer to Logging.</p>  <p>Note</p> <p>Please note that Kafka comes with a preconfigured <code>kafka.authorizer.logger</code> logger in <code>config/log4j.properties</code>:</p> <pre><code>log4j.appender.authorizerAppender=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.authorizerAppender.DatePattern='.'yyyy-MM-dd-HH\nlog4j.appender.authorizerAppender.File=${kafka.logs.dir}/kafka-authorizer.log\nlog4j.appender.authorizerAppender.layout=org.apache.log4j.PatternLayout\nlog4j.appender.authorizerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n\n\n# Access denials are logged at INFO level, change to DEBUG to also log allowed accesses\nlog4j.logger.kafka.authorizer.logger=INFO, authorizerAppender\nlog4j.additivity.kafka.authorizer.logger=false\n</code></pre> <p>That means that the logs of <code>AclAuthorizer</code> go to <code>logs/kafka-authorizer.log</code> file at <code>INFO</code> logging level and are not added to the main logs (per <code>log4j.additivity</code> being off).</p>","text":""},{"location":"authorization/AclPublisher/","title":"AclPublisher","text":"<p><code>AclPublisher</code> is a MetadataPublisher.</p>"},{"location":"authorization/AclPublisher/#creating-instance","title":"Creating Instance","text":"<p><code>AclPublisher</code> takes the following to be created:</p> <ul> <li> Node ID <li> <code>FaultHandler</code> <li> Node Type <li> Authorizer <p><code>AclPublisher</code> is created when:</p> <ul> <li><code>BrokerServer</code> is requested to start up (to create a BrokerMetadataPublisher)</li> <li><code>ControllerServer</code> is requested to start up</li> </ul>"},{"location":"authorization/AuthHelper/","title":"AuthHelper","text":"<p><code>AuthHelper</code> is used by <code>ControllerApis</code> and KafkaApis to authorize requests to execute an operation on a resource (by type and name).</p>"},{"location":"authorization/AuthHelper/#creating-instance","title":"Creating Instance","text":"<p><code>AuthHelper</code> takes the following to be created:</p> <ul> <li> Authorizer <p><code>AuthHelper</code> is created when:</p> <ul> <li><code>ControllerApis</code> is created (<code>authHelper</code>)</li> <li><code>KafkaApis</code> is created</li> </ul>"},{"location":"authorization/AuthHelper/#authorizing-request","title":"Authorizing Request <pre><code>authorize(\n  requestContext: RequestContext,\n  operation: AclOperation,\n  resourceType: ResourceType,\n  resourceName: String,\n  logIfAllowed: Boolean = true,\n  logIfDenied: Boolean = true,\n  refCount: Int = 1): Boolean\n</code></pre> <p><code>authorize</code> requests the Authorizer (if defined) to authorize the request (to execute the <code>AclOperation</code> on a resource by ResourceType and <code>resourceName</code>).</p>  <p><code>authorize</code> is used when:</p>    Kafka Service Request AclOperation ResourceType Resource Name     <code>AuthHelper</code> authorizeClusterOperation  <code>CLUSTER</code> kafka-cluster   <code>ControllerApis</code> FIXME         <code>KafkaApis</code> handleOffsetCommitRequest READ GROUP groupId     FIXME","text":""},{"location":"authorization/AuthHelper/#authorizebyresourcetype","title":"authorizeByResourceType <pre><code>authorizeByResourceType(\n  requestContext: RequestContext,\n  operation: AclOperation,\n  resourceType: ResourceType): Boolean\n</code></pre> <p><code>authorizeByResourceType</code> requests the Authorizer (if defined) to authorizeByResourceType.</p>  <p><code>authorizeByResourceType</code> is used when:</p> <ul> <li><code>KafkaApis</code> is requested to handleInitProducerIdRequest (to authorize <code>AclOperation.WRITE</code> action on <code>ResourceType.TOPIC</code>)</li> </ul>","text":""},{"location":"authorization/AuthHelper/#authorizeclusteroperation","title":"authorizeClusterOperation <pre><code>authorizeClusterOperation(\n  request: RequestChannel.Request,\n  operation: AclOperation): Unit\n</code></pre> <p><code>authorizeClusterOperation</code> authorizes the given <code>AclOperation</code> with <code>CLUSTER</code> resource type and (hardcoded) <code>kafka-cluster</code> name.</p> <p>If access is denied, <code>authorizeClusterOperation</code> throws a <code>ClusterAuthorizationException</code>:</p> <pre><code>Request [request] is not authorized.\n</code></pre>  <p><code>authorizeClusterOperation</code> is used when:</p>    Kafka Service Request AclOperation     <code>AclApis</code> handleCreateAcls <code>ALTER</code>     handleDeleteAcls <code>ALTER</code>     handleDescribeAcls <code>DESCRIBE</code>   <code>ControllerApis</code> FIXME    <code>KafkaApis</code> handleLeaderAndIsrRequest <code>CLUSTER_ACTION</code>     handleStopReplicaRequest <code>CLUSTER_ACTION</code>     handleUpdateMetadataRequest <code>CLUSTER_ACTION</code>     handleControlledShutdownRequest <code>CLUSTER_ACTION</code>     handleWriteTxnMarkersRequest <code>CLUSTER_ACTION</code>     handleAlterPartitionRequest <code>CLUSTER_ACTION</code>     handleAllocateProducerIdsRequest <code>CLUSTER_ACTION</code>     handleAlterPartitionReassignmentsRequest <code>ALTER</code>     handleListPartitionReassignmentsRequest <code>DESCRIBE</code>","text":""},{"location":"authorization/Authorizer/","title":"Authorizer","text":"<p><code>Authorizer</code> is an abstraction of broker authorizers that Kafka brokers use to authorize operations based on access-control list (ACL).</p> <p>From Wikipedia's Access-control list:</p> <p>An access-control list (ACL) is a list of permissions attached to an object.</p> <p>An ACL specifies which users or system processes are granted access to objects, as well as what operations are allowed on given objects.</p> <p>Each entry in a typical ACL specifies a subject and an operation. For instance, if a file object has an ACL that contains (Alice: read,write; Bob: read), this would give Alice permission to read and write the file and Bob to only read it</p> <p><code>Authorizer</code> is configured by authorizer.class.name configuration property.</p> <p>KIP-504</p> <p><code>Authorizer</code> abstraction is part of KIP-504 - Add new Java Authorizer Interface.</p>"},{"location":"authorization/Authorizer/#contract","title":"Contract","text":""},{"location":"authorization/Authorizer/#acls","title":"ACL Bindings","text":"<pre><code>Iterable&lt;AclBinding&gt; acls(\n  AclBindingFilter filter)\n</code></pre> <p>ACL bindings for the provided <code>filter</code> (synchronously)</p> <p>Used when:</p> <ul> <li><code>Authorizer</code> is requested to authorizeByResourceType</li> <li><code>AuthorizerService</code> is requested to getAcls</li> <li><code>AclApis</code> is requested to handleDescribeAcls</li> </ul>"},{"location":"authorization/Authorizer/#authorizing-request-to-execute-actions","title":"Authorizing Request to Execute Actions <pre><code>List&lt;AuthorizationResult&gt; authorize(\n  AuthorizableRequestContext requestContext,\n  List&lt;Action&gt; actions)\n</code></pre> <p>Authorizes the actions performed by the request (synchronously)</p> <p>Used when:</p> <ul> <li><code>Authorizer</code> is requested to authorizeByResourceType</li> <li><code>AuthHelper</code> is requested to authorize, authorizedOperations, filterByAuthorized</li> </ul>","text":""},{"location":"authorization/Authorizer/#createacls","title":"createAcls <pre><code>List&lt;? extends CompletionStage&lt;AclCreateResult&gt;&gt; createAcls(\n  AuthorizableRequestContext requestContext,\n  List&lt;AclBinding&gt; aclBindings)\n</code></pre> <p>Creates new ACL bindings (asynchronously)</p> <p>Used when:</p> <ul> <li><code>AuthorizerService</code> is requested to addAcls</li> <li><code>AclApis</code> is requested to handleCreateAcls</li> </ul>","text":""},{"location":"authorization/Authorizer/#deleteacls","title":"deleteAcls <pre><code>List&lt;? extends CompletionStage&lt;AclDeleteResult&gt;&gt; deleteAcls(\n  AuthorizableRequestContext requestContext,\n  List&lt;AclBindingFilter&gt; aclBindingFilters)\n</code></pre> <p>Deletes all ACL bindings matching the <code>aclBindingFilters</code> filters (asynchronously)</p> <p>Used when:</p> <ul> <li><code>AuthorizerService</code> is requested to removeAcls</li> <li><code>AclApis</code> is requested to handleDeleteAcls</li> </ul>","text":""},{"location":"authorization/Authorizer/#start","title":"start <pre><code>Map&lt;Endpoint, ? extends CompletionStage&lt;Void&gt;&gt; start(\n  AuthorizerServerInfo serverInfo)\n</code></pre> <p>Starts loading authorization metadata (asynchronously)</p> <p>Returns futures that can be used to wait until metadata for authorizing requests on each listener is available. The future returned for each listener must return only when authorizer is ready to authorize requests on the listener.</p> <p>Used when:</p> <ul> <li><code>BrokerServer</code> is requested to start up</li> <li><code>ControllerServer</code> is requested to start up</li> <li><code>KafkaServer</code> is requested to start up</li> </ul>","text":""},{"location":"authorization/Authorizer/#implementations","title":"Implementations","text":"<ul> <li>AclAuthorizer</li> <li>ClusterMetadataAuthorizer</li> </ul>"},{"location":"authorization/Authorizer/#configurable","title":"Configurable <p><code>Authorizer</code> is a Configurable.</p>","text":""},{"location":"authorization/Authorizer/#authorizebyresourcetype","title":"authorizeByResourceType <pre><code>AuthorizationResult authorizeByResourceType(\n  AuthorizableRequestContext requestContext,\n  AclOperation op,\n  ResourceType resourceType)\n</code></pre> <p><code>authorizeByResourceType</code> authorizes access to the <code>resourceType</code> by super users.</p> <p><code>authorizeByResourceType</code> creates a <code>KafkaPrincipal</code> (based on the <code>PrincipalType</code> and <code>Name</code> from the <code>requestContext</code>) and reads the request's host address. <code>authorizeByResourceType</code> tries to authorize the request based on the ACL bindings (with a <code>AclBindingFilter</code> for the <code>resourceType</code> and <code>ANY</code> pattern).</p>  <p><code>authorizeByResourceType</code> is used when:</p> <ul> <li><code>AuthHelper</code> is requested to authorizeByResourceType</li> </ul>","text":""},{"location":"authorization/ClusterMetadataAuthorizer/","title":"ClusterMetadataAuthorizer","text":"<p><code>ClusterMetadataAuthorizer</code> is an extension of the Authorizer abstraction for authorizers that store state in the <code>__cluster_metadata</code> log.</p>"},{"location":"authorization/ClusterMetadataAuthorizer/#contract-subset","title":"Contract (Subset)","text":""},{"location":"authorization/ClusterMetadataAuthorizer/#loadSnapshot","title":"loadSnapshot","text":"<pre><code>void loadSnapshot(\n  Map&lt;Uuid, StandardAcl&gt; acls)\n</code></pre> <p>See:</p> <ul> <li>StandardAuthorizer</li> </ul> <p>Used when:</p> <ul> <li><code>AclPublisher</code> is requested to onMetadataUpdate</li> </ul>"},{"location":"authorization/ClusterMetadataAuthorizer/#implementations","title":"Implementations","text":"<ul> <li>StandardAuthorizer</li> </ul>"},{"location":"authorization/StandardAuthorizer/","title":"StandardAuthorizer","text":"<p><code>StandardAuthorizer</code> is a ClusterMetadataAuthorizer.</p>"},{"location":"authorization/StandardAuthorizer/#loadSnapshot","title":"loadSnapshot","text":"ClusterMetadataAuthorizer <pre><code>void loadSnapshot(\n  Map&lt;Uuid, StandardAcl&gt; acls)\n</code></pre> <p><code>loadSnapshot</code> is part of the ClusterMetadataAuthorizer abstraction.</p> <p><code>loadSnapshot</code>...FIXME</p>"},{"location":"broker/","title":"Broker","text":""},{"location":"broker/KafkaBroker/","title":"KafkaBroker","text":"<p><code>KafkaBroker</code> is an abstraction of Kafka brokers.</p> <p>Every <code>KafkaBroker</code> is a KafkaMetricsGroup.</p>"},{"location":"broker/KafkaBroker/#contract","title":"Contract","text":""},{"location":"broker/KafkaBroker/#logmanager","title":"LogManager <pre><code>logManager: LogManager\n</code></pre> <p>LogManager</p> <p>Used when:</p> <ul> <li><code>DynamicBrokerConfig</code> is requested to <code>addReconfigurables</code></li> <li><code>DynamicThreadPool</code> is requested to <code>reconfigure</code></li> </ul>","text":""},{"location":"broker/KafkaBroker/#starting-up","title":"Starting Up <pre><code>startup(): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>KafkaServerTestHarness</code> is requested to <code>restartDeadBrokers</code> and <code>createBrokers</code></li> </ul>","text":""},{"location":"broker/KafkaBroker/#others","title":"others  <p>Note</p> <p>There are other methods.</p>","text":""},{"location":"broker/KafkaBroker/#implementations","title":"Implementations","text":"<ul> <li>BrokerServer (for KRaft mode)</li> <li>KafkaServer</li> </ul>"},{"location":"broker/KafkaServer/","title":"KafkaServer","text":"<p><code>KafkaServer</code> is a Server for Zookeeper mode (non-KRaft mode).</p>"},{"location":"broker/KafkaServer/#creating-instance","title":"Creating Instance","text":"<p><code>KafkaServer</code> takes the following to be created:</p> <ul> <li> KafkaConfig <li> <code>Time</code> (default: <code>SYSTEM</code>) <li> Optional Thread Name Prefix (default: undefined) <li>enableForwarding flag</li> <p><code>KafkaServer</code> is created\u00a0when:</p> <ul> <li><code>Kafka</code> command-line application is launched (and builds a server with process.roles specified)</li> </ul>"},{"location":"broker/KafkaServer/#enableforwarding","title":"enableForwarding <pre><code>enableForwarding: Boolean\n</code></pre> <p><code>KafkaServer</code> can be given <code>enableForwarding</code> flag when created.</p> <p><code>enableForwarding</code> is <code>false</code> unless specified explicitly that seems never happen.</p> <p>When enabled, <code>KafkaServer</code> creates a ForwardingManager and uses BrokerToControllerChannelManager.</p>","text":""},{"location":"broker/KafkaServer/#zkconfigmanager","title":"ZkConfigManager <p><code>KafkaServer</code> creates a ZkConfigManager when started with the following:</p> <ul> <li>KafkaZkClient</li> <li>ConfigHandlers</li> </ul> <p><code>KafkaServer</code> requests the <code>ZkConfigManager</code> to startup immediately.</p>","text":""},{"location":"broker/KafkaServer/#confighandlers","title":"ConfigHandlers <pre><code>dynamicConfigHandlers: Map[String, ConfigHandler]\n</code></pre> <p><code>KafkaServer</code> uses <code>dynamicConfigHandlers</code> registry of ConfigHandlers (by their name).</p>    Name ConfigHandler     topics TopicConfigHandler   clients <code>ClientIdConfigHandler</code>   users <code>UserConfigHandler</code>   brokers BrokerConfigHandler   ips <code>IpConfigHandler</code>    <p><code>KafkaServer</code> uses the <code>dynamicConfigHandlers</code> to create ZkConfigManager (at startup).</p>","text":""},{"location":"broker/KafkaServer/#transactioncoordinator","title":"TransactionCoordinator <p><code>KafkaServer</code> creates and starts a TransactionCoordinator when created.</p> <p><code>KafkaServer</code> uses the <code>TransactionCoordinator</code> to create the following:</p> <ul> <li>data-plane and the control-plane request processors</li> <li>AutoTopicCreationManager</li> </ul> <p>The <code>TransactionCoordinator</code> is requested to shutdown along with KafkaServer.</p>","text":""},{"location":"broker/KafkaServer/#data-plane-request-processor","title":"Data-Plane Request Processor <p><code>KafkaServer</code> creates a KafkaApis for data-related communication.</p> <p><code>KafkaApis</code> is used to create data-plane request handler pool.</p>","text":""},{"location":"broker/KafkaServer/#kafkarequesthandlerpool","title":"KafkaRequestHandlerPool","text":""},{"location":"broker/KafkaServer/#control-plane-request-processor","title":"Control-Plane Request Processor <p><code>KafkaServer</code> creates a KafkaApis for control-related communication.</p>","text":""},{"location":"broker/KafkaServer/#starting-up","title":"Starting Up <pre><code>startup(): Unit\n</code></pre> <p><code>startup</code>\u00a0is part of the Server abstraction.</p>  <p><code>startup</code> prints out the following INFO message to the logs:</p> <pre><code>starting\n</code></pre> <p><code>startup</code> initZkClient and creates a ZkConfigRepository (with a new <code>AdminZkClient</code>).</p> <p><code>startup</code>...FIXME</p> <p><code>startup</code> getOrGenerateClusterId (that becomes the _clusterId) and prints out the following INFO message to the logs:</p> <pre><code>Cluster ID = [clusterId]\n</code></pre> <p><code>startup</code> getBrokerMetadataAndOfflineDirs with the logDirs.</p> <p><code>startup</code> looks up the broker ID.</p> <p><code>startup</code>...FIXME</p> <p><code>startup</code> creates a LogManager that is requested to start up.</p> <p><code>startup</code>...FIXME</p> <p><code>startup</code> creates a TransactionCoordinator (with the ReplicaManager) and requests it to startup.</p> <p><code>startup</code>...FIXME</p>","text":""},{"location":"broker/KafkaServer/#kafkabroker","title":"KafkaBroker <p><code>KafkaServer</code> is a KafkaBroker.</p>","text":""},{"location":"broker/KafkaServer/#looking-up-broker-id","title":"Looking Up Broker ID <pre><code>getOrGenerateBrokerId(\n  brokerMetadata: RawMetaProperties): Int\n</code></pre> <p><code>getOrGenerateBrokerId</code> takes the broker.id (from the KafkaConfig) and makes sure that it matches the <code>RawMetaProperties</code>'s (or an <code>InconsistentBrokerIdException</code> is thrown).</p> <p><code>getOrGenerateBrokerId</code> uses the given <code>RawMetaProperties</code> for the broker ID if defined.</p> <p>Otherwise, if <code>broker.id</code> (from the KafkaConfig) is negative and broker.id.generation.enable is enabled, <code>getOrGenerateBrokerId</code> generates a broker ID.</p> <p>In the end, when all the earlier attempts \"fail\", <code>getOrGenerateBrokerId</code> uses the broker.id (from the KafkaConfig).</p>  <p><code>getOrGenerateBrokerId</code> is used when:</p> <ul> <li><code>KafkaServer</code> is requested to start up</li> </ul>","text":""},{"location":"broker/KafkaServer/#logmanager","title":"LogManager <pre><code>logManager: LogManager\n</code></pre> <p><code>logManager</code> is part of the KafkaBroker abstraction.</p>  <p><code>KafkaServer</code> creates a LogManager at startup.</p>","text":""},{"location":"broker/KafkaServer/#authorizer","title":"Authorizer <pre><code>authorizer: Option[Authorizer]\n</code></pre> <p><code>authorizer</code> is part of the KafkaBroker abstraction.</p>  <p><code>KafkaServer</code> is given an Authorizer at startup based on authorizer.class.name configuration property.</p>","text":""},{"location":"broker/KafkaServer/#kafkacontroller","title":"KafkaController <p><code>KafkaServer</code> creates a KafkaController at startup.</p> <p>The <code>KafkaController</code> is requested to start up immediately and shut down alongside the KafkaServer.</p> <p>The <code>KafkaController</code> is used when:</p> <ul> <li>Creating a <code>TopicConfigHandler</code> (in the dynamicConfigHandlers)</li> <li>controlledShutdown (for the brokerEpoch)</li> <li><code>DynamicLogConfig</code> is requested to <code>reconfigure</code></li> <li><code>DynamicListenerConfig</code> is requested to <code>reconfigure</code></li> <li><code>KafkaServer</code> is requested to startup<ul> <li>for the brokerEpoch for AlterIsrManager</li> <li>for the brokerEpoch for ProducerIdManager</li> <li><code>ZkSupport</code></li> </ul> </li> </ul>","text":""},{"location":"broker/KafkaServer/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>kafka.server.KafkaServer</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.server.KafkaServer=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"broker/RaftControllerNodeProvider/","title":"RaftControllerNodeProvider","text":"<p><code>RaftControllerNodeProvider</code> is...FIXME</p>"},{"location":"clients/","title":"Kafka Clients","text":""},{"location":"clients/CommonClientConfigs/","title":"CommonClientConfigs","text":""},{"location":"clients/CommonClientConfigs/#clientid","title":"client.id","text":""},{"location":"clients/CommonClientConfigs/#groupid","title":"group.id <p>A unique identifier of the consumer group a consumer belongs to. Required if the consumer uses either the group management functionality by using Consumer.subscribe or the Kafka-based offset management strategy.</p> <p>Default: (undefined)</p>","text":""},{"location":"clients/CommonClientConfigs/#retries","title":"retries","text":""},{"location":"clients/CommonClientConfigs/#retrybackoffms","title":"retry.backoff.ms","text":""},{"location":"clients/CommonClientConfigs/#requesttimeoutms","title":"request.timeout.ms","text":""},{"location":"clients/KafkaClient/","title":"KafkaClient","text":"<p><code>KafkaClient</code> is an interface to NetworkClient.</p>"},{"location":"clients/KafkaClient/#contract","title":"Contract","text":""},{"location":"clients/KafkaClient/#inflightrequestcount","title":"inFlightRequestCount <pre><code>int inFlightRequestCount()\nint inFlightRequestCount(\n  String nodeId)\n</code></pre> <p>Used when:</p> <ul> <li><code>ConsumerNetworkClient</code> is requested to pendingRequestCount and poll</li> <li><code>Sender</code> is requested to run</li> <li><code>SenderMetrics</code> is requested for <code>requests-in-flight</code> performance metric</li> </ul>","text":""},{"location":"clients/KafkaClient/#leastloadednode","title":"leastLoadedNode <pre><code>Node leastLoadedNode(\n  long now)\n</code></pre> <p>Used when:</p> <ul> <li><code>ConsumerNetworkClient</code> is requested for the leastLoadedNode</li> <li><code>DefaultMetadataUpdater</code> is requested to <code>maybeUpdate</code></li> <li><code>KafkaAdminClient</code> is used</li> <li><code>Sender</code> is requested for the maybeSendAndPollTransactionalRequest</li> </ul>","text":""},{"location":"clients/KafkaClient/#newclientrequest","title":"newClientRequest <pre><code>ClientRequest newClientRequest(\n  String nodeId,\n  AbstractRequest.Builder&lt;?&gt; requestBuilder,\n  long createdTimeMs,\n  boolean expectResponse)\nClientRequest newClientRequest(\n  String nodeId,\n  AbstractRequest.Builder&lt;?&gt; requestBuilder,\n  long createdTimeMs,\n  boolean expectResponse,\n  int requestTimeoutMs,\n  RequestCompletionHandler callback)\n</code></pre> <p>Used when:</p> <ul> <li><code>AdminClientRunnable</code> is requested to <code>sendEligibleCalls</code></li> <li><code>ConsumerNetworkClient</code> is requested to send</li> <li><code>NetworkClient</code> is requested to newClientRequest, sendInternalMetadataRequest and handleInitiateApiVersionRequests</li> <li><code>RequestSendThread</code> is requested to <code>doWork</code></li> <li><code>Sender</code> is requested to run</li> <li><code>KafkaServer</code> is requested to <code>controlledShutdown</code></li> <li><code>ReplicaFetcherBlockingSend</code> is requested to <code>sendRequest</code></li> </ul>","text":""},{"location":"clients/KafkaClient/#poll","title":"poll <pre><code>List&lt;ClientResponse&gt; poll(\n  long timeout,\n  long now)\n</code></pre> <p>Used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"clients/KafkaClient/#polldelayms","title":"pollDelayMs <pre><code>long pollDelayMs(\n  Node node,\n  long now)\n</code></pre> <p>Used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"clients/KafkaClient/#is-node-ready-and-connected","title":"Is Node Ready and Connected <pre><code>boolean ready(\n  Node node,\n  long now);\n</code></pre> <p>Used when:</p> <ul> <li><code>AdminClientRunnable</code> is requested to sendEligibleCalls</li> <li><code>ConsumerNetworkClient</code> is requested to tryConnect and trySend</li> <li><code>InterBrokerSendThread</code> is requested to sendRequests</li> <li><code>NetworkClientUtils</code> is requested to awaitReady</li> <li><code>Sender</code> is requested to sendProducerData</li> </ul>","text":""},{"location":"clients/KafkaClient/#send","title":"send <pre><code>void send(\n  ClientRequest request,\n  long now)\n</code></pre> <p>Used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"clients/KafkaClient/#wakeup","title":"wakeup <pre><code>void wakeup()\n</code></pre> <p>Used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"clients/KafkaClient/#implementations","title":"Implementations","text":"<ul> <li>NetworkClient</li> </ul>"},{"location":"clients/KafkaClient/#closeable","title":"Closeable <p><code>KafkaClient</code> is a <code>Closeable</code> (Java).</p>","text":""},{"location":"clients/Metadata/","title":"Metadata","text":""},{"location":"clients/Metadata/#update","title":"update <pre><code>void update(\n  int requestVersion,\n  MetadataResponse response,\n  boolean isPartialUpdate,\n  long nowMs)\n</code></pre> <p><code>update</code>...FIXME</p> <p><code>update</code>\u00a0is used when:</p> <ul> <li><code>ProducerMetadata</code> is requested to <code>update</code></li> <li><code>Metadata</code> is requested to updateWithCurrentRequestVersion</li> <li><code>DefaultMetadataUpdater</code> is requested to <code>handleSuccessfulResponse</code></li> </ul>","text":""},{"location":"clients/MetadataUpdater/","title":"MetadataUpdater","text":"<p><code>MetadataUpdater</code> is...FIXME</p>"},{"location":"clients/NetworkClient/","title":"NetworkClient","text":"<p><code>NetworkClient</code> is a KafkaClient.</p>"},{"location":"clients/NetworkClient/#leastloadednode","title":"leastLoadedNode <pre><code>Node leastLoadedNode(\n  long now)\n</code></pre> <p><code>leastLoadedNode</code> requests the MetadataUpdater for the nodes (in a non-blocking fashion).</p> <p><code>leastLoadedNode</code> generates a random number to offset the first node to start checking node candidates from.</p> <p><code>leastLoadedNode</code> finds three nodes:</p> <ol> <li><code>foundReady</code> that is ready (perhaps with some in-flight requests)</li> <li><code>foundConnecting</code> with a connection already being established (using the <code>ClusterConnectionStates</code> registry)</li> <li><code>foundCanConnect</code> that can be connected</li> </ol> <p>When a node is found that is ready and has no in-flight requests, <code>leastLoadedNode</code> prints out the following TRACE message to the logs and returns the node immediately:</p> <pre><code>Found least loaded node [node] connected with no in-flight requests\n</code></pre> <p>When a node candidate does not meet any of the above requirements, <code>leastLoadedNode</code> prints out the following TRACE message to the logs:</p> <pre><code>Removing node [node] from least loaded node selection since it is neither ready for sending nor connecting\n</code></pre> <p><code>leastLoadedNode</code> prefers the <code>foundReady</code> node over the <code>foundConnecting</code> with the <code>foundCanConnect</code> as the last resort.</p> <p>When no node could be found, <code>leastLoadedNode</code> prints out the following TRACE message to the logs (and returns <code>null</code>):</p> <pre><code>Least loaded node selection failed to find an available node\n</code></pre> <p><code>leastLoadedNode</code>\u00a0is part of the KafkaClient abstraction.</p>","text":""},{"location":"clients/NetworkClientUtils/","title":"NetworkClientUtils","text":""},{"location":"clients/admin/","title":"Kafka Admin","text":""},{"location":"clients/admin/Admin/","title":"Admin","text":""},{"location":"clients/admin/Admin/#create","title":"create <pre><code>Admin create(\n  Map&lt;String, Object&gt; conf)\nAdmin create(\n  Properties props)\n</code></pre> <p><code>create</code> creates a KafkaAdminClient.</p>  <p><code>create</code> is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"clients/admin/AdminClientRunnable/","title":"AdminClientRunnable","text":""},{"location":"clients/admin/KafkaAdminClient/","title":"KafkaAdminClient","text":"<p><code>KafkaAdminClient</code> is a <code>AdminClient</code> that is used in Kafka administration utilities.</p>"},{"location":"clients/admin/KafkaAdminClient/#creating-instance","title":"Creating Instance","text":"<p><code>KafkaAdminClient</code> takes the following to be created:</p> <ul> <li> <code>AdminClientConfig</code> <li> Client ID <li> Time <li> <code>AdminMetadataManager</code> <li> Metrics <li> KafkaClient <li> <code>TimeoutProcessorFactory</code> <li> <code>LogContext</code> <p><code>KafkaAdminClient</code> is created using createInternal.</p>"},{"location":"clients/admin/KafkaAdminClient/#createinternal","title":"createInternal <pre><code>KafkaAdminClient createInternal(\n  AdminClientConfig config,\n  AdminMetadataManager metadataManager,\n  KafkaClient client,\n  Time time)\nKafkaAdminClient createInternal(\n  AdminClientConfig config,\n  TimeoutProcessorFactory timeoutProcessorFactory) // (1)!\nKafkaAdminClient createInternal(\n  AdminClientConfig config,\n  TimeoutProcessorFactory timeoutProcessorFactory,\n  HostResolver hostResolver)\n</code></pre> <ol> <li>Uses an undefined <code>HostResolver</code></li> </ol> <p><code>createInternal</code>...FIXME</p>  <p><code>createInternal</code> is used when:</p> <ul> <li><code>Admin</code> is requested to create</li> </ul>","text":""},{"location":"clients/admin/KafkaAdminClient/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.clients.admin.KafkaAdminClient</code> logger to see what happens inside.</p> <p>Add the following line to <code>config/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.clients.admin.KafkaAdminClient=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"clients/admin/KafkaAdminClient/#review-me","title":"Review Me","text":""},{"location":"clients/admin/KafkaAdminClient/#triggerring-preferred-replica-leader-election","title":"Triggerring Preferred Replica Leader Election <pre><code>ElectPreferredLeadersResult electPreferredLeaders(\n  Collection&lt;TopicPartition&gt; partitions,\n  ElectPreferredLeadersOptions options)\n</code></pre> <p>NOTE: <code>electPreferredLeaders</code> is part of the &lt;&gt; to trigger &lt;&gt;. <p><code>electPreferredLeaders</code> creates a electPreferredLeaders call that simply uses &lt;&gt; to &lt;&gt; and, when a response comes in, requests the <code>ElectPreferredLeadersRequest</code> to &lt;&gt;. <p>In the end, <code>electPreferredLeaders</code> requests the &lt;&gt; to &lt;&gt; the <code>electPreferredLeaders</code> call.","text":""},{"location":"clients/consumer/","title":"Kafka Consumers","text":"<p>KafkaConsumer uses Fetcher to fetch records from a Kafka cluster. One could say that <code>KafkaConsumer</code> is a developer-oriented interface to <code>Fetcher</code>.</p> <p><code>KafkaConsumer</code> is assigned a <code>IsolationLevel</code> based on isolation.level configuration property.</p>"},{"location":"clients/consumer/AbstractCoordinator/","title":"AbstractCoordinator","text":"<p><code>AbstractCoordinator</code> is an abstraction of consumer group coordination manager.</p>"},{"location":"clients/consumer/AbstractCoordinator/#contract","title":"Contract","text":""},{"location":"clients/consumer/AbstractCoordinator/#metadata","title":"metadata <pre><code>JoinGroupRequestData.JoinGroupRequestProtocolCollection metadata()\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractCoordinator</code> is requested to ensureActiveGroup (and sendJoinGroupRequest)</li> </ul>","text":""},{"location":"clients/consumer/AbstractCoordinator/#onjoincomplete","title":"onJoinComplete <pre><code>void onJoinComplete(\n  int generation,\n  String memberId,\n  String protocol,\n  ByteBuffer memberAssignment)\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractCoordinator</code> is requested to ensureActiveGroup (and joinGroupIfNeeded)</li> </ul>","text":""},{"location":"clients/consumer/AbstractCoordinator/#onjoinprepare","title":"onJoinPrepare <pre><code>void onJoinPrepare(\n  int generation,\n  String memberId)\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractCoordinator</code> is requested to ensureActiveGroup (and joinGroupIfNeeded)</li> </ul>","text":""},{"location":"clients/consumer/AbstractCoordinator/#performassignment","title":"performAssignment <pre><code>Map&lt;String, ByteBuffer&gt; performAssignment(\n  String leaderId,\n  String protocol,\n  List&lt;JoinGroupResponseData.JoinGroupResponseMember&gt; allMemberMetadata)\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractCoordinator</code> is requested to handle a JoinGroup response (having joined the group as the leader)</li> </ul>","text":""},{"location":"clients/consumer/AbstractCoordinator/#protocoltype","title":"protocolType <pre><code>String protocolType()\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractCoordinator</code> is requested to sendJoinGroupRequest, onJoinFollower, onJoinLeader, isProtocolTypeInconsistent</li> </ul>","text":""},{"location":"clients/consumer/AbstractCoordinator/#implementations","title":"Implementations","text":"<ul> <li>ConsumerCoordinator</li> <li><code>WorkerCoordinator</code> (Kafka Connect)</li> </ul>"},{"location":"clients/consumer/AbstractCoordinator/#handling-joingroup-response","title":"Handling JoinGroup Response","text":""},{"location":"clients/consumer/AbstractCoordinator/#group-leader","title":"Group Leader <pre><code>RequestFuture&lt;ByteBuffer&gt; onJoinLeader(\n  JoinGroupResponse joinResponse)\n</code></pre> <p><code>onJoinLeader</code> performAssignment (with the leader ID, the group protocol and the members).</p> <p><code>onJoinLeader</code> prints out the following DEBUG message to the logs:</p> <pre><code>Sending leader SyncGroup to coordinator [coordinator] at generation [generation]: [SyncGroupRequest]\n</code></pre> <p>In the end, <code>onJoinLeader</code> sends a <code>SyncGroupRequest</code> to the coordinator.</p> <p><code>onJoinLeader</code>\u00a0is used when:</p> <ul> <li>JoinGroupResponseHandler is requested to handle a <code>JoinGroup</code> response (after joining group successfully as the leader)</li> </ul>","text":""},{"location":"clients/consumer/AbstractCoordinator/#group-follower","title":"Group Follower <pre><code>RequestFuture&lt;ByteBuffer&gt; onJoinFollower()\n</code></pre> <p><code>onJoinFollower</code>...FIXME</p> <p><code>onJoinFollower</code>\u00a0is used when:</p> <ul> <li>JoinGroupResponseHandler is requested to handle a <code>JoinGroup</code> response (after joining group successfully as a follower)</li> </ul>","text":""},{"location":"clients/consumer/AbstractCoordinator/#ensureactivegroup","title":"ensureActiveGroup <pre><code>void ensureActiveGroup()\nboolean ensureActiveGroup(\n  Timer timer)\n</code></pre> <p><code>ensureActiveGroup</code> ensureCoordinatorReady (and returns <code>false</code> if not).</p> <p><code>ensureActiveGroup</code>...FIXME</p> <p><code>ensureActiveGroup</code>\u00a0is used when:</p> <ul> <li><code>ConsumerCoordinator</code> is requested to poll</li> <li><code>WorkerCoordinator</code> (Kafka Connect) is requested to <code>poll</code></li> </ul>","text":""},{"location":"clients/consumer/AbstractCoordinator/#joingroupifneeded","title":"joinGroupIfNeeded <pre><code>boolean joinGroupIfNeeded(\n  Timer timer)\n</code></pre> <p><code>joinGroupIfNeeded</code>...FIXME</p>","text":""},{"location":"clients/consumer/AbstractCoordinator/#initiatejoingroup","title":"initiateJoinGroup <pre><code>RequestFuture&lt;ByteBuffer&gt; initiateJoinGroup()\n</code></pre> <p><code>initiateJoinGroup</code>...FIXME</p>","text":""},{"location":"clients/consumer/AbstractCoordinator/#sendjoingrouprequest","title":"sendJoinGroupRequest <pre><code>RequestFuture&lt;ByteBuffer&gt; sendJoinGroupRequest()\n</code></pre> <p><code>sendJoinGroupRequest</code>...FIXME</p>","text":""},{"location":"clients/consumer/AbstractCoordinator/#waiting-for-consumer-group-coordinator-known-and-ready","title":"Waiting for Consumer Group Coordinator Known and Ready <pre><code>boolean ensureCoordinatorReady(\n  Timer timer)\n</code></pre> <p><code>ensureCoordinatorReady</code> returns <code>true</code> immediately when the consumer group coordinator is known and available.</p> <p>Otherwise, <code>ensureCoordinatorReady</code> keeps looking up the group coordinator (by sending <code>FindCoordinator</code> requests to the least loaded broker) until the coordinator is available or the timer timed out.</p> <p>In the end, <code>ensureCoordinatorReady</code> returns whether the coordinator is known and available or not.</p> <p><code>ensureCoordinatorReady</code>\u00a0is used when:</p> <ul> <li><code>AbstractCoordinator</code> is requested to ensureActiveGroup (and joinGroupIfNeeded)</li> <li><code>ConsumerCoordinator</code> is requested to poll, fetchCommittedOffsets, close, commitOffsetsSync</li> </ul>","text":""},{"location":"clients/consumer/AbstractCoordinator/#joingroupresponsehandler","title":"JoinGroupResponseHandler <p><code>JoinGroupResponseHandler</code> is a <code>CoordinatorResponseHandler</code> to handle responses from the group coordinator after sendJoinGroupRequest.</p>","text":""},{"location":"clients/consumer/AbstractPartitionAssignor/","title":"AbstractPartitionAssignor","text":"<p><code>AbstractPartitionAssignor</code> is...FIXME</p>"},{"location":"clients/consumer/Consumer/","title":"Consumer","text":"<p><code>Consumer&lt;K, V&gt;</code> is an interface to KafkaConsumer for Kafka developers to use to consume records (with <code>K</code> keys and <code>V</code> values) from a Kafka cluster.</p>"},{"location":"clients/consumer/Consumer/#contract-subset","title":"Contract (Subset)","text":""},{"location":"clients/consumer/Consumer/#enforcerebalance","title":"enforceRebalance <pre><code>void enforceRebalance()\n</code></pre>","text":""},{"location":"clients/consumer/Consumer/#groupmetadata","title":"groupMetadata <pre><code>ConsumerGroupMetadata groupMetadata()\n</code></pre>","text":""},{"location":"clients/consumer/Consumer/#subscribing-to-topics","title":"Subscribing to Topics <pre><code>void subscribe(\n  Collection&lt;String&gt; topics)\nvoid subscribe(\n  Collection&lt;String&gt; topics,\n  ConsumerRebalanceListener callback)\nvoid subscribe(\n  Pattern pattern)\nvoid subscribe(\n  Pattern pattern,\n  ConsumerRebalanceListener callback)\n</code></pre>","text":""},{"location":"clients/consumer/Consumer/#waking-up","title":"Waking Up <pre><code>void wakeup()\n</code></pre>","text":""},{"location":"clients/consumer/ConsumerConfig/","title":"ConsumerConfig","text":""},{"location":"clients/consumer/ConsumerConfig/#autocommitintervalms","title":"auto.commit.interval.ms","text":""},{"location":"clients/consumer/ConsumerConfig/#autooffsetreset","title":"auto.offset.reset <p>What to do when there is no initial offset in Kafka or if the current offset does not exist anymore on a broker (e.g. because that data has been deleted)</p> <p>Default: <code>latest</code></p> <p>Supported values:</p> <ul> <li><code>latest</code> - reset the offset to the latest offset</li> <li><code>earliest</code> - reset the offset to the earliest offset</li> <li><code>none</code> - throw exception to the consumer if no previous offset is found for the consumer's group</li> </ul>","text":""},{"location":"clients/consumer/ConsumerConfig/#checkcrcs","title":"check.crcs","text":""},{"location":"clients/consumer/ConsumerConfig/#clientrack","title":"client.rack","text":""},{"location":"clients/consumer/ConsumerConfig/#enableautocommit","title":"enable.auto.commit <p>Controls whether consumer offsets should be periodically committed in the background or not</p> <p>Default: <code>true</code></p> <p>kafka-console-consumer supports the property using <code>--consumer-property</code> or <code>--consumer.config</code> options.</p> <p>Used when:</p> <ul> <li><code>ConsumerConfig</code> is requested to maybeOverrideEnableAutoCommit</li> </ul>","text":""},{"location":"clients/consumer/ConsumerConfig/#fetchmaxbytes","title":"fetch.max.bytes","text":""},{"location":"clients/consumer/ConsumerConfig/#fetchmaxwaitms","title":"fetch.max.wait.ms","text":""},{"location":"clients/consumer/ConsumerConfig/#fetchminbytes","title":"fetch.min.bytes","text":""},{"location":"clients/consumer/ConsumerConfig/#groupid","title":"group.id <p>See CommonClientConfigs</p>","text":""},{"location":"clients/consumer/ConsumerConfig/#internalthrowonfetchstableoffsetunsupported","title":"internal.throw.on.fetch.stable.offset.unsupported","text":""},{"location":"clients/consumer/ConsumerConfig/#isolationlevel","title":"isolation.level <p>Controls how KafkaConsumer should read messages written transactionally</p> <p>Default: <code>read_uncommitted</code></p> <p>Supported values:</p> <ul> <li> <p><code>read_uncommitted</code> - Consumer.poll() will only return transactional messages which have been committed (filtering out transactional messages which are not committed).</p> </li> <li> <p><code>read_committed</code> - Consumer.poll() will return all messages, even transactional messages which have been not committed yet or even aborted.</p> </li> </ul> <p>Non-transactional messages will be returned unconditionally in either mode.</p> <p>Messages will always be returned in offset order. Hence, in <code>read_committed</code> mode, <code>consumer.poll()</code> will only return messages up to the last stable offset (LSO), which is the one less than the offset of the first open transaction.</p> <p>In particular any messages appearing after messages belonging to ongoing transactions will be withheld until the relevant transaction has been completed.</p> <p>As a result, <code>read_committed</code> consumers will not be able to read up to the high watermark when there are in-flight transactions.</p> <p>Further, when in <code>read_committed</code> the <code>seekToEnd</code> method will return the last stable offset.</p> <p>kafka-console-consumer supports the property using <code>--isolation-level</code> option.</p>","text":""},{"location":"clients/consumer/ConsumerConfig/#maxpartitionfetchbytes","title":"max.partition.fetch.bytes","text":""},{"location":"clients/consumer/ConsumerConfig/#maxpollrecords","title":"max.poll.records","text":""},{"location":"clients/consumer/ConsumerConfig/#requesttimeoutms","title":"request.timeout.ms","text":""},{"location":"clients/consumer/ConsumerConfig/#retrybackoffms","title":"retry.backoff.ms","text":""},{"location":"clients/consumer/ConsumerConfig/#maybeoverrideenableautocommit","title":"maybeOverrideEnableAutoCommit <pre><code>boolean maybeOverrideEnableAutoCommit()\n</code></pre> <p><code>maybeOverrideEnableAutoCommit</code> returns <code>false</code> when neither group.id nor enable.auto.commit are specified. Otherwise, <code>maybeOverrideEnableAutoCommit</code> returns the value of enable.auto.commit configuration property.</p> <p><code>maybeOverrideEnableAutoCommit</code> throws an <code>InvalidConfigurationException</code> when no group.id is given with enable.auto.commit enabled:</p> <pre><code>enable.auto.commit cannot be set to true when default group id (null) is used.\n</code></pre> <p><code>maybeOverrideEnableAutoCommit</code>\u00a0is used when:</p> <ul> <li><code>KafkaConsumer</code> is created (and creates a ConsumerCoordinator)</li> </ul>","text":""},{"location":"clients/consumer/ConsumerCoordinator/","title":"ConsumerCoordinator","text":"<p><code>ConsumerCoordinator</code> is a consumer group coordination manager.</p>"},{"location":"clients/consumer/ConsumerCoordinator/#creating-instance","title":"Creating Instance","text":"<p><code>ConsumerCoordinator</code> takes the following to be created:</p> <ul> <li> <code>GroupRebalanceConfig</code> <li> <code>LogContext</code> <li> ConsumerNetworkClient <li> ConsumerPartitionAssignors <li> ConsumerMetadata <li> SubscriptionState <li> <code>Metrics</code> <li> Metrics Group Prefix <li> <code>Time</code> <li>autoCommitEnabled</li> <li> auto.commit.interval.ms <li> <code>ConsumerInterceptors</code> <li> internal.throw.on.fetch.stable.offset.unsupported <p>While being created, <code>ConsumerCoordinator</code> requests the ConsumerMetadata for an update of the current cluster metadata.</p> <p><code>ConsumerCoordinator</code> is created\u00a0when:</p> <ul> <li><code>KafkaConsumer</code> is created (and group.id configuration property is specified)</li> </ul>"},{"location":"clients/consumer/ConsumerCoordinator/#autocommitenabled","title":"autoCommitEnabled <p><code>ConsumerCoordinator</code> is given <code>autoCommitEnabled</code> flag when created with the value based on group.id and enable.auto.commit configuration properties.</p>","text":""},{"location":"clients/consumer/ConsumerCoordinator/#metadata","title":"metadata <pre><code>JoinGroupRequestData.JoinGroupRequestProtocolCollection metadata()\n</code></pre> <p><code>metadata</code>...FIXME</p> <p><code>metadata</code>\u00a0is part of the AbstractCoordinator abstraction.</p>","text":""},{"location":"clients/consumer/ConsumerMetadata/","title":"ConsumerMetadata","text":"<p><code>ConsumerMetadata</code> is...FIXME</p>"},{"location":"clients/consumer/ConsumerNetworkClient/","title":"ConsumerNetworkClient","text":""},{"location":"clients/consumer/ConsumerPartitionAssignor/","title":"ConsumerPartitionAssignor","text":"<p><code>ConsumerPartitionAssignor</code> is an abstraction of partition assignors.</p>"},{"location":"clients/consumer/ConsumerPartitionAssignor/#contract","title":"Contract","text":""},{"location":"clients/consumer/ConsumerPartitionAssignor/#assign","title":"assign <pre><code>GroupAssignment assign(\n  Cluster metadata,\n  GroupSubscription groupSubscription)\n</code></pre> <p>Used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"clients/consumer/ConsumerPartitionAssignor/#name","title":"name <pre><code>String name()\n</code></pre> <p>Used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"clients/consumer/ConsumerPartitionAssignor/#implementations","title":"Implementations","text":"<ul> <li>AbstractPartitionAssignor</li> <li><code>StreamsPartitionAssignor</code> (Kafka Streams)</li> </ul>"},{"location":"clients/consumer/ConsumerPartitionAssignor/#onassignment","title":"onAssignment <pre><code>void onAssignment(\n  Assignment assignment,\n  ConsumerGroupMetadata metadata)\n</code></pre> <p><code>onAssignment</code>...FIXME</p> <p><code>onAssignment</code> is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"clients/consumer/ConsumerPartitionAssignor/#supportedprotocols","title":"supportedProtocols <pre><code>List&lt;RebalanceProtocol&gt; supportedProtocols()\n</code></pre> <p>Default: <code>RebalanceProtocol.EAGER</code></p> <p><code>supportedProtocols</code> is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"clients/consumer/ConsumerPartitionAssignor/#subscriptionuserdata","title":"subscriptionUserData <pre><code>ByteBuffer subscriptionUserData(\n  Set&lt;String&gt; topics)\n</code></pre> <p><code>subscriptionUserData</code> is <code>null</code> by default.</p> <p><code>subscriptionUserData</code> is used when:</p> <ul> <li><code>ConsumerCoordinator</code> is requested for metadata</li> </ul>","text":""},{"location":"clients/consumer/Fetcher/","title":"Fetcher","text":"<p><code>Fetcher&lt;K, V&gt;</code> is used by KafkaConsumer for fetching records.</p>"},{"location":"clients/consumer/Fetcher/#creating-instance","title":"Creating Instance","text":"<p><code>Fetcher</code> takes the following to be created:</p> <ul> <li> <code>LogContext</code> <li> ConsumerNetworkClient <li> fetch.min.bytes <li> fetch.max.bytes <li> fetch.max.wait.ms <li> max.partition.fetch.bytes <li> max.poll.records <li> check.crcs <li> client.rack <li> Key <code>Deserializer</code> <li> Value <code>Deserializer</code> <li> <code>ConsumerMetadata</code> <li> SubscriptionState <li> <code>Metrics</code> <li> <code>FetcherMetricsRegistry</code> <li> <code>Time</code> <li> retry.backoff.ms <li> request.timeout.ms <li>IsolationLevel</li> <li> <code>ApiVersions</code> <p><code>Fetcher</code> is created\u00a0along with KafkaConsumer.</p>"},{"location":"clients/consumer/Fetcher/#isolationlevel","title":"IsolationLevel <p><code>Fetcher</code> is given an <code>IsolationLevel</code> when created (based on isolation.level configuration property)</p> <p><code>Fetcher</code> uses the <code>IsolationLevel</code> for the following:</p> <ul> <li>sendFetches (and prepareFetchRequests)</li> <li>fetchOffsetsByTimes</li> <li>fetchRecords</li> <li>sendListOffsetRequest</li> </ul>","text":""},{"location":"clients/consumer/Fetcher/#sending-fetch-requests","title":"Sending Fetch Requests <pre><code>int sendFetches()\n</code></pre> <p><code>sendFetches</code> prepare fetch requests for the nodes with the assigned partitions (preferred read replicas or leaders).</p> <p>For every fetch request,  <code>sendFetches</code> prints out the following DEBUG message to the logs:</p> <pre><code>Sending [isolationLevel] [data] to broker [fetchTarget]\n</code></pre> <p><code>sendFetches</code> requests the ConsumerNetworkClient to send the fetch request (to the <code>fetchTarget</code> ndoe) and registers the node in the nodesWithPendingFetchRequests.</p> <p>On successful response, for every partitions <code>sendFetches</code> prints out the following DEBUG message to the logs and adds a new <code>CompletedFetch</code> to the completedFetches registry.</p> <pre><code>Fetch [isolationLevel] at offset [fetchOffset] for partition [partition]\nreturned fetch data [partitionData]\n</code></pre> <p>In the end, <code>sendFetches</code> removes the request from the nodesWithPendingFetchRequests registry.</p> <p><code>sendFetches</code>\u00a0is used when:</p> <ul> <li><code>KafkaConsumer</code> is requested to poll (and pollForFetches)</li> </ul>","text":""},{"location":"clients/consumer/Fetcher/#preparefetchrequests","title":"prepareFetchRequests <pre><code>Map&lt;Node, FetchSessionHandler.FetchRequestData&gt; prepareFetchRequests()\n</code></pre> <p><code>prepareFetchRequests</code>...FIXME</p>","text":""},{"location":"clients/consumer/Fetcher/#preferred-read-replica","title":"Preferred Read Replica <pre><code>Node selectReadReplica(\n  TopicPartition partition,\n  Node leaderReplica,\n  long currentTimeMs)\n</code></pre> <p><code>selectReadReplica</code> requests the SubscriptionState for the preferredReadReplica of the given <code>TopicPartition</code>.</p>","text":""},{"location":"clients/consumer/Fetcher/#offsetsfortimes","title":"offsetsForTimes <pre><code>Map&lt;TopicPartition, OffsetAndTimestamp&gt; offsetsForTimes(\n  Map&lt;TopicPartition, Long&gt; timestampsToSearch,\n  Timer timer)\n</code></pre> <p><code>offsetsForTimes</code>...FIXME</p> <p><code>offsetsForTimes</code>\u00a0is used when:</p> <ul> <li><code>KafkaConsumer</code> is requested to offsetsForTimes</li> </ul>","text":""},{"location":"clients/consumer/Fetcher/#beginningoffsets","title":"beginningOffsets <pre><code>Map&lt;TopicPartition, Long&gt; beginningOffsets(\n  Collection&lt;TopicPartition&gt; partitions,\n  Timer timer)\n</code></pre> <p><code>beginningOffsets</code>...FIXME</p> <p><code>beginningOffsets</code>\u00a0is used when:</p> <ul> <li><code>KafkaConsumer</code> is requested to beginningOffsets</li> </ul>","text":""},{"location":"clients/consumer/Fetcher/#endoffsets","title":"endOffsets <pre><code>Map&lt;TopicPartition, Long&gt; endOffsets(\n  Collection&lt;TopicPartition&gt; partitions,\n  Timer timer)\n</code></pre> <p><code>endOffsets</code>...FIXME</p> <p><code>endOffsets</code>\u00a0is used when:</p> <ul> <li><code>KafkaConsumer</code> is requested to endOffsets and currentLag</li> </ul>","text":""},{"location":"clients/consumer/Fetcher/#beginningorendoffset","title":"beginningOrEndOffset <pre><code>Map&lt;TopicPartition, Long&gt; beginningOrEndOffset(\n  Collection&lt;TopicPartition&gt; partitions,\n  long timestamp,\n  Timer timer)\n</code></pre> <p><code>beginningOrEndOffset</code>...FIXME</p> <p><code>beginningOrEndOffset</code>\u00a0is used when:</p> <ul> <li><code>Fetcher</code> is requested to beginningOffsets and endOffsets</li> </ul>","text":""},{"location":"clients/consumer/Fetcher/#fetchoffsetsbytimes","title":"fetchOffsetsByTimes <pre><code>ListOffsetResult fetchOffsetsByTimes(\n  Map&lt;TopicPartition, Long&gt; timestampsToSearch,\n  Timer timer,\n  boolean requireTimestamps)\n</code></pre> <p><code>fetchOffsetsByTimes</code>...FIXME</p> <p><code>fetchOffsetsByTimes</code>\u00a0is used when:</p> <ul> <li><code>Fetcher</code> is requested to offsetsForTimes and beginningOrEndOffset</li> </ul>","text":""},{"location":"clients/consumer/Fetcher/#sendlistoffsetsrequests","title":"sendListOffsetsRequests <pre><code>RequestFuture&lt;ListOffsetResult&gt; sendListOffsetsRequests(\n  Map&lt;TopicPartition, Long&gt; timestampsToSearch,\n  boolean requireTimestamps)\n</code></pre> <p><code>sendListOffsetsRequests</code>...FIXME</p>","text":""},{"location":"clients/consumer/Fetcher/#fetched-records","title":"Fetched Records <pre><code>Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; fetchedRecords()\n</code></pre> <p><code>fetchedRecords</code> returns up to max.poll.records number of records from the CompletedFetch Queue.</p>  <p>For <code>nextInLineFetch</code> unintialized or consumed already, <code>fetchedRecords</code> takes a peek at a <code>CompletedFetch</code> collection of records (in the CompletedFetch Queue). If uninitialized, <code>fetchedRecords</code> initializeCompletedFetch with the records. <code>fetchedRecords</code> saves the <code>CompletedFetch</code> records to the nextInLineFetch internal registry. <code>fetchedRecords</code> takes the <code>CompletedFetch</code> collection of records out (off the CompletedFetch Queue).</p> <p>For the partition of the nextInLineFetch collection of records paused, <code>fetchedRecords</code> prints out the following DEBUG message to the logs and <code>null</code>s the nextInLineFetch registry.</p> <pre><code>Skipping fetching records for assigned partition [p] because it is paused\n</code></pre> <p>For all the other cases, <code>fetchedRecords</code> fetches the records out of the nextInLineFetch collection of records (up to the number of records left to fetch).</p> <p>In the end, <code>fetchedRecords</code> returns the <code>ConsumerRecord</code>s per <code>TopicPartition</code> (out of the CompletedFetch Queue).</p> <p><code>fetchedRecords</code>\u00a0is used when:</p> <ul> <li><code>KafkaConsumer</code> is requested to pollForFetches</li> </ul>","text":""},{"location":"clients/consumer/Fetcher/#fetchrecords","title":"fetchRecords <pre><code>List&lt;ConsumerRecord&lt;K, V&gt;&gt; fetchRecords(\n  CompletedFetch completedFetch,\n  int maxRecords)\n</code></pre> <p><code>fetchRecords</code>...FIXME</p>","text":""},{"location":"clients/consumer/Fetcher/#initializecompletedfetch","title":"initializeCompletedFetch <pre><code>CompletedFetch initializeCompletedFetch(\n  CompletedFetch nextCompletedFetch)\n</code></pre> <p><code>initializeCompletedFetch</code> returns the given <code>CompletedFetch</code> if there were no errors. <code>initializeCompletedFetch</code> updates the SubscriptionState with the current metadata about the partition.</p>  <p><code>initializeCompletedFetch</code> takes the partition, the <code>PartitionData</code> and the fetch offset from the given <code>CompletedFetch</code>.</p> <p><code>initializeCompletedFetch</code> prints out the following TRACE message to the logs:</p> <pre><code>Preparing to read [n] bytes of data for partition [p] with offset [o]\n</code></pre> <p><code>initializeCompletedFetch</code> takes the <code>RecordBatch</code>es from the <code>PartitionData</code>.</p> <p>With a high watermark given, <code>initializeCompletedFetch</code> prints out the following TRACE message to the logs and requests the SubscriptionState to updateHighWatermark for the partition.</p> <pre><code>Updating high watermark for partition [p] to [highWatermark]\n</code></pre> <p>With a log start offset given, <code>initializeCompletedFetch</code> prints out the following TRACE message to the logs and requests the SubscriptionState to updateLogStartOffset for the partition.</p> <pre><code>Updating log start offset for partition [p] to [logStartOffset]\n</code></pre> <p>With a last stable offset given, <code>initializeCompletedFetch</code> prints out the following TRACE message to the logs and requests the SubscriptionState to updateLastStableOffset for the partition.</p> <pre><code>Updating last stable offset for partition [p] to [lastStableOffset]\n</code></pre> <p>With a preferred read replica given, <code>initializeCompletedFetch</code> prints out the following DEBUG message to the logs and requests the SubscriptionState to updatePreferredReadReplica for the partition.</p> <pre><code>Updating preferred read replica for partition [p] to [preferredReadReplica] set to expire at [expireTimeMs]\n</code></pre> <p>For errors like <code>NOT_LEADER_OR_FOLLOWER</code>, <code>REPLICA_NOT_AVAILABLE</code>, <code>FENCED_LEADER_EPOCH</code>, <code>KAFKA_STORAGE_ERROR</code>, <code>OFFSET_NOT_AVAILABLE</code>, <code>initializeCompletedFetch</code> prints out the following DEBUG message to the logs and requests the ConsumerMetadata to <code>requestUpdate</code>.</p> <pre><code>Error in fetch for partition [p]: [exceptionName]\n</code></pre> <p>For <code>OFFSET_OUT_OF_RANGE</code> error, <code>initializeCompletedFetch</code> requests the SubscriptionState to clearPreferredReadReplica for the partition. With no preferred read replica, it is assumed that the fetch came from the leader.</p>","text":""},{"location":"clients/consumer/Fetcher/#resetoffsetsifneeded","title":"resetOffsetsIfNeeded <pre><code>void resetOffsetsIfNeeded()\n</code></pre> <p><code>resetOffsetsIfNeeded</code>...FIXME</p> <p><code>resetOffsetsIfNeeded</code>\u00a0is used when:</p> <ul> <li><code>KafkaConsumer</code> is requested to updateFetchPositions</li> </ul>","text":""},{"location":"clients/consumer/Fetcher/#resetoffsetsasync","title":"resetOffsetsAsync <pre><code>void resetOffsetsAsync(\n  Map&lt;TopicPartition, Long&gt; partitionResetTimestamps)\n</code></pre> <p><code>resetOffsetsAsync</code>...FIXME</p>","text":""},{"location":"clients/consumer/Fetcher/#sendlistoffsetrequest","title":"sendListOffsetRequest <pre><code>RequestFuture&lt;ListOffsetResult&gt; sendListOffsetRequest(\n  Node node,\n  Map&lt;TopicPartition, ListOffsetsPartition&gt; timestampsToSearch,\n  boolean requireTimestamp)\n</code></pre> <p><code>sendListOffsetRequest</code>...FIXME</p> <p><code>sendListOffsetRequest</code>\u00a0is used when:</p> <ul> <li><code>Fetcher</code> is requested to resetOffsetsIfNeeded (via resetOffsetsAsync) and fetchOffsetsByTimes (via sendListOffsetsRequests)</li> </ul>","text":""},{"location":"clients/consumer/Fetcher/#clearbuffereddataforunassignedtopics","title":"clearBufferedDataForUnassignedTopics <pre><code>void clearBufferedDataForUnassignedTopics(\n  Collection&lt;String&gt; assignedTopics)\n</code></pre> <p><code>clearBufferedDataForUnassignedTopics</code>...FIXME</p> <p><code>clearBufferedDataForUnassignedTopics</code>\u00a0is used when:</p> <ul> <li><code>KafkaConsumer</code> is requested to subscribe</li> </ul>","text":""},{"location":"clients/consumer/Fetcher/#completedfetch-queue","title":"CompletedFetch Queue <p><code>Fetcher</code> creates an empty <code>ConcurrentLinkedQueue</code> (Java) of <code>CompletedFetch</code>es when created.</p> <p>New <code>CompletedFetch</code>es (one per partition) are added to the queue in sendFetches (on a successful receipt of response from a Kafka cluster).</p>","text":""},{"location":"clients/consumer/Fetcher/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.clients.consumer.internals.Fetcher</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.clients.consumer.internals.Fetcher=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"clients/consumer/KafkaConsumer/","title":"KafkaConsumer","text":"<p><code>KafkaConsumer</code> is a Consumer.</p>"},{"location":"clients/consumer/KafkaConsumer/#creating-instance","title":"Creating Instance","text":"<p><code>KafkaConsumer</code> takes the following to be created:</p> <ul> <li> Configuration (ConsumerConfig or <code>Map&lt;String, Object&gt;</code> or <code>Properties</code>) <li> <code>Deserializer&lt;K&gt;</code> <li> <code>Deserializer&lt;V&gt;</code>"},{"location":"clients/consumer/KafkaConsumer/#group-id","title":"Group ID <p><code>KafkaConsumer</code> can be given a group ID using group.id (indirectly in the config) configuration property when created.</p>","text":""},{"location":"clients/consumer/KafkaConsumer/#isolationlevel","title":"IsolationLevel <p><code>KafkaConsumer</code> can be given an <code>IsolationLevel</code> using isolation.level configuration property (indirectly in the config) when created.</p> <p><code>KafkaConsumer</code> uses the <code>IsolationLevel</code> for the following:</p> <ul> <li>Creating a Fetcher (when created)</li> <li>currentLag</li> </ul>","text":""},{"location":"clients/consumer/KafkaConsumer/#fetcher","title":"Fetcher <p><code>KafkaConsumer</code> creates a Fetcher when created.</p>","text":""},{"location":"clients/consumer/KafkaConsumer/#consumercoordinator","title":"ConsumerCoordinator <p><code>KafkaConsumer</code> creates a ConsumerCoordinator when created with the group.id specified.</p>","text":""},{"location":"clients/consumer/KafkaConsumer/#enforcerebalance","title":"enforceRebalance <pre><code>void enforceRebalance()\n</code></pre> <p><code>enforceRebalance</code> requests the ConsumerCoordinator to requestRejoin with the following reason:</p> <pre><code>rebalance enforced by user\n</code></pre> <p><code>enforceRebalance</code>\u00a0is part of the Consumer abstraction.</p>","text":""},{"location":"clients/consumer/KafkaConsumer/#groupmetadata","title":"groupMetadata <pre><code>ConsumerGroupMetadata groupMetadata()\n</code></pre> <p><code>groupMetadata</code>...FIXME</p> <p><code>groupMetadata</code>\u00a0is part of the Consumer abstraction.</p>","text":""},{"location":"clients/consumer/KafkaConsumer/#polling-for-records","title":"Polling for Records <pre><code>ConsumerRecords&lt;K, V&gt; poll(\n  Duration timeout) // (1)\nConsumerRecords&lt;K, V&gt; poll(\n  Timer timer,\n  boolean includeMetadataInTimeout) // (2)\n</code></pre> <ol> <li>Uses <code>includeMetadataInTimeout</code> enabled (<code>true</code>)</li> <li>A private method</li> </ol> <p><code>poll</code>...FIXME</p> <p><code>poll</code>\u00a0is part of the Consumer abstraction.</p>","text":""},{"location":"clients/consumer/KafkaConsumer/#polling-for-fetches","title":"Polling for Fetches <pre><code>Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; pollForFetches(\n  Timer timer)\n</code></pre> <p><code>pollForFetches</code> requests the Fetcher for fetched records and returns them immediately if available.</p> <p>Otherwise, <code>pollForFetches</code> requests the Fetcher to sendFetches.</p> <p><code>pollForFetches</code> prints out the following TRACE message to the logs:</p> <pre><code>Polling for fetches with timeout [pollTimeout]\n</code></pre> <p><code>pollForFetches</code> requests the ConsumerNetworkClient to poll (with the <code>pollTimeout</code> until it expires or the Fetcher has some available fetches ready).</p> <p>In the end, <code>pollForFetches</code> requests the Fetcher for the fetched records again.</p>","text":""},{"location":"clients/consumer/KafkaConsumer/#subscribing-to-topics","title":"Subscribing to Topics <pre><code>void subscribe(\n  Collection&lt;String&gt; topics) // (1)\nvoid subscribe(\n  Collection&lt;String&gt; topics,\n  ConsumerRebalanceListener listener)\nvoid subscribe(\n  Pattern pattern) // (2)\nvoid subscribe(\n  Pattern pattern,\n  ConsumerRebalanceListener callback)\n</code></pre> <ol> <li>Uses <code>NoOpConsumerRebalanceListener</code></li> <li>Uses <code>NoOpConsumerRebalanceListener</code></li> </ol> <p><code>subscribe</code>...FIXME</p> <p><code>subscribe</code>\u00a0is part of the Consumer abstraction.</p>","text":""},{"location":"clients/consumer/KafkaConsumer/#waking-up","title":"Waking Up <pre><code>void wakeup()\n</code></pre> <p><code>wakeup</code>...FIXME</p> <p><code>wakeup</code>\u00a0is part of the Consumer abstraction.</p>","text":""},{"location":"clients/consumer/SubscriptionState/","title":"SubscriptionState","text":""},{"location":"clients/consumer/SubscriptionState/#preferred-read-replica","title":"Preferred Read Replica","text":""},{"location":"clients/consumer/SubscriptionState/#preferredreadreplica","title":"preferredReadReplica <pre><code>Optional&lt;Integer&gt; preferredReadReplica(\n  TopicPartition tp,\n  long timeMs)\n</code></pre> <p><code>preferredReadReplica</code> looks up the state of the given <code>TopicPartition</code> and, if found, requests it for the preferredReadReplica. Otherwise, <code>preferredReadReplica</code> returns an undefined preferred read replica.</p> <p><code>preferredReadReplica</code>\u00a0is used when:</p> <ul> <li><code>Fetcher</code> is requested to selectReadReplica</li> </ul>","text":""},{"location":"clients/consumer/SubscriptionState/#updatepreferredreadreplica","title":"updatePreferredReadReplica <pre><code>void updatePreferredReadReplica(\n  TopicPartition tp,\n  int preferredReadReplicaId,\n  LongSupplier timeMs)\n</code></pre> <p><code>updatePreferredReadReplica</code> looks up the state of the given <code>TopicPartition</code> and requests it to updatePreferredReadReplica.</p> <p><code>updatePreferredReadReplica</code>\u00a0is used when:</p> <ul> <li><code>Fetcher</code> is requested to initializeCompletedFetch</li> </ul>","text":""},{"location":"clients/consumer/SubscriptionState/#clearpreferredreadreplica","title":"clearPreferredReadReplica <pre><code>Optional&lt;Integer&gt; clearPreferredReadReplica(\n  TopicPartition tp)\n</code></pre> <p><code>clearPreferredReadReplica</code> looks up the state of the given <code>TopicPartition</code> and requests it to clearPreferredReadReplica.</p> <p><code>clearPreferredReadReplica</code>\u00a0is used when:</p> <ul> <li><code>Fetcher</code> is requested to selectReadReplica and initializeCompletedFetch</li> </ul>","text":""},{"location":"clients/consumer/TopicPartitionState/","title":"TopicPartitionState","text":""},{"location":"clients/consumer/TopicPartitionState/#preferredreadreplica","title":"preferredReadReplica <pre><code>Integer preferredReadReplica\n</code></pre> <p><code>TopicPartitionState</code> manages the preferred read replica (of a <code>TopicPartition</code>) for a specified amount of time (until expires or is cleared out).</p> <p><code>preferredReadReplica</code> is used when:</p> <ul> <li><code>SubscriptionState</code> is requested for the preferredReadReplica, updatePreferredReadReplica and clearPreferredReadReplica</li> </ul>","text":""},{"location":"clients/consumer/preferred-read-replica/","title":"Preferred Read Replica","text":"<p>Preferred Read Replica is the broker ID of one of the in-sync replicas of a partition for Kafka Consumer to read records from.</p>"},{"location":"clients/producer/","title":"Kafka Producers","text":"<p>KafkaProducer uses RecordAccumulator with records to be sent out.</p> <p><code>KafkaProducer</code> groups together records that arrive in-between request transmissions into a single batched request. Normally this occurs only under load when records arrive faster than they can be sent out. However in some circumstances the client may want to reduce the number of requests even under moderate load using linger.ms configuration property.</p> <p><code>KafkaProducer</code> uses Sender to send records to a Kafka cluster.</p> <p><code>KafkaProducer</code> can be transactional or idempotent (and associated with a TransactionManager).</p>"},{"location":"clients/producer/#demo","title":"Demo","text":"<pre><code>// Necessary imports\nimport org.apache.kafka.clients.producer.KafkaProducer\nimport org.apache.kafka.clients.producer.ProducerConfig\nimport org.apache.kafka.common.serialization.StringSerializer\n\n// Creating a KafkaProducer\nimport java.util.Properties\nval props = new Properties()\nprops.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName)\nprops.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName)\nprops.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \":9092\")\nval producer = new KafkaProducer[String, String](props)\n\n// Creating a record to be sent\nimport org.apache.kafka.clients.producer.ProducerRecord\nval r = new ProducerRecord[String, String](\"0\", \"this is a message\")\n\n// Sending the record (with no Callback)\nimport java.util.concurrent.Future\nimport org.apache.kafka.clients.producer.RecordMetadata\nval metadataF: Future[RecordMetadata] = producer.send(r)\n</code></pre>"},{"location":"clients/producer/BufferPool/","title":"BufferPool","text":"<p><code>BufferPool</code> is created alongsize KafkaProducer for the RecordAccumulator.</p>"},{"location":"clients/producer/BufferPool/#creating-instance","title":"Creating Instance","text":"<p><code>BufferPool</code> takes the following to be created:</p> <ul> <li>Total Memory Size</li> <li>Batch Size</li> <li> Metrics <li> <code>Time</code> <li> Metric Group Name <p><code>BufferPool</code> is created when:</p> <ul> <li><code>KafkaProducer</code> is created (to create a RecordAccumulator)</li> </ul>"},{"location":"clients/producer/BufferPool/#memory","title":"Total Memory Size","text":"<p><code>BufferPool</code> is given the total memory size when created.</p> <p>The size is by default the value of buffer.memory configuration property.</p>"},{"location":"clients/producer/BufferPool/#poolableSize","title":"Batch Size","text":"<p><code>BufferPool</code> is given the batch size when created.</p> <p>The size is by default the value of batch.size configuration property.</p>"},{"location":"clients/producer/BufferPool/#metrics","title":"Metrics","text":"<p><code>BufferPool</code> registers the metrics under the producer-metrics group name.</p>"},{"location":"clients/producer/BufferPool/#buffer-exhausted-rate","title":"buffer-exhausted-rate","text":"<p>The average per-second number of record sends that are dropped due to buffer exhaustion</p>"},{"location":"clients/producer/BufferPool/#buffer-exhausted-records","title":"buffer-exhausted-records","text":""},{"location":"clients/producer/BufferPool/#buffer-exhausted-total","title":"buffer-exhausted-total","text":"<p>The total number of record sends that are dropped due to buffer exhaustion</p>"},{"location":"clients/producer/BufferPool/#bufferpool-wait-ratio","title":"bufferpool-wait-ratio","text":"<p>The fraction of time an appender waits for space allocation</p>"},{"location":"clients/producer/BufferPool/#bufferpool-wait-time-ns-total","title":"bufferpool-wait-time-ns-total","text":"<p>The total time (in ns) an appender waits for space allocation</p>"},{"location":"clients/producer/Callback/","title":"Callback","text":"<p><code>Callback</code> is...FIXME</p>"},{"location":"clients/producer/DefaultPartitioner/","title":"DefaultPartitioner","text":"<p><code>DefaultPartitioner</code> is a Partitioner.</p>"},{"location":"clients/producer/DefaultPartitioner/#demo","title":"Demo","text":"<pre><code>import org.apache.kafka.clients.producer.internals.DefaultPartitioner\nval partitioner = new DefaultPartitioner\n\nval keyBytes = \"hello\".getBytes\nval numPartitions = 3\n\nval p = partitioner.partition(null, null, keyBytes, null, null, null, numPartitions)\n\nprintln(p)\n</code></pre> <p>The following snippet should generate the same partition value (since it is exactly how <code>DefaultPartitioner</code> does it).</p> <pre><code>import org.apache.kafka.common.utils.Utils\n\nval keyBytes = \"hello\".getBytes\nval numPartitions = 3\n\nval p = Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions\n\nprintln(p)\n</code></pre>"},{"location":"clients/producer/KafkaProducer/","title":"KafkaProducer","text":"<p><code>KafkaProducer&lt;K, V&gt;</code> is a concrete Producer.</p>"},{"location":"clients/producer/KafkaProducer/#creating-instance","title":"Creating Instance","text":"<p><code>KafkaProducer</code> takes the following to be created:</p> <ul> <li> ProducerConfig <li> Key <code>Serializer&lt;K&gt;</code> <li> Value <code>Serializer&lt;V&gt;</code> <li> ProducerMetadata <li> KafkaClient <li> <code>ProducerInterceptor&lt;K, V&gt;</code>s <li> Time"},{"location":"clients/producer/KafkaProducer/#configuretransactionstate","title":"configureTransactionState <pre><code>TransactionManager configureTransactionState(\n  ProducerConfig config,\n  LogContext logContext)\n</code></pre> <p><code>configureTransactionState</code> creates a new TransactionManager or returns <code>null</code>.</p>  <p><code>configureTransactionState</code> checks whether the following configuration properties are specified in the given ProducerConfig:</p> <ol> <li>enable.idempotence</li> <li>transactional.id</li> </ol> <p>With transactional.id specified, <code>configureTransactionState</code> turns the enable.idempotence on and prints out the following INFO message to the logs:</p> <pre><code>Overriding the default [enable.idempotence] to true since transactional.id is specified.\n</code></pre> <p>With idempotence enabled, <code>configureTransactionState</code> creates a TransactionManager with the values of the following configuration properties:</p> <ol> <li>transactional.id</li> <li>transaction.timeout.ms</li> <li>retry.backoff.ms</li> </ol> <p>When the <code>TransactionManager</code> is transactional, <code>configureTransactionState</code> prints out the following INFO message to the logs:</p> <pre><code>Instantiated a transactional producer.\n</code></pre> <p>Otherwise, <code>configureTransactionState</code> prints out the following INFO message to the logs:</p> <pre><code>Instantiated an idempotent producer.\n</code></pre> <p>In the end, <code>configureTransactionState</code> returns the <code>TransactionManager</code> or <code>null</code>.</p>","text":""},{"location":"clients/producer/KafkaProducer/#newsender","title":"newSender <pre><code>Sender newSender(\n  LogContext logContext,\n  KafkaClient kafkaClient,\n  ProducerMetadata metadata)\n</code></pre> <p><code>newSender</code>...FIXME</p>","text":""},{"location":"clients/producer/KafkaProducer/#configureinflightrequests","title":"configureInflightRequests <pre><code>int configureInflightRequests(\n  ProducerConfig config)\n</code></pre> <p><code>configureInflightRequests</code> gives the value of the max.in.flight.requests.per.connection (in the given <code>ProducerConfig</code>).</p> <p><code>configureInflightRequests</code> throws a <code>ConfigException</code> when the idempotence is enabled and the value of the max.in.flight.requests.per.connection is above 5:</p> <pre><code>Must set max.in.flight.requests.per.connection to at most 5 to use the idempotent producer.\n</code></pre>","text":""},{"location":"clients/producer/KafkaProducer/#configureacks","title":"configureAcks <pre><code>short configureAcks(\n  ProducerConfig config,\n  Logger log)\n</code></pre> <p><code>configureAcks</code> returns the value of acks configuration property (in the given ProducerConfig).</p> <p>With idempotenceEnabled, <code>configureAcks</code> prints out the following INFO message to the logs when there is no acks configuration property defined:</p> <pre><code>Overriding the default [acks] to all since idempotence is enabled.\n</code></pre> <p>With idempotenceEnabled and the <code>acks</code> not <code>-1</code>, <code>configureAcks</code> throws a <code>ConfigException</code>:</p> <pre><code>Must set acks to all in order to use the idempotent producer.\nOtherwise we cannot guarantee idempotence.\n</code></pre>","text":""},{"location":"clients/producer/KafkaProducer/#configuredeliverytimeout","title":"configureDeliveryTimeout <pre><code>int configureDeliveryTimeout(\n  ProducerConfig config,\n  Logger log)\n</code></pre> <p><code>configureDeliveryTimeout</code>...FIXME</p>","text":""},{"location":"clients/producer/KafkaProducer/#transactionmanager","title":"TransactionManager <p><code>KafkaProducer</code> may create a TransactionManager when created (with idempotenceEnabled).</p> <p><code>TransactionManager</code> is used to create the following:</p> <ul> <li>RecordAccumulator</li> <li>Sender</li> </ul> <p><code>KafkaProducer</code> uses the <code>TransactionManager</code> for the following transactional methods:</p> <ul> <li>abortTransaction</li> <li>beginTransaction</li> <li>commitTransaction</li> <li>initTransactions</li> <li>sendOffsetsToTransaction</li> <li>doSend</li> </ul>","text":""},{"location":"clients/producer/KafkaProducer/#throwifnotransactionmanager","title":"throwIfNoTransactionManager <p><code>KafkaProducer</code> throws an <code>IllegalStateException</code> for the transactional methods but TransactionManager is not configured.</p> <pre><code>Cannot use transactional methods without enabling transactions by setting the transactional.id configuration property\n</code></pre>","text":""},{"location":"clients/producer/KafkaProducer/#sender-thread","title":"Sender Thread <p><code>KafkaProducer</code> creates a Sender when created.</p> <p><code>Sender</code> is immediately started as a daemon thread with the following name (using the clientId):</p> <pre><code>kafka-producer-network-thread | [clientId]\n</code></pre> <p><code>KafkaProducer</code> is actually considered open (and usable) as long as the <code>Sender</code> is running.</p> <p><code>KafkaProducer</code> simply requests the <code>Sender</code> to wake up for the following:</p> <ul> <li>initTransactions</li> <li>sendOffsetsToTransaction</li> <li>commitTransaction</li> <li>abortTransaction</li> <li>doSend</li> <li>waitOnMetadata</li> <li>flush</li> </ul>","text":""},{"location":"clients/producer/KafkaProducer/#recordaccumulator","title":"RecordAccumulator <p><code>KafkaProducer</code> creates a RecordAccumulator when created.</p> <p>This <code>RecordAccumulator</code> is used for the following:</p> <ul> <li>Create a Sender</li> <li>append when doSend</li> <li>beginFlush when flush</li> </ul>","text":""},{"location":"clients/producer/KafkaProducer/#maxblockms","title":"max.block.ms <p><code>KafkaProducer</code> uses max.block.ms configuration property.</p>","text":""},{"location":"clients/producer/KafkaProducer/#transactional-methods","title":"Transactional Methods","text":""},{"location":"clients/producer/KafkaProducer/#aborttransaction","title":"abortTransaction <pre><code>void abortTransaction()\n</code></pre> <p><code>abortTransaction</code> prints out the following INFO message to the logs:</p> <pre><code>Aborting incomplete transaction\n</code></pre> <p><code>abortTransaction</code>...FIXME</p> <p><code>abortTransaction</code>\u00a0is part of the Producer abstraction.</p>","text":""},{"location":"clients/producer/KafkaProducer/#begintransaction","title":"beginTransaction <pre><code>void beginTransaction()\n</code></pre> <p><code>beginTransaction</code> requests the TransactionManager to beginTransaction.</p> <p><code>beginTransaction</code>\u00a0is part of the Producer abstraction.</p>","text":""},{"location":"clients/producer/KafkaProducer/#inittransactions","title":"initTransactions <pre><code>void initTransactions()\n</code></pre> <p><code>initTransactions</code> requests the TransactionManager to initializeTransactions and requests the Sender to wakeup.</p> <p>In the end, <code>initTransactions</code> waits max.block.ms until transaction initialization is completed (successfully or not).</p> <p><code>initTransactions</code>\u00a0is part of the Producer abstraction.</p>","text":""},{"location":"clients/producer/KafkaProducer/#sendoffsetstotransaction","title":"sendOffsetsToTransaction <pre><code>void sendOffsetsToTransaction(\n  Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,\n  ConsumerGroupMetadata groupMetadata)\n</code></pre> <p><code>sendOffsetsToTransaction</code> requests the TransactionManager to sendOffsetsToTransaction and requests the Sender to wakeup.</p> <p>In the end, <code>sendOffsetsToTransaction</code> waits max.block.ms for the send to be completed (successfully or not).</p> <p><code>sendOffsetsToTransaction</code>\u00a0is part of the Producer abstraction.</p>","text":""},{"location":"clients/producer/KafkaProducer/#sending-record","title":"Sending Record <pre><code>Future&lt;RecordMetadata&gt; send(\n  ProducerRecord&lt;K, V&gt; record) // (1)\nFuture&lt;RecordMetadata&gt; send(\n  ProducerRecord&lt;K, V&gt; record,\n  Callback callback)\n</code></pre> <ol> <li>Uses uninitialized Callback (<code>null</code>)</li> </ol> <p><code>send</code> requests the interceptors to onSend with the given <code>record</code> (possibly modifying it) followed by doSend.</p> <p><code>send</code>\u00a0is part of the Producer abstraction.</p>","text":""},{"location":"clients/producer/KafkaProducer/#dosend","title":"doSend <pre><code>Future&lt;RecordMetadata&gt; doSend(\n  ProducerRecord&lt;K, V&gt; record,\n  Callback callback)\n</code></pre> <p><code>doSend</code> waitOnMetadata for the topic and partition of the given record.</p> <p><code>doSend</code> requests the key Serializer to <code>serialize</code> the record (passing in the topic, the headers and the key of the record).</p> <p><code>doSend</code> requests the value Serializer to <code>serialize</code> the record (passing in the topic, the headers and the value of the record).</p> <p><code>doSend</code> determines the partition for the record.</p> <p><code>doSend</code> ensureValidRecordSize for the record (upper bound estimate).</p> <p><code>doSend</code> prints out the following TRACE message to the logs:</p> <pre><code>Attempting to append record [r] with callback [c] to topic [t] partition [p]\n</code></pre> <p><code>doSend</code> requests the RecordAccumulator to append the record (with the <code>abortOnNewBatch</code> flag enabled).</p> <p>When aborted for a new batch, <code>doSend</code>...FIXME (repeats the steps)...and prints out the following TRACE message to the logs:</p> <pre><code>Retrying append due to new batch creation for topic [t] partition [p].\nThe old partition was [prev]\n</code></pre> <p>When transactional, <code>doSend</code> requests the TransactionManager to maybeAddPartitionToTransaction.</p> <p>For <code>batchIsFull</code> or a new batch created, <code>doSend</code> prints out the following TRACE message to the logs and requests the Sender to wakeup.</p> <pre><code>Waking up the sender since topic [t] partition [p] is either full or getting a new batch\n</code></pre>","text":""},{"location":"clients/producer/KafkaProducer/#partition","title":"partition <pre><code>int partition(\n  ProducerRecord&lt;K, V&gt; record,\n  byte[] serializedKey,\n  byte[] serializedValue,\n  Cluster cluster)\n</code></pre> <p><code>partition</code> is the <code>partition</code> (of the given <code>ProducerRecord</code>) if defined or requests the Partitioner for the partition.</p>","text":""},{"location":"clients/producer/KafkaProducer/#flushing","title":"Flushing <pre><code>void flush()\n</code></pre> <p><code>flush</code> requests the RecordAccumulator to beginFlush.</p> <p><code>flush</code> requests the Sender to wakeup.</p> <p><code>flush</code> requests the RecordAccumulator to awaitFlushCompletion.</p> <p><code>flush</code>\u00a0is part of the Producer abstraction.</p>","text":""},{"location":"clients/producer/KafkaProducer/#waitOnMetadata","title":"waitOnMetadata <pre><code>ClusterAndWaitTime waitOnMetadata(\n  String topic,\n  Integer partition,\n  long nowMs,\n  long maxWaitMs)\n</code></pre> <p><code>waitOnMetadata</code> requests the ProducerMetadata for the current cluster info.</p> <p><code>waitOnMetadata</code>...FIXME</p>  <p><code>waitOnMetadata</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to doSend and partitionsFor</li> </ul>","text":""},{"location":"clients/producer/KafkaProducer/#lingerMs","title":"lingerMs <pre><code>lingerMs(\n  ProducerConfig config)\n</code></pre> <p><code>lingerMs</code> provides an upper bound on linger.ms to be up to the maximum integer value.</p>  <p><code>lingerMs</code> is used when:</p> <ul> <li><code>KafkaProducer</code> is created (to create the RecordAccumulator) and requested to configureDeliveryTimeout</li> </ul>","text":""},{"location":"clients/producer/KafkaProducer/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.clients.producer.KafkaProducer</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"clients/producer/Partitioner/","title":"Partitioner","text":"<p><code>Partitioner</code> is an abstraction of partitioners for a KafkaProducer to determine the partition of records (to be sent out).</p>"},{"location":"clients/producer/Partitioner/#configurable","title":"Configurable <p><code>Partitioner</code> is a Configurable.</p>","text":""},{"location":"clients/producer/Partitioner/#closeable","title":"Closeable <p><code>Partitioner</code> is a <code>Closeable</code> (Java).</p>","text":""},{"location":"clients/producer/Partitioner/#contract","title":"Contract","text":""},{"location":"clients/producer/Partitioner/#onnewbatch","title":"onNewBatch <pre><code>void onNewBatch(\n  String topic,\n  Cluster cluster,\n  int prevPartition)\n</code></pre> <p>Used when:</p> <ul> <li><code>KafkaProducer</code> is requested to send a record (and doSend)</li> </ul>","text":""},{"location":"clients/producer/Partitioner/#computing-partition","title":"Computing Partition <pre><code>int partition(\n  String topic,\n  Object key,\n  byte[] keyBytes,\n  Object value,\n  byte[] valueBytes,\n  Cluster cluster)\n</code></pre> <p>Used when:</p> <ul> <li><code>KafkaProducer</code> is requested to send a record (and determines the partition)</li> </ul>","text":""},{"location":"clients/producer/Partitioner/#implementations","title":"Implementations <ul> <li>DefaultPartitioner</li> <li>UniformStickyPartitioner</li> <li>RoundRobinPartitioner</li> </ul>","text":""},{"location":"clients/producer/Producer/","title":"Producer","text":"<p><code>Producer&lt;K, V&gt;</code> is an interface to KafkaProducer for Kafka developers to use to send records (with <code>K</code> keys and <code>V</code> values) to a Kafka cluster.</p>"},{"location":"clients/producer/Producer/#contract-subset","title":"Contract (Subset)","text":""},{"location":"clients/producer/Producer/#aborttransaction","title":"abortTransaction <pre><code>void abortTransaction()\n</code></pre>","text":""},{"location":"clients/producer/Producer/#begintransaction","title":"beginTransaction <pre><code>void beginTransaction()\n</code></pre>","text":""},{"location":"clients/producer/Producer/#committransaction","title":"commitTransaction <pre><code>void commitTransaction()\n</code></pre>","text":""},{"location":"clients/producer/Producer/#inittransactions","title":"initTransactions <pre><code>void initTransactions()\n</code></pre>","text":""},{"location":"clients/producer/Producer/#sendoffsetstotransaction","title":"sendOffsetsToTransaction <pre><code>void sendOffsetsToTransaction(\n  Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,\n  ConsumerGroupMetadata groupMetadata)\n</code></pre> <p>Used when the producer is also a Consumer for a consume-transform-produce pattern</p>","text":""},{"location":"clients/producer/ProducerBatch/","title":"ProducerBatch","text":""},{"location":"clients/producer/ProducerBatch/#creating-instance","title":"Creating Instance","text":"<p><code>ProducerBatch</code> takes the following to be created:</p> <ul> <li> <code>TopicPartition</code> <li> MemoryRecordsBuilder <li> createdMs <li> <code>isSplitBatch</code> flag (default: <code>false</code>) <p><code>ProducerBatch</code> is created\u00a0when:</p> <ul> <li><code>ProducerBatch</code> is requested to createBatchOffAccumulatorForRecord</li> <li><code>RecordAccumulator</code> is requested to append a record</li> </ul>"},{"location":"clients/producer/ProducerBatch/#tryappend","title":"tryAppend <pre><code>FutureRecordMetadata tryAppend(\n  long timestamp,\n  byte[] key,\n  byte[] value,\n  Header[] headers,\n  Callback callback,\n  long now)\n</code></pre> <p><code>tryAppend</code>...FIXME</p> <p><code>tryAppend</code>\u00a0is used when:</p> <ul> <li><code>RecordAccumulator</code> is requested to append a record</li> </ul>","text":""},{"location":"clients/producer/ProducerConfig/","title":"ProducerConfig","text":""},{"location":"clients/producer/ProducerConfig/#acks","title":"acks","text":""},{"location":"clients/producer/ProducerConfig/#batchsize","title":"batch.size <p>The buffer size allocated for a partition. When records are received (which are smaller than this size) KafkaProducer will attempt to optimistically group them together until this size is reached.</p> <p>Default: <code>16384</code></p> <p>Must be at least 0. If <code>0</code>, <code>KafkaProducer</code> will assume <code>1</code> anyway (to create the RecordAccumulator)</p> <p>Related to:</p> <ul> <li>linger.ms</li> <li><code>max-partition-memory-bytes</code> (<code>ConsoleProducer</code>)</li> </ul> <p>Used when:</p> <ul> <li><code>KafkaProducer</code> is created (to create a RecordAccumulator and an accompanying BufferPool)</li> <li><code>KafkaLog4jAppender</code> is requested to <code>activateOptions</code></li> </ul>","text":""},{"location":"clients/producer/ProducerConfig/#enableidempotence","title":"enable.idempotence <p>Default: <code>false</code></p> <p>Used when:</p> <ul> <li><code>KafkaProducer</code> is requested to configureTransactionState</li> <li><code>ProducerConfig</code> is requested to maybeOverrideEnableIdempotence and idempotenceEnabled</li> </ul>","text":""},{"location":"clients/producer/ProducerConfig/#linger.ms","title":"linger.ms <p>A delay for how long a KafkaProducer should wait before sending out a record to allow other records to be sent so that the sends can be batched together. This can be thought of as analogous to Nagle's algorithm in TCP.</p>  Cambridge Dictionary <p>linger verb</p>  <p>to take a long time to leave or disappear</p>   <p>This setting gives the upper bound on the delay for batching: once we get batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if we have fewer than this many bytes accumulated for this partition we will 'linger' for the specified time waiting for more records to show up.</p> <p>Default: <code>0</code> (no delay)</p> <p>For example, <code>5</code> would have the effect of reducing the number of requests sent out but would add up to <code>5</code> ms of latency to records sent in the absence of load.</p>  Kafka Streams <p>Kafka Streams prefers <code>linger.ms</code> to be <code>100</code>.</p>  <p>Available as lingerMs</p> <p>Correlated with:</p> <ul> <li>delivery.timeout.ms</li> <li>request.timeout.ms</li> </ul>","text":""},{"location":"clients/producer/ProducerConfig/#maxblockms","title":"max.block.ms","text":""},{"location":"clients/producer/ProducerConfig/#maxinflightrequestsperconnection","title":"max.in.flight.requests.per.connection <p>The maximum number of unacknowledged requests the client will send on a single connection before blocking. Note that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled).</p> <p>Default: <code>5</code></p> <p>Must be at least 1</p> <p>Related to:</p> <ul> <li>enable.idempotence</li> <li>retries</li> </ul> <p>Used when:</p> <ul> <li><code>KafkaProducer</code> is requested to configureInflightRequests</li> </ul>","text":""},{"location":"clients/producer/ProducerConfig/#partitionerclass","title":"partitioner.class <p>The class of the Partitioner for a KafkaProducer</p> <p>Default: DefaultPartitioner</p>","text":""},{"location":"clients/producer/ProducerConfig/#retries","title":"retries","text":""},{"location":"clients/producer/ProducerConfig/#retrybackoffms","title":"retry.backoff.ms <p>retry.backoff.ms</p>","text":""},{"location":"clients/producer/ProducerConfig/#transactionalid","title":"transactional.id <p>The ID of a KafkaProducer for transactional delivery</p> <p>Default: (undefined)</p> <p>This enables reliability semantics which span multiple producer sessions since it allows the client to guarantee that transactions using the same <code>transactional.id</code> have been completed prior to starting any new transactions.</p> <p>With no <code>transactional.id</code>, a producer is limited to idempotent delivery.</p> <p>When configured, enable.idempotence is implied (and configured when <code>KafkaProducer</code> is created).</p> <p>With <code>transactional.id</code>, <code>KafkaProducer</code> uses a modified client.id (that includes the ID).</p> <p>Note that, by default, transactions require a cluster of at least three brokers which is the recommended setting for production; for development you can change this, by adjusting broker setting transaction.state.log.replication.factor.</p> <p><code>transactional.id</code> is required for the transactional methods.</p> <p>Used when:</p> <ul> <li>KafkaProducer prints out log messages (with the transactional ID included in the log prefix)</li> <li><code>KafkaProducer</code> is created (and creates a TransactionManager)</li> </ul>","text":""},{"location":"clients/producer/ProducerConfig/#transactionstatelogreplicationfactor","title":"transaction.state.log.replication.factor","text":""},{"location":"clients/producer/ProducerConfig/#transactiontimeoutms","title":"transaction.timeout.ms","text":""},{"location":"clients/producer/ProducerConfig/#idempotenceenabled","title":"idempotenceEnabled <pre><code>boolean idempotenceEnabled()\n</code></pre> <p><code>idempotenceEnabled</code> is enabled (<code>true</code>) when one of the following holds:</p> <ol> <li>transactional.id is defined</li> <li>enable.idempotence is enabled</li> </ol> <p><code>idempotenceEnabled</code> throws a <code>ConfigException</code> when enable.idempotence is disabled but transactional.id is defined:</p> <pre><code>Cannot set a transactional.id without also enabling idempotence.\n</code></pre> <p><code>idempotenceEnabled</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is created (and requested to configureTransactionState, configureInflightRequests, configureAcks)</li> <li><code>ProducerConfig</code> is requested to maybeOverrideAcksAndRetries</li> </ul>","text":""},{"location":"clients/producer/ProducerConfig/#postprocessparsedconfig","title":"postProcessParsedConfig <pre><code>Map&lt;String, Object&gt; postProcessParsedConfig(\n  Map&lt;String, Object&gt; parsedValues)\n</code></pre> <p><code>postProcessParsedConfig</code> maybeOverrideEnableIdempotence. <code>postProcessParsedConfig</code> maybeOverrideClientId. <code>postProcessParsedConfig</code> maybeOverrideAcksAndRetries.</p> <p><code>postProcessParsedConfig</code>\u00a0is part of the AbstractConfig abstraction.</p>","text":""},{"location":"clients/producer/ProducerConfig/#maybeoverrideclientid","title":"maybeOverrideClientId <p><code>maybeOverrideAcksAndRetries</code> overrides client.id configuration property unless already defined.</p> <p>The new value uses transactional.id (if defined) or the next available ID with the <code>producer-</code> prefix.</p>","text":""},{"location":"clients/producer/ProducerConfig/#maybeoverrideacksandretries","title":"maybeOverrideAcksAndRetries <pre><code>void maybeOverrideAcksAndRetries(\n  Map&lt;String, Object&gt; configs)\n</code></pre> <p><code>maybeOverrideAcksAndRetries</code>...FIXME</p>","text":""},{"location":"clients/producer/ProducerConfig/#maybeoverrideenableidempotence","title":"maybeOverrideEnableIdempotence <pre><code>void maybeOverrideEnableIdempotence(\n  Map&lt;String, Object&gt; configs)\n</code></pre> <p><code>maybeOverrideEnableIdempotence</code> sets enable.idempotence configuration property to <code>true</code> when transactional.id is defined with no enable.idempotence.</p>","text":""},{"location":"clients/producer/ProducerInterceptors/","title":"ProducerInterceptors","text":"<p><code>ProducerInterceptors</code> is...FIXME</p>"},{"location":"clients/producer/RecordAccumulator/","title":"RecordAccumulator","text":"<p><code>RecordAccumulator</code> uses BufferPool.</p>"},{"location":"clients/producer/RecordAccumulator/#creating-instance","title":"Creating Instance","text":"<p><code>RecordAccumulator</code> takes the following to be created:</p> <ul> <li> LogContext <li> batch.size <li> CompressionType <li> linger.ms <li> retry.backoff.ms <li> configureDeliveryTimeout <li> <code>Metrics</code> <li> Name of the Metrics Group <li> <code>Time</code> <li> <code>ApiVersions</code> <li>TransactionManager</li> <li> BufferPool <p><code>RecordAccumulator</code> is created\u00a0along with KafkaProducer.</p>"},{"location":"clients/producer/RecordAccumulator/#free","title":"BufferPool","text":"<p><code>RecordAccumulator</code> is given a BufferPool when created.</p> <p>All the metrics of <code>RecordAccumulator</code> report performance of the <code>BufferPool</code>.</p> <p>The <code>BufferPool</code> is used for the following:</p> <ul> <li>Adding a record</li> <li>partitionReady</li> <li>deallocate</li> <li>bufferPoolAvailableMemory</li> </ul> <p>The <code>BufferPool</code> is closed while <code>RecordAccumulator</code> is requested to close.</p>"},{"location":"clients/producer/RecordAccumulator/#metrics","title":"Metrics","text":"<p><code>RecordAccumulator</code> registers the metrics under the producer-metrics group name.</p>"},{"location":"clients/producer/RecordAccumulator/#buffer-available-bytes","title":"buffer-available-bytes","text":"<p>The total amount of buffer memory that is not being used (either unallocated or in the free list)</p> <p>availableMemory of the BufferPool</p>"},{"location":"clients/producer/RecordAccumulator/#buffer-total-bytes","title":"buffer-total-bytes","text":"<p>The maximum amount of buffer memory the client can use (whether or not it is currently used)</p> <p>totalMemory of the BufferPool</p>"},{"location":"clients/producer/RecordAccumulator/#waiting-threads","title":"waiting-threads","text":"<p>The number of user threads blocked waiting for buffer memory to enqueue their records</p> <p>queued of the BufferPool</p>"},{"location":"clients/producer/RecordAccumulator/#transactionmanager","title":"TransactionManager <p><code>RecordAccumulator</code> is given a TransactionManager when created.</p> <p><code>RecordAccumulator</code> uses the <code>TransactionManager</code> when requested for the following:</p> <ul> <li>reenqueue</li> <li>splitAndReenqueue</li> <li>insertInSequenceOrder</li> <li>drain (drainBatchesForOneNode and shouldStopDrainBatchesForPartition)</li> <li>abortUndrainedBatches</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#appendsinprogress-counter","title":"appendsInProgress Counter <p><code>RecordAccumulator</code> creates an <code>AtomicInteger</code> (Java) for <code>appendsInProgress</code> internal counter when created.</p> <p><code>appendsInProgress</code> simply marks a single execution of append (and is incremented at the beginning and decremented right at the end).</p> <p><code>appendsInProgress</code> is used when flushInProgress.</p>","text":""},{"location":"clients/producer/RecordAccumulator/#flushinprogress","title":"flushInProgress","text":"<pre><code>boolean appendsInProgress()\n</code></pre> <p><code>appendsInProgress</code> indicates if the appendsInProgress counter is above <code>0</code>.</p> <p><code>appendsInProgress</code> is used when abortIncompleteBatches.</p>"},{"location":"clients/producer/RecordAccumulator/#flushesinprogress-counter","title":"flushesInProgress Counter <p><code>RecordAccumulator</code> creates an <code>AtomicInteger</code> (Java) for <code>flushesInProgress</code> internal counter when created.</p> <p><code>flushesInProgress</code> is incremented when beginFlush and decremented when awaitFlushCompletion.</p> <p><code>flushesInProgress</code> is used when flushInProgress.</p>","text":""},{"location":"clients/producer/RecordAccumulator/#flushinprogress_1","title":"flushInProgress <pre><code>boolean flushInProgress()\n</code></pre> <p><code>flushInProgress</code> indicates if the flushesInProgress counter is above <code>0</code>.</p> <p><code>flushInProgress</code>\u00a0is used when:</p> <ul> <li><code>RecordAccumulator</code> is requested to ready</li> <li><code>Sender</code> is requested to maybeSendAndPollTransactionalRequest</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#append","title":"Appending Record <pre><code>RecordAppendResult append(\n  String topic,\n  int partition,\n  long timestamp,\n  byte[] key,\n  byte[] value,\n  Header[] headers,\n  AppendCallbacks callbacks,\n  long maxTimeToBlock,\n  boolean abortOnNewBatch,\n  long nowMs,\n  Cluster cluster)\n</code></pre> <p><code>append</code> increments appendsInProgress.</p> <p><code>append</code> setPartition with the given <code>AppendCallbacks</code>.</p> <p><code>append</code> finds an in-progress batch (among the deque of ProducerBatchs).</p> <p><code>append</code> tryAppend.</p> <p><code>append</code>...FIXME (there is so much more magic going on yet it doesn't seem as important).</p>  <p><code>append</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to send a record (and doSend)</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#tryAppend","title":"tryAppend <pre><code>RecordAppendResult tryAppend(\n  long timestamp,\n  byte[] key,\n  byte[] value,\n  Header[] headers,\n  Callback callback,\n  Deque&lt;ProducerBatch&gt; deque,\n  long nowMs)\n</code></pre> <p><code>tryAppend</code>...FIXME</p>","text":""},{"location":"clients/producer/RecordAccumulator/#ready","title":"ready <pre><code>ReadyCheckResult ready(\n  Cluster cluster,\n  long nowMs)\n</code></pre> <p><code>ready</code> is a list of partitions with data ready to send.</p> <p><code>ready</code>...FIXME</p> <p><code>ready</code>\u00a0is used when:</p> <ul> <li><code>Sender</code> is requested to sendProducerData</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#beginflush","title":"beginFlush <pre><code>void beginFlush()\n</code></pre> <p><code>beginFlush</code> atomically increments the flushesInProgress counter.</p> <p><code>beginFlush</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to flush</li> <li><code>Sender</code> is requested to maybeSendAndPollTransactionalRequest</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#awaitflushcompletion","title":"awaitFlushCompletion <pre><code>void awaitFlushCompletion()\n</code></pre> <p><code>awaitFlushCompletion</code>...FIXME</p> <p><code>awaitFlushCompletion</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to flush</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#splitandreenqueue","title":"splitAndReenqueue <pre><code>int splitAndReenqueue(\n  ProducerBatch bigBatch)\n</code></pre> <p><code>splitAndReenqueue</code>...FIXME</p> <p><code>splitAndReenqueue</code>\u00a0is used when:</p> <ul> <li><code>Sender</code> is requested to completeBatch</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#deallocate","title":"deallocate <pre><code>void deallocate(\n  ProducerBatch batch)\n</code></pre> <p><code>deallocate</code>...FIXME</p> <p><code>deallocate</code>\u00a0is used when:</p> <ul> <li><code>RecordAccumulator</code> is requested to abortBatches and abortUndrainedBatches</li> <li><code>Sender</code> is requested to maybeRemoveAndDeallocateBatch</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#abortbatches","title":"abortBatches <pre><code>void abortBatches() // (1)\nvoid abortBatches(\n  RuntimeException reason)\n</code></pre> <ol> <li>Uses a <code>KafkaException</code></li> </ol> <p><code>abortBatches</code>...FIXME</p> <p><code>abortBatches</code>\u00a0is used when:</p> <ul> <li><code>RecordAccumulator</code> is requested to abortIncompleteBatches</li> <li><code>Sender</code> is requested to maybeAbortBatches</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#abortincompletebatches","title":"abortIncompleteBatches <pre><code>void abortIncompleteBatches()\n</code></pre> <p><code>abortIncompleteBatches</code> abortBatches as long as there are appendsInProgress. <code>abortIncompleteBatches</code> abortBatches one last time (after no thread was appending in case there was a new batch appended by the last appending thread).</p> <p>In the end, <code>abortIncompleteBatches</code> clears the batches registry.</p> <p><code>abortIncompleteBatches</code>\u00a0is used when:</p> <ul> <li><code>Sender</code> is requested to run (and forceClose)</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#abortundrainedbatches","title":"abortUndrainedBatches <pre><code>void abortUndrainedBatches(\n  RuntimeException reason)\n</code></pre> <p><code>abortUndrainedBatches</code>...FIXME</p> <p><code>abortUndrainedBatches</code>\u00a0is used when:</p> <ul> <li><code>Sender</code> is requested to maybeSendAndPollTransactionalRequest</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#incomplete-pending-batches","title":"Incomplete (Pending) Batches <p><code>RecordAccumulator</code> creates an <code>IncompleteBatches</code> for <code>incomplete</code> internal registry of pending batches when created.</p> <p><code>RecordAccumulator</code> uses the <code>IncompleteBatches</code> when:</p> <ul> <li>append (to add a new <code>ProducerBatch</code>)</li> <li>splitAndReenqueue (to add a new <code>ProducerBatch</code>)</li> <li>deallocate (to remove a <code>ProducerBatch</code>)</li> <li>awaitFlushCompletion, abortBatches and abortUndrainedBatches (to copy all <code>ProducerBatch</code>s)</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#hasincomplete","title":"hasIncomplete <pre><code>boolean hasIncomplete()\n</code></pre> <p><code>hasIncomplete</code> is <code>true</code> when the incomplete registry is not empty.</p> <p><code>hasIncomplete</code> is used when:</p> <ul> <li><code>Sender</code> is requested to maybeSendAndPollTransactionalRequest and maybeAbortBatches</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#in-progress-batches","title":"In-Progress Batches <pre><code>ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt; batches\n</code></pre> <p><code>RecordAccumulator</code> creates a <code>ConcurrentMap</code> (Java) for the <code>batches</code> internal registry of in-progress ProducerBatches (per <code>TopicPartition</code>).</p> <p><code>RecordAccumulator</code> adds a new <code>ArrayDeque</code> (Java) when getOrCreateDeque.</p> <p><code>batches</code>\u00a0is used when:</p> <ul> <li>expiredBatches</li> <li>ready</li> <li>hasUndrained</li> <li>getDeque</li> <li>batches</li> <li>abortIncompleteBatches</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#getorcreatedeque","title":"getOrCreateDeque <pre><code>Deque&lt;ProducerBatch&gt; getOrCreateDeque(\n  TopicPartition tp)\n</code></pre> <p><code>getOrCreateDeque</code>...FIXME</p> <p><code>getOrCreateDeque</code>\u00a0is used when:</p> <ul> <li><code>RecordAccumulator</code> is requested to append, reenqueue, splitAndReenqueue</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#reenqueue","title":"reenqueue <pre><code>void reenqueue(\n  ProducerBatch batch,\n  long now)\n</code></pre> <p><code>reenqueue</code>...FIXME</p> <p><code>reenqueue</code>\u00a0is used when:</p> <ul> <li><code>Sender</code> is requested to reenqueueBatch</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#insertinsequenceorder","title":"insertInSequenceOrder <pre><code>void insertInSequenceOrder(\n  Deque&lt;ProducerBatch&gt; deque,\n  ProducerBatch batch)\n</code></pre> <p><code>insertInSequenceOrder</code>...FIXME</p> <p><code>insertInSequenceOrder</code>\u00a0is used when:</p> <ul> <li><code>RecordAccumulator</code> is requested to reenqueue and splitAndReenqueue</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#drain","title":"drain <pre><code>Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; drain(\n  Cluster cluster,\n  Set&lt;Node&gt; nodes,\n  int maxSize,\n  long now)\n</code></pre> <p><code>drain</code>...FIXME</p> <p><code>drain</code>\u00a0is used when:</p> <ul> <li><code>Sender</code> is requested to sendProducerData</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#drainbatchesforonenode","title":"drainBatchesForOneNode <pre><code>List&lt;ProducerBatch&gt; drainBatchesForOneNode(\n  Cluster cluster,\n  Node node,\n  int maxSize,\n  long now)\n</code></pre> <p><code>drainBatchesForOneNode</code>...FIXME</p>","text":""},{"location":"clients/producer/RecordAccumulator/#shouldstopdrainbatchesforpartition","title":"shouldStopDrainBatchesForPartition <pre><code>boolean shouldStopDrainBatchesForPartition(\n  ProducerBatch first,\n  TopicPartition tp)\n</code></pre> <p><code>shouldStopDrainBatchesForPartition</code>...FIXME</p>","text":""},{"location":"clients/producer/RecordAccumulator/#registerMetrics","title":"registerMetrics <pre><code>void registerMetrics(\n  Metrics metrics,\n  String metricGrpName)\n</code></pre> <p><code>registerMetrics</code> registers (adds) the metrics to the given Metrics.</p>  <p><code>registerMetrics</code> is used when:</p> <ul> <li><code>RecordAccumulator</code> is created</li> </ul>","text":""},{"location":"clients/producer/RecordAccumulator/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.clients.producer.internals.RecordAccumulator</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.clients.producer.internals.RecordAccumulator=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"clients/producer/Sender/","title":"Sender","text":"<p><code>Sender</code> is a <code>Runnable</code> (Java) that is executed as a separate thread alongside KafkaProducer to send records to a Kafka cluster.</p>"},{"location":"clients/producer/Sender/#creating-instance","title":"Creating Instance","text":"<p><code>Sender</code> takes the following to be created:</p> <ul> <li> <code>LogContext</code> <li>KafkaClient</li> <li> <code>ProducerMetadata</code> <li> RecordAccumulator <li> <code>guaranteeMessageOrder</code> flag <li> maxRequestSize <li> acks <li> retries <li> <code>SenderMetricsRegistry</code> <li> <code>Time</code> <li> requestTimeoutMs <li> retryBackoffMs <li> TransactionManager <li> <code>ApiVersions</code> <p><code>Sender</code> is created\u00a0along with KafkaProducer.</p>"},{"location":"clients/producer/Sender/#kafkaclient","title":"KafkaClient <p><code>Sender</code> is given a KafkaClient when created.</p>","text":""},{"location":"clients/producer/Sender/#running-thread","title":"Running Thread <pre><code>void run()\n</code></pre> <p><code>run</code> prints out the following DEBUG message to the logs:</p> <pre><code>Starting Kafka producer I/O thread.\n</code></pre> <p><code>run</code> runs once and repeats until the running flag is turned off.</p> <p>Right after the running flag is off, <code>run</code> prints out the following DEBUG message to the logs:</p> <pre><code>Beginning shutdown of Kafka producer I/O thread, sending remaining records.\n</code></pre> <p><code>run</code>...FIXME</p> <p>In the end, <code>run</code>\u00a0prints out the following DEBUG message to the logs:</p> <pre><code>Shutdown of Kafka producer I/O thread has completed.\n</code></pre> <p><code>run</code>\u00a0is part of the <code>Runnable</code> (Java) abstraction.</p>","text":""},{"location":"clients/producer/Sender/#runonce","title":"runOnce <pre><code>void runOnce()\n</code></pre> <p>If executed with a TransactionManager, <code>runOnce</code>...FIXME</p> <p><code>runOnce</code> sendProducerData.</p> <p><code>runOnce</code> requests the KafkaClient to poll.</p>","text":""},{"location":"clients/producer/Sender/#sendproducerdata","title":"sendProducerData <pre><code>long sendProducerData(\n  long now)\n</code></pre> <p><code>sendProducerData</code> requests the ProducerMetadata for the current cluster info</p> <p><code>sendProducerData</code> requests the RecordAccumulator for the partitions with data ready to send.</p> <p><code>sendProducerData</code> requests a metadata update when there are partitions with no leaders.</p> <p><code>sendProducerData</code> removes nodes not ready to send to.</p> <p><code>sendProducerData</code> requests the RecordAccumulator to drain (and create ProducerBatchs).</p> <p><code>sendProducerData</code> registers the batches (in the inFlightBatches registry).</p> <p>With guaranteeMessageOrder, <code>sendProducerData</code> mutes all the partitions drained.</p> <p><code>sendProducerData</code> requests the RecordAccumulator to resetNextBatchExpiryTime.</p> <p><code>sendProducerData</code> requests the RecordAccumulator for the expired batches and adds all expired InflightBatches.</p> <p>If there are any expired batches, <code>sendProducerData</code>...FIXME</p> <p><code>sendProducerData</code> requests the <code>SenderMetrics</code> to <code>updateProduceRequestMetrics</code>.</p> <p>With at least one broker to send batches to, <code>sendProducerData</code> prints out the following TRACE message to the logs:</p> <pre><code>Nodes with data ready to send: [readyNodes]\n</code></pre> <p><code>sendProducerData</code> sendProduceRequests.</p>","text":""},{"location":"clients/producer/Sender/#sendproducerequests","title":"sendProduceRequests <pre><code>void sendProduceRequests(\n  Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; collated,\n  long now)\n</code></pre> <p>For every pair of a broker node and an associated ProducerBatch (in the given <code>collated</code> collection), <code>sendProduceRequests</code> sendProduceRequest with the broker node, the acks, the requestTimeoutMs and the <code>ProducerBatch</code>.</p>","text":""},{"location":"clients/producer/Sender/#sendproducerequest","title":"sendProduceRequest <pre><code>void sendProduceRequest(\n  long now,\n  int destination,\n  short acks,\n  int timeout,\n  List&lt;ProducerBatch&gt; batches)\n</code></pre> <p><code>sendProduceRequest</code> creates a collection of ProducerBatches by <code>TopicPartition</code> from the given <code>batches</code>.</p> <p><code>sendProduceRequest</code> requests the KafkaClient for a new ClientRequest (for the <code>destination</code> broker) and to send it.</p> <p><code>sendProduceRequest</code> registers a [handleProduceResponse] callback to invoke when a response arrives. <code>sendProduceRequest</code> expects a response for all the <code>acks</code> but <code>0</code>.</p> <p>In the end, <code>sendProduceRequest</code> prints out the following TRACE message to the logs:</p> <pre><code>Sent produce request to [nodeId]: [requestBuilder]\n</code></pre>","text":""},{"location":"clients/producer/Sender/#handleproduceresponse","title":"handleProduceResponse <pre><code>void handleProduceResponse(\n  ClientResponse response,\n  Map&lt;TopicPartition, ProducerBatch&gt; batches,\n  long now)\n</code></pre>","text":""},{"location":"clients/producer/Sender/#maybesendandpolltransactionalrequest","title":"maybeSendAndPollTransactionalRequest <pre><code>boolean maybeSendAndPollTransactionalRequest()\n</code></pre>","text":""},{"location":"clients/producer/Sender/#running-flag","title":"running Flag <p><code>Sender</code> runs as long as the <code>running</code> internal flag is on.</p> <p>The <code>running</code> flag is on from when <code>Sender</code> is created until requested to initiateClose.</p>","text":""},{"location":"clients/producer/Sender/#initiateclose","title":"initiateClose <pre><code>void initiateClose()\n</code></pre> <p><code>initiateClose</code> requests the RecordAccumulator to close and turns the running flag off.</p> <p>In the end, <code>initiateClose</code> wakes up the KafkaClient.</p> <p><code>initiateClose</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to close</li> <li><code>Sender</code> is requested to forceClose</li> </ul>","text":""},{"location":"clients/producer/Sender/#waking-up","title":"Waking Up <pre><code>void wakeup()\n</code></pre> <p><code>wakeup</code> requests the KafkaClient to wakeup.</p> <p><code>wakeup</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to initTransactions, sendOffsetsToTransaction, commitTransaction, abortTransaction, doSend, waitOnMetadata, flush</li> <li><code>Sender</code> is requested to initiateClose</li> </ul>","text":""},{"location":"clients/producer/Sender/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.clients.producer.internals.Sender</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.clients.producer.internals.Sender=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"clients/producer/TransactionManager/","title":"TransactionManager","text":""},{"location":"clients/producer/TransactionManager/#creating-instance","title":"Creating Instance","text":"<p><code>TransactionManager</code> takes the following to be created:</p> <ul> <li> <code>LogContext</code> <li> transactional.id <li> transaction.timeout.ms <li> retry.backoff.ms <li> <code>ApiVersions</code> <p><code>TransactionManager</code> is created\u00a0along with KafkaProducer (with idempotenceEnabled).</p>"},{"location":"clients/producer/TransactionManager/#states","title":"States <p><code>TransactionManager</code> can be in one of the following states:</p> <ul> <li>UNINITIALIZED</li> <li>INITIALIZING</li> <li>READY</li> <li>IN_TRANSACTION</li> <li>COMMITTING_TRANSACTION</li> <li>ABORTING_TRANSACTION</li> <li>ABORTABLE_ERROR</li> <li>FATAL_ERROR</li> </ul>","text":""},{"location":"clients/producer/TransactionManager/#valid-transitions","title":"Valid Transitions    Source (Current) State Target States transitionTo     ABORTABLE_ERROR <ul><li>ABORTING_TRANSACTION</li><li>ABORTABLE_ERROR</li></ul>    ABORTING_TRANSACTION <ul><li>INITIALIZING</li><li>READY</li></ul> beginAbort   COMMITTING_TRANSACTION <ul><li>READY</li><li>ABORTABLE_ERROR</li></ul> beginCommit   IN_TRANSACTION <ul><li>COMMITTING_TRANSACTION</li><li>ABORTING_TRANSACTION</li><li>ABORTABLE_ERROR</li></ul> beginTransaction   INITIALIZING READY <ul><li>initializeTransactions</li><li> bumpIdempotentEpochAndResetIdIfNeeded</li><li>completeTransaction</li></ul>   READY <ul><li>UNINITIALIZED</li><li>IN_TRANSACTION</li></ul> <ul><li>completeTransaction</li><li><code>InitProducerIdHandler</code></li></ul>   UNINITIALIZED INITIALIZING resetIdempotentProducerId   any state FATAL_ERROR","text":""},{"location":"clients/producer/TransactionManager/#beginabort","title":"beginAbort <pre><code>TransactionalRequestResult beginAbort()\n</code></pre> <p><code>beginAbort</code>...FIXME</p> <p><code>beginAbort</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to abortTransaction</li> <li><code>Sender</code> is requested to run (and is shuting down)</li> </ul>","text":""},{"location":"clients/producer/TransactionManager/#begincommit","title":"beginCommit <pre><code>TransactionalRequestResult beginCommit()\n</code></pre> <p><code>beginCommit</code>...FIXME</p> <p><code>beginCommit</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to commitTransaction</li> </ul>","text":""},{"location":"clients/producer/TransactionManager/#begincompletingtransaction","title":"beginCompletingTransaction <pre><code>TransactionalRequestResult beginCompletingTransaction(\n  TransactionResult transactionResult)\n</code></pre> <p><code>beginCompletingTransaction</code>...FIXME</p> <p><code>beginCompletingTransaction</code>\u00a0is used when:</p> <ul> <li><code>TransactionManager</code> is requested to beginCommit and beginAbort</li> </ul>","text":""},{"location":"clients/producer/TransactionManager/#begintransaction","title":"beginTransaction <pre><code>void beginTransaction()\n</code></pre> <p><code>beginTransaction</code> makes sure that the producer is transactional and transition to <code>IN_TRANSACTION</code> state.</p> <p><code>beginTransaction</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to beginTransaction</li> </ul>","text":""},{"location":"clients/producer/TransactionManager/#initializetransactions","title":"initializeTransactions <pre><code>TransactionalRequestResult initializeTransactions() // (1)\nTransactionalRequestResult initializeTransactions(\n  ProducerIdAndEpoch producerIdAndEpoch)\n</code></pre> <ol> <li>Uses <code>ProducerIdAndEpoch.NONE</code></li> </ol> <p><code>initializeTransactions</code>...FIXME</p> <p><code>initializeTransactions</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to initTransactions</li> <li><code>TransactionManager</code> is requested to beginCompletingTransaction</li> </ul>","text":""},{"location":"clients/producer/TransactionManager/#istransactional","title":"isTransactional <pre><code>boolean isTransactional()\n</code></pre> <p><code>isTransactional</code> is enabled (<code>true</code>) when the transactional.id configuration property is defined (for the producer and the transactionalId was given when created).</p>","text":""},{"location":"clients/producer/TransactionManager/#maybeaddpartitiontotransaction","title":"maybeAddPartitionToTransaction <pre><code>void maybeAddPartitionToTransaction(\n  TopicPartition topicPartition)\n</code></pre> <p><code>maybeAddPartitionToTransaction</code>...FIXME</p> <p><code>maybeAddPartitionToTransaction</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to doSend</li> </ul>","text":""},{"location":"clients/producer/TransactionManager/#sendoffsetstotransaction","title":"sendOffsetsToTransaction <pre><code>TransactionalRequestResult sendOffsetsToTransaction(\n  Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,\n  ConsumerGroupMetadata groupMetadata)\n</code></pre> <p><code>sendOffsetsToTransaction</code>...FIXME</p> <p><code>sendOffsetsToTransaction</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to sendOffsetsToTransaction</li> </ul>","text":""},{"location":"consumer-groups/","title":"Consumer Groups","text":"<p>GroupCoordinator is the main actor here and is started alongside KafkaServer.</p>"},{"location":"consumer-groups/#__consumer_offsets","title":"__consumer_offsets <p><code>__consumer_offsets</code> is the offset commit topic (with offsets.topic.num.partitions).</p>","text":""},{"location":"consumer-groups/GroupCoordinator/","title":"GroupCoordinator","text":"<p><code>GroupCoordinator</code> is elected as the group coordinator for every partition to handle consumer groups that are \"assigned\" to this partition. There are going to be as many <code>GroupCoordinator</code>s as there are offsets.topic.num.partitions.</p>"},{"location":"consumer-groups/GroupCoordinator/#creating-instance","title":"Creating Instance","text":"<p><code>GroupCoordinator</code> takes the following to be created:</p> <ul> <li> broker.id <li>GroupConfig</li> <li> OffsetConfig <li>GroupMetadataManager</li> <li> <code>DelayedOperationPurgatory[DelayedHeartbeat]</code> <li> <code>DelayedOperationPurgatory[DelayedRebalance]</code> <li> <code>Time</code> <li> Metrics <p><code>GroupCoordinator</code> is created using apply factory.</p>"},{"location":"consumer-groups/GroupCoordinator/#groupmetadatamanager","title":"GroupMetadataManager <p><code>GroupCoordinator</code> is given a GroupMetadataManager when created.</p>","text":""},{"location":"consumer-groups/GroupCoordinator/#groupconfig","title":"GroupConfig <p><code>GroupCoordinator</code> is given a <code>GroupConfig</code> when created.</p> <p><code>GroupConfig</code> is a collection of the configuration properties:</p> <ul> <li>groupMinSessionTimeoutMs</li> <li>groupMaxSessionTimeoutMs</li> <li>groupMaxSize</li> <li>groupInitialRebalanceDelay</li> </ul>","text":""},{"location":"consumer-groups/GroupCoordinator/#creating-groupcoordinator-instance","title":"Creating GroupCoordinator Instance <pre><code>apply(\n  config: KafkaConfig,\n  replicaManager: ReplicaManager,\n  time: Time,\n  metrics: Metrics): GroupCoordinator // (1)!\napply(\n  config: KafkaConfig,\n  replicaManager: ReplicaManager,\n  heartbeatPurgatory: DelayedOperationPurgatory[DelayedHeartbeat],\n  rebalancePurgatory: DelayedOperationPurgatory[DelayedRebalance],\n  time: Time,\n  metrics: Metrics): GroupCoordinator\n</code></pre> <ol> <li>Creates <code>DelayedOperationPurgatory</code>s and calls the other <code>apply</code></li> </ol>  <p>Note</p> <p>All <code>GroupCoordinator</code> really needs for work is ReplicaManager.</p>  <p><code>apply</code> creates an OffsetConfig (based on the given KafkaConfig).</p> <p><code>apply</code> creates a GroupConfig.</p> <p><code>apply</code> creates a GroupMetadataManager based on the following configuration properties (in the KafkaConfig):</p> <ul> <li>broker.id</li> <li>interBrokerProtocolVersion</li> </ul> <p>In the end, <code>apply</code> creates a GroupCoordinator.</p>  <p><code>apply</code> is used when:</p> <ul> <li><code>BrokerServer</code> is requested to start up</li> <li><code>KafkaServer</code> is requested to start up</li> </ul>","text":""},{"location":"consumer-groups/GroupCoordinator/#creating-offsetconfig","title":"Creating OffsetConfig <pre><code>offsetConfig(\n  config: KafkaConfig): OffsetConfig\n</code></pre> <p><code>offsetConfig</code> uses the KafkaConfig for the following configuration properties to create an OffsetConfig:</p> <ul> <li>offsetMetadataMaxSize</li> <li>offsetsLoadBufferSize</li> <li>offsetsRetentionMinutes</li> <li>offsetsRetentionCheckIntervalMs</li> <li>offsets.topic.num.partitions</li> <li>offsetsTopicSegmentBytes</li> <li>offsetsTopicReplicationFactor</li> <li>offsetsTopicCompressionCodec</li> <li>offsetCommitTimeoutMs</li> <li>offsetCommitRequiredAcks</li> </ul>","text":""},{"location":"consumer-groups/GroupCoordinator/#onelection","title":"onElection <pre><code>onElection(\n  offsetTopicPartitionId: Int,\n  coordinatorEpoch: Int): Unit\n</code></pre> <p><code>onElection</code> prints out the following INFO message to the logs:</p> <pre><code>Elected as the group coordinator for partition [offsetTopicPartitionId] in epoch [coordinatorEpoch]\n</code></pre> <p><code>onElection</code> requests the GroupMetadataManager to scheduleLoadGroupAndOffsets.</p>  <p><code>onElection</code> is used when:</p> <ul> <li><code>RequestHandlerHelper</code> is requested to <code>onLeadershipChange</code> for __consumer_offsets</li> <li><code>BrokerMetadataPublisher</code> is requested to <code>publish</code> metadata for __consumer_offsets</li> </ul>","text":""},{"location":"consumer-groups/GroupCoordinator/#starting-up","title":"Starting Up <pre><code>startup(\n  retrieveGroupMetadataTopicPartitionCount: () =&gt; Int,\n  enableMetadataExpiration: Boolean = true): Unit\n</code></pre> <p><code>startup</code> prints out the following INFO message to the logs:</p> <pre><code>Starting up.\n</code></pre> <p><code>startup</code> requests the GroupMetadataManager to start up.</p> <p>In the end, <code>startup</code> prints out the following INFO message to the logs:</p> <pre><code>Startup complete.\n</code></pre>  <p><code>startup</code> is used when:</p> <ul> <li><code>KafkaServer</code> is requested to start up</li> <li><code>BrokerMetadataPublisher</code> is requested to <code>initializeManagers</code></li> </ul>","text":""},{"location":"consumer-groups/GroupCoordinator/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>kafka.coordinator.group.GroupCoordinator</code> logger to see what happens inside.</p> <p>Add the following line to <code>config/log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.coordinator.group.GroupCoordinator=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"consumer-groups/GroupCoordinator/#logident","title":"logIdent <p><code>GroupCoordinator</code> uses the following logging prefix (with the broker.id):</p> <pre><code>[GroupCoordinator [brokerId]]\n</code></pre>","text":""},{"location":"consumer-groups/GroupMetadataManager/","title":"GroupMetadataManager","text":""},{"location":"consumer-groups/GroupMetadataManager/#creating-instance","title":"Creating Instance","text":"<p><code>GroupMetadataManager</code> takes the following to be created:</p> <ul> <li> broker.id <li> ApiVersion <li> OffsetConfig <li> ReplicaManager <li> <code>Time</code> <li> Metrics <p><code>GroupMetadataManager</code> is created alongside a GroupCoordinator.</p>"},{"location":"consumer-groups/GroupMetadataManager/#performance-metrics","title":"Performance Metrics","text":"<p><code>GroupMetadataManager</code> is a KafkaMetricsGroup and registers the following performance metrics in kafka.coordinator.group:type=GroupMetadataManager group.</p> Metric Name Description NumGroups NumGroupsCompletingRebalance NumGroupsDead NumGroupsEmpty NumGroupsPreparingRebalance NumGroupsStable NumOffsets <p></p>"},{"location":"consumer-groups/GroupMetadataManager/#logging","title":"Logging","text":"<p>Enable <code>ALL</code> logging level for <code>kafka.coordinator.group.GroupMetadataManager</code> logger to see what happens inside.</p> <p>Add the following line to <code>config/log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.coordinator.group.GroupMetadataManager=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"consumer-groups/OffsetConfig/","title":"OffsetConfig","text":"<p><code>OffsetConfig</code> is...FIXME</p>"},{"location":"controller/","title":"Controller Broker","text":"<p>Controller Broker (KafkaController) is a Kafka service that runs on every broker in a Kafka cluster, but only one can be active (elected) at any point in time.</p> <p>The process of promoting a broker to be the active controller is called Kafka Controller Election.</p> <p>Kafka Controller Internals</p> <p>In a Kafka cluster, one of the brokers serves as the controller, which is responsible for managing the states of partitions and replicas and for performing administrative tasks like reassigning partitions.</p> <p>Kafka Controller registers handlers to be notified about changes in Zookeeper and propagate them across brokers in a Kafka cluster.</p>"},{"location":"controller/#lifecycle","title":"Lifecycle","text":"<p>When started, <code>KafkaController</code> emits Startup controller event. That starts controller election (on the controller-event-thread).</p> <p>During controller election, one <code>KafkaController</code> becomes active (elected) and onControllerFailover. The ControllerContext is built on what is available in Zookeeper.</p> <p>While in initializeControllerContext, <code>KafkaController</code> updateLeaderAndIsrCache (and reads partition state from <code>/brokers/topics/[topic]/partitions/[partition]/state</code> paths in Zookeeper that is then stored as partitionLeadershipInfo of the ControllerContext).</p>"},{"location":"controller/AbstractControlRequest/","title":"AbstractControlRequest","text":"<p><code>AbstractControlRequest</code> is an extension of the AbstractRequest abstraction for controller requests that KafkaController uses to propage broker and partition state changes to brokers.</p>"},{"location":"controller/AbstractControlRequest/#contract","title":"Contract","text":""},{"location":"controller/AbstractControlRequest/#brokerepoch","title":"brokerEpoch <pre><code>long brokerEpoch()\n</code></pre> <p>Used when:</p> <ul> <li><code>KafkaApis</code> is requested to handleLeaderAndIsrRequest, handleStopReplicaRequest, handleUpdateMetadataRequest</li> </ul>","text":""},{"location":"controller/AbstractControlRequest/#controllerepoch","title":"controllerEpoch <pre><code>int controllerEpoch()\n</code></pre> <p>Used when:</p> <ul> <li><code>KafkaApis</code> is requested to handleStopReplicaRequest</li> <li><code>ReplicaManager</code> is requested to maybeUpdateMetadataCache, becomeLeaderOrFollower</li> <li><code>ZkMetadataCache</code> is requested to <code>updateMetadata</code></li> </ul>","text":""},{"location":"controller/AbstractControlRequest/#controllerid","title":"controllerId <pre><code>int controllerId()\n</code></pre> <p>Used when:</p> <ul> <li><code>KafkaApis</code> is requested to handleStopReplicaRequest</li> <li><code>ReplicaManager</code> is requested to maybeUpdateMetadataCache, becomeLeaderOrFollower</li> <li><code>ZkMetadataCache</code> is requested to <code>updateMetadata</code></li> </ul>","text":""},{"location":"controller/AbstractControlRequest/#implementations","title":"Implementations","text":"<ul> <li>LeaderAndIsrRequest</li> <li><code>StopReplicaRequest</code></li> <li><code>UpdateMetadataRequest</code></li> </ul>"},{"location":"controller/AbstractControlRequest/#creating-instance","title":"Creating Instance","text":"<p><code>AbstractControlRequest</code> takes the following to be created:</p> <ul> <li> <code>ApiKeys</code> <li> Version <p>Abstract Class</p> <p><code>AbstractControlRequest</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete AbstractControlRequests.</p>"},{"location":"controller/AbstractControllerBrokerRequestBatch/","title":"AbstractControllerBrokerRequestBatch","text":"<p><code>AbstractControllerBrokerRequestBatch</code> is an abstraction of ControllerBrokerRequestBatches that can send controller requests to brokers.</p>"},{"location":"controller/AbstractControllerBrokerRequestBatch/#contract","title":"Contract","text":""},{"location":"controller/AbstractControllerBrokerRequestBatch/#sending-controllerevent","title":"Sending ControllerEvent <pre><code>sendEvent(\n  event: ControllerEvent): Unit\n</code></pre> <p>Sends the ControllerEvent</p> <p>Used when:</p> <ul> <li><code>AbstractControllerBrokerRequestBatch</code> is requested to sendLeaderAndIsrRequest, sendUpdateMetadataRequests, sendStopReplicaRequests</li> </ul>","text":""},{"location":"controller/AbstractControllerBrokerRequestBatch/#sending-request-to-broker","title":"Sending Request (to Broker) <pre><code>sendRequest(\n  brokerId: Int,\n  request: AbstractControlRequest.Builder[_ &lt;: AbstractControlRequest],\n  callback: AbstractResponse =&gt; Unit = null): Unit\n</code></pre> <p>Sends out an AbstractControlRequest to the given broker (with optional callback to handle a response)</p> <p>Used when:</p> <ul> <li><code>AbstractControllerBrokerRequestBatch</code> is requested to sendLeaderAndIsrRequest, sendUpdateMetadataRequests, sendStopReplicaRequests</li> </ul>","text":""},{"location":"controller/AbstractControllerBrokerRequestBatch/#implementations","title":"Implementations","text":"<ul> <li>ControllerBrokerRequestBatch</li> </ul>"},{"location":"controller/AbstractControllerBrokerRequestBatch/#creating-instance","title":"Creating Instance","text":"<p><code>AbstractControllerBrokerRequestBatch</code> takes the following to be created:</p> <ul> <li> KafkaConfig <li> ControllerContext <li> <code>StateChangeLogger</code> <p>Abstract Class</p> <p><code>AbstractControllerBrokerRequestBatch</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete AbstractControllerBrokerRequestBatches.</p>"},{"location":"controller/AbstractControllerBrokerRequestBatch/#preparing-new-batch","title":"Preparing New Batch <pre><code>newBatch(): Unit\n</code></pre> <p><code>newBatch</code> throws an <code>IllegalStateException</code> when any of the internal registries are not empty: </p> <ul> <li> <p>leaderAndIsrRequestMap</p> <pre><code>Controller to broker state change requests batch is not empty while creating a new one.\nSome LeaderAndIsr state changes [leaderAndIsrRequestMap] might be lost\n</code></pre> </li> <li> <p>stopReplicaRequestMap</p> <pre><code>Controller to broker state change requests batch is not empty while creating a new one.\nSome StopReplica state changes [stopReplicaRequestMap] might be lost\n</code></pre> </li> <li> <p>updateMetadataRequestBrokerSet</p> <pre><code>Controller to broker state change requests batch is not empty while creating a new one.\nSome UpdateMetadata state changes to brokers [updateMetadataRequestBrokerSet] with partition info [updateMetadataRequestPartitionInfoMap] might be lost\n</code></pre> </li> </ul>  <p><code>newBatch</code> is used when:</p> <ul> <li><code>KafkaController</code> is requested to updateLeaderEpochAndSendRequest, sendUpdateMetadataRequest, doControlledShutdown</li> <li><code>ZkPartitionStateMachine</code> is requested to handleStateChanges</li> <li><code>ZkReplicaStateMachine</code> is requested to handleStateChanges</li> </ul>","text":""},{"location":"controller/AbstractControllerBrokerRequestBatch/#sending-controller-requests-to-brokers","title":"Sending Controller Requests To Brokers <pre><code>sendRequestsToBrokers(\n  controllerEpoch: Int): Unit\n</code></pre> <p><code>sendRequestsToBrokers</code> sends LeaderAndIsr requests out to brokers.</p> <p><code>sendRequestsToBrokers</code> sends UpdateMetadata requests out to brokers.</p> <p><code>sendRequestsToBrokers</code> sends StopReplica requests out to brokers.</p>  <p><code>sendRequestsToBrokers</code> is used when:</p> <ul> <li><code>KafkaController</code> is requested to updateLeaderEpochAndSendRequest, sendUpdateMetadataRequest, doControlledShutdown</li> <li><code>ZkPartitionStateMachine</code> is requested to handleStateChanges</li> <li><code>ZkReplicaStateMachine</code> is requested to handleStateChanges</li> </ul>","text":""},{"location":"controller/AbstractControllerBrokerRequestBatch/#sending-out-updatemetadata-requests","title":"Sending Out UpdateMetadata Requests <pre><code>sendUpdateMetadataRequests(\n  controllerEpoch: Int,\n  stateChangeLog: StateChangeLogger): Unit\n</code></pre> <p><code>sendUpdateMetadataRequests</code> prints out the following INFO message to the logs:</p> <pre><code>Sending UpdateMetadata request to brokers [updateMetadataRequestBrokerSet] for [size] partitions\n</code></pre> <p><code>sendUpdateMetadataRequests</code>...FIXME</p>","text":""},{"location":"controller/AbstractControllerBrokerRequestBatch/#sending-out-leaderandisr-requests","title":"Sending Out LeaderAndIsr Requests <pre><code>sendLeaderAndIsrRequest(\n  controllerEpoch: Int,\n  stateChangeLog: StateChangeLogger): Unit\n</code></pre> <p><code>sendLeaderAndIsrRequest</code> uses the leaderAndIsrRequestMap internal registry for the brokers and partitions to send LeaderAndIsr requests to.</p>  <p>For every broker (that is liveOrShuttingDownBrokerIds) and partitions (with their state), <code>sendLeaderAndIsrRequest</code> determines the type of request:</p> <ul> <li><code>become-leader</code> when the broker is assumed the leader</li> <li><code>become-follower</code> otherwise</li> </ul> <p><code>sendLeaderAndIsrRequest</code> prints out the following TRACE message to the logs:</p> <pre><code>Sending [typeOfRequest] LeaderAndIsr request [state] to broker [broker] for partition [partition]\n</code></pre> <p><code>sendLeaderAndIsrRequest</code> prints out the following INFO message to the logs:</p> <pre><code>Sending LeaderAndIsr request to broker [broker]\nwith [numBecomeLeaders] become-leader and [becomeFollower] become-follower partitions\n</code></pre> <p><code>sendLeaderAndIsrRequest</code> creates a LeaderAndIsrRequest.Builder for sending a LeaderAndIsr request to the brokers (one by one).</p> <p>In the end, <code>sendLeaderAndIsrRequest</code> clears out the leaderAndIsrRequestMap.</p>","text":""},{"location":"controller/AbstractControllerBrokerRequestBatch/#addleaderandisrrequestforbrokers","title":"addLeaderAndIsrRequestForBrokers <pre><code>addLeaderAndIsrRequestForBrokers(\n  brokerIds: Seq[Int],\n  topicPartition: TopicPartition,\n  leaderIsrAndControllerEpoch: LeaderIsrAndControllerEpoch,\n  replicaAssignment: ReplicaAssignment,\n  isNew: Boolean): Unit\n</code></pre> <p><code>addLeaderAndIsrRequestForBrokers</code>...FIXME</p>  <p><code>addLeaderAndIsrRequestForBrokers</code> is used when:</p> <ul> <li><code>KafkaController</code> is requested to updateLeaderEpochAndSendRequest</li> <li><code>ZkPartitionStateMachine</code> is requested to initializeLeaderAndIsrForPartitions and doElectLeaderForPartitions</li> <li><code>ZkReplicaStateMachine</code> is requested to doHandleStateChanges</li> </ul>","text":""},{"location":"controller/AutoPreferredReplicaLeaderElection/","title":"AutoPreferredReplicaLeaderElection","text":""},{"location":"controller/AutoPreferredReplicaLeaderElection/#state","title":"State <p><code>AutoPreferredReplicaLeaderElection</code> is a ControllerEvent that transition the KafkaController to <code>AutoLeaderBalance</code> state.</p> <p><code>AutoPreferredReplicaLeaderElection</code> is enqueued (to the ControllerEventManager) exclusively from the &lt;&gt;.","text":""},{"location":"controller/AutoPreferredReplicaLeaderElection/#process","title":"Process <p>When processed on a controller broker, <code>AutoPreferredReplicaLeaderElection</code> event checkAndTriggerAutoLeaderRebalance and in the end scheduleAutoLeaderRebalanceTask with the delay based on leader.imbalance.check.interval.seconds configuration property.</p>  <p>Note</p> <p><code>AutoPreferredReplicaLeaderElection</code> event is ignored (skipped) when processed on any broker but controller broker.</p>","text":""},{"location":"controller/ControllerBrokerRequestBatch/","title":"ControllerBrokerRequestBatch","text":"<p><code>ControllerBrokerRequestBatch</code> is an AbstractControllerBrokerRequestBatch.</p> <p>Every time <code>ControllerBrokerRequestBatch</code> is used it is first requested to prepare a new batch (of controller requests) followed by sending them out to brokers.</p>"},{"location":"controller/ControllerBrokerRequestBatch/#creating-instance","title":"Creating Instance","text":"<p><code>ControllerBrokerRequestBatch</code> takes the following to be created:</p> <ul> <li> KafkaConfig <li> <code>ControllerChannelManager</code> <li> ControllerEventManager <li> ControllerContext <li> <code>StateChangeLogger</code> <p><code>ControllerBrokerRequestBatch</code> is created along with a KafkaController, separately for the following:</p> <ul> <li>ControllerBrokerRequestBatch</li> <li>ZkReplicaStateMachine</li> <li>ZkPartitionStateMachine</li> </ul>"},{"location":"controller/ControllerBrokerRequestBatch/#sending-controllerevent","title":"Sending ControllerEvent <pre><code>sendEvent(\n  event: ControllerEvent): Unit\n</code></pre> <p><code>sendEvent</code> is part of the AbstractControllerBrokerRequestBatch abstraction.</p>  <p><code>sendEvent</code> requests the ControllerEventManager to enqueue the input ControllerEvent.</p>","text":""},{"location":"controller/ControllerBrokerRequestBatch/#sending-request-to-broker","title":"Sending Request (to Broker) <pre><code>sendRequest(\n  brokerId: Int,\n  request: AbstractControlRequest.Builder[_ &lt;: AbstractControlRequest],\n  callback: AbstractResponse =&gt; Unit = null): Unit\n</code></pre> <p><code>sendRequest</code> is part of the AbstractControllerBrokerRequestBatch abstraction.</p>  <p><code>sendRequest</code> requests the given ControllerChannelManager to send an controller request out to a broker.</p>","text":""},{"location":"controller/ControllerChannelManager/","title":"ControllerChannelManager","text":""},{"location":"controller/ControllerChannelManager/#review-me","title":"Review Me","text":"<p><code>ControllerChannelManager</code> manages &lt;&gt; for link:kafka-controller-KafkaController.adoc#controllerChannelManager[KafkaController]. <p><code>ControllerChannelManager</code> is &lt;&gt; when &lt;&gt; is created (to &lt;&gt; or &lt;&gt; brokers as they \"announce\" themselves in Zookeeper) for &lt;&gt; of the &lt;&gt; itself, the &lt;&gt; and &lt;&gt; (so they can send broker and partition changes out to all brokers in a Kafka cluster). <p>When &lt;&gt;, <code>ControllerChannelManager</code> establishes connection to every broker and starts a corresponding &lt;&gt; to keep sending queued controller requests. <p>[[logIdent]] <code>ControllerChannelManager</code> uses [Channel manager on controller [brokerId]] as the logging prefix (aka <code>logIdent</code>).</p> <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging levels for <code>kafka.controller.ControllerChannelManager</code> logger to see what happens inside.</p> <p>Add the following line to <code>config/log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.controller.ControllerChannelManager=ALL\n</code></pre>"},{"location":"controller/ControllerChannelManager/#refer-to","title":"Refer to &lt;&gt;. <p>=== [[brokerStateInfo]] Connection Metadata of All Brokers -- <code>brokerStateInfo</code> Internal Registry</p>","text":""},{"location":"controller/ControllerChannelManager/#source-scala","title":"[source, scala]","text":""},{"location":"controller/ControllerChannelManager/#brokerstateinfo-hashmapint-controllerbrokerstateinfo","title":"brokerStateInfo: HashMap[Int, ControllerBrokerStateInfo]","text":"<p><code>brokerStateInfo</code> is &lt;&gt; by broker ID. <p>Request threads for brokers are all &lt;&gt; when <code>ControllerChannelManager</code> is requested to &lt;&gt;. <p>A new broker is added when <code>ControllerChannelManager</code> is requested to &lt;&gt; (when &lt;&gt;). <p>A broker is removed when <code>ControllerChannelManager</code> is requested to &lt;&gt; (when &lt;&gt; or &lt;&gt;). <p>Use the &lt;&gt; gauge metric for the queue depth (i.e. how many link:kafka-controller-AbstractControlRequest.adoc[controller requests] are waiting to be sent out to all brokers). <p>=== [[KafkaMetricsGroup]][[metrics]] Performance Metrics</p> <p><code>ControllerChannelManager</code> is a &lt;&gt; with the following performance metrics. <p>.ControllerChannelManager's Performance Metrics [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Metric Name | Description</p> <p>| QueueSize a| [[QueueSize]] Controller requests (&lt;&gt;) queue size (per broker) <p>| RequestRateAndQueueTimeMs a| [[RequestRateAndQueueTimeMs]][[requestRateAndQueueTimeMetrics]] For every broker</p> <p>| TotalQueueSize a| [[TotalQueueSize]] Total number of controller requests (&lt;&gt;) to be sent out to brokers <p>|===</p> <p>The performance metrics are registered in kafka.controller:type=ControllerChannelManager group.</p> <p>.ControllerChannelManager in jconsole image::images/ControllerChannelManager-jconsole.png[align=\"center\"]</p> <p>=== [[creating-instance]] Creating ControllerChannelManager Instance</p> <p><code>ControllerChannelManager</code> takes the following to be created:</p> <ul> <li>[[controllerContext]] &lt;&gt; <li>[[config]] &lt;&gt; <li>[[time]] <code>Time</code></li> <li>[[metrics]] &lt;&gt; <li>[[stateChangeLogger]] link:kafka-controller-StateChangeLogger.adoc[StateChangeLogger]</li> <li>[[threadNamePrefix]] Thread name prefix (default: <code>(empty)</code>)</li> <p>=== [[addNewBroker]] Registering New Broker -- <code>addNewBroker</code> Internal Method</p>"},{"location":"controller/ControllerChannelManager/#source-scala_1","title":"[source, scala]","text":"<p>addNewBroker(   broker: Broker): Unit</p> <p><code>addNewBroker</code> prints out the following DEBUG message to the logs:</p> <pre><code>Controller [brokerId] trying to connect to broker [id]\n</code></pre> <p><code>addNewBroker</code> finds the name of the listener to use for communication with the broker based on link:kafka-properties.adoc#control.plane.listener.name[control.plane.listener.name] configuration property (if defined) or link:kafka-properties.adoc#inter.broker.listener.name[inter.broker.listener.name].</p> <p><code>addNewBroker</code> finds the security protocol to use for communication with the broker based on link:kafka-properties.adoc#control.plane.listener.name[control.plane.listener.name] and link:kafka-properties.adoc#listener.security.protocol.map[listener.security.protocol.map] configuration properties (if defined) or link:kafka-properties.adoc#security.inter.broker.protocol[security.inter.broker.protocol].</p> <p><code>addNewBroker</code> requests the <code>Broker</code> for the link:kafka-cluster-Broker.adoc#node[node] for the listener name.</p> <p><code>addNewBroker</code> creates a new <code>LogContext</code> to use the prefix:</p> <pre><code>[Controller id=[brokerId], targetBrokerId=[brokerNode]]\n</code></pre> <p><code>addNewBroker</code> creates a link:kafka-clients-NetworkClient.adoc[NetworkClient]. Firstly, <code>addNewBroker</code> creates a link:kafka-common-network-ChannelBuilders.adoc#clientChannelBuilder[ChannelBuilder] (for the security protocol, the listener name, <code>SERVER</code> JAAS context type and SASL-related properties) and, if it is a link:kafka-common-Reconfigurable.adoc[Reconfigurable], adds it to the &lt;&gt; as link:kafka-server-KafkaConfig.adoc#addReconfigurable[reconfigurable]. <code>addNewBroker</code> then creates a link:kafka-common-network-Selector.adoc[Selector] with <code>controller-channel</code> metric group (and <code>broker-id</code> of the broker node). <p><code>addNewBroker</code> builds a thread name per the optional &lt;&gt;: <pre><code>[threadNamePrefix]:Controller-[brokerId]-to-broker-[id]-send-thread\n</code></pre> <p><code>addNewBroker</code> creates a new &lt;&gt; timer metric with the id of the broker to connect to. <p><code>addNewBroker</code> creates a daemon link:kafka-controller-RequestSendThread.adoc[RequestSendThread] for the broker ID (of the controller broker), the &lt;&gt;, the <code>NetworkClient</code>, the <code>RequestRateAndQueueTimeMs</code> metric, the &lt;&gt;, and the thread name. <p><code>addNewBroker</code> creates a &lt;&gt; gauge metric (with the id of the broker to connect) that is the number of the link:kafka-controller-AbstractControlRequest.adoc[AbstractControlRequest] messages in the queue. <p>In the end, <code>addNewBroker</code> registers (adds) the id of the broker to connect with a new &lt;&gt; to the &lt;&gt; internal registry. <p>NOTE: <code>addNewBroker</code> is used when <code>ControllerChannelManager</code> is requested to &lt;&gt; (and connect to brokers) and &lt;&gt;. <p>=== [[addBroker]] Registering Newly-Added Broker -- <code>addBroker</code> Method</p>"},{"location":"controller/ControllerChannelManager/#source-scala_2","title":"[source, scala]","text":""},{"location":"controller/ControllerChannelManager/#addbrokerbroker-broker-unit","title":"addBroker(broker: Broker): Unit","text":"<p><code>addBroker</code>...FIXME</p> <p>NOTE: <code>addBroker</code> is used when <code>KafkaController</code> is requested to link:kafka-controller-KafkaController.adoc#processBrokerChange[process a BrokerChange controller event].</p> <p>=== [[removeBroker]] Deregistering Broker -- <code>removeBroker</code> Method</p>"},{"location":"controller/ControllerChannelManager/#source-scala_3","title":"[source, scala]","text":""},{"location":"controller/ControllerChannelManager/#removebrokerbrokerid-int-unit","title":"removeBroker(brokerId: Int): Unit","text":"<p><code>removeBroker</code> finds the broker metadata in the &lt;&gt; internal registry that is then used to &lt;&gt;. <p>NOTE: <code>removeBroker</code> is used exclusively when <code>KafkaController</code> is requested to &lt;&gt;. <p>=== [[startup]] Starting Up -- <code>startup</code> Method</p>"},{"location":"controller/ControllerChannelManager/#source-scala_4","title":"[source, scala]","text":""},{"location":"controller/ControllerChannelManager/#startup-unit","title":"startup(): Unit","text":"<p><code>startup</code>...FIXME</p> <p>NOTE: <code>startup</code> is used when <code>KafkaController</code> is requested to link:kafka-controller-KafkaController.adoc#initializeControllerContext[initializeControllerContext].</p> <p>=== [[shutdown]] Shutting Down -- <code>shutdown</code> Method</p>"},{"location":"controller/ControllerChannelManager/#source-scala_5","title":"[source, scala]","text":""},{"location":"controller/ControllerChannelManager/#shutdown-unit","title":"shutdown(): Unit","text":"<p><code>shutdown</code>...FIXME</p> <p>NOTE: <code>shutdown</code> is used when...FIXME</p> <p>=== [[sendRequest]] Sending AbstractControlRequest Out to Broker -- <code>sendRequest</code> Method</p>"},{"location":"controller/ControllerChannelManager/#source-scala_6","title":"[source, scala]","text":"<p>sendRequest(   brokerId: Int,   request: AbstractControlRequest.Builder[_ &lt;: AbstractControlRequest],   callback: AbstractResponse =&gt; Unit = null)</p> <p><code>sendRequest</code>...FIXME</p> <p>NOTE: <code>sendRequest</code> is used exclusively when <code>ControllerBrokerRequestBatch</code> is requested to link:kafka-controller-ControllerBrokerRequestBatch.adoc#sendRequest[send a controller request to a broker].</p> <p>=== [[removeExistingBroker]] <code>removeExistingBroker</code> Internal Method</p>"},{"location":"controller/ControllerChannelManager/#source-scala_7","title":"[source, scala]","text":"<p>removeExistingBroker(   brokerState: ControllerBrokerStateInfo): Unit</p> <p><code>removeExistingBroker</code>...FIXME</p> <p>NOTE: <code>removeExistingBroker</code> is used when...FIXME</p> <p>=== [[startRequestSendThread]] Starting RequestSendThread -- <code>startRequestSendThread</code> Internal Method</p>"},{"location":"controller/ControllerChannelManager/#source-scala_8","title":"[source, scala]","text":"<p>startRequestSendThread(   brokerId: Int): Unit</p> <p><code>startRequestSendThread</code> finds the <code>RequestSendThread</code> in the broker metadata in the &lt;&gt; internal registry and, if the thread has not started yet, <code>startRequestSendThread</code> &lt;&gt;. <p>NOTE: <code>startRequestSendThread</code> is used when <code>ControllerChannelManager</code> is requested to &lt;&gt; and &lt;&gt;. <p>=== [[ControllerBrokerStateInfo]] ControllerBrokerStateInfo</p> <p><code>ControllerBrokerStateInfo</code> is a broker metadata that holds the following:</p> <ul> <li>[[networkClient]] &lt;&gt; <li>[[brokerNode]] Broker Node</li> <li>[[messageQueue]] Message Queue (<code>BlockingQueue[QueueItem]</code>)</li> <li>[[requestSendThread]] <code>RequestSendThread</code></li> <li>[[queueSizeGauge]] Queue Size (<code>Gauge[Int]</code>)</li> <li>[[requestRateAndTimeMetrics]] RequestRateAndTime Metrics</li> <li>[[reconfigurableChannelBuilder]] &lt;&gt;"},{"location":"controller/ControllerContext/","title":"ControllerContext","text":""},{"location":"controller/ControllerContext/#review-me","title":"Review Me","text":"<p><code>ControllerContext</code> is the context of an active &lt;&gt; (and is &lt;&gt; right when <code>KafkaController</code> is &lt;&gt;). <p>[[creating-instance]] <code>ControllerContext</code> takes no input arguments to be created.</p> <p>=== [[allPartitions]] <code>allPartitions</code> Method</p>"},{"location":"controller/ControllerContext/#source-scala","title":"[source, scala]","text":""},{"location":"controller/ControllerContext/#allpartitions-settopicpartition","title":"allPartitions: Set[TopicPartition]","text":"<p><code>allPartitions</code> converts the &lt;&gt; into <code>TopicPartitions</code>, i.e. <code>allPartitions</code> takes the partitions for the topics and simply creates new <code>TopicPartitions</code>."},{"location":"controller/ControllerContext/#note","title":"[NOTE]","text":"<p><code>allPartitions</code> is used when:</p> <ul> <li> <p><code>KafkaController</code> is requested to &lt;&gt;, &lt;&gt;, and &lt;&gt; <li> <p><code>PartitionStateMachine</code> is requested to &lt;&gt;"},{"location":"controller/ControllerContext/#replicastatemachine-is-requested-to","title":"* <code>ReplicaStateMachine</code> is requested to &lt;&gt; <p>=== [[updatePartitionReplicaAssignment]] <code>updatePartitionReplicaAssignment</code> Method</p>","text":""},{"location":"controller/ControllerContext/#source-scala_1","title":"[source, scala]","text":""},{"location":"controller/ControllerContext/#updatepartitionreplicaassignmenttopicpartition-topicpartition-newreplicas-seqint-unit","title":"updatePartitionReplicaAssignment(topicPartition: TopicPartition, newReplicas: Seq[Int]): Unit","text":"<p><code>updatePartitionReplicaAssignment</code> simply updates the &lt;&gt; registry with <code>newReplicas</code> for the topic and the partition (of a given <code>TopicPartition</code>)."},{"location":"controller/ControllerContext/#note_1","title":"[NOTE] <p><code>updatePartitionReplicaAssignment</code> is used when:</p> <ul> <li><code>KafkaController</code> is requested to &lt;&gt;, &lt;&gt;, &lt;&gt;, and at &lt;&gt; and &lt;&gt; controller events","text":""},{"location":"controller/ControllerContext/#replicastatemachine-is-requested-to_1","title":"* <code>ReplicaStateMachine</code> is requested to &lt;&gt; <p>=== [[partitionReplicaAssignment]] <code>partitionReplicaAssignment</code> Method</p>","text":""},{"location":"controller/ControllerContext/#source-scala_2","title":"[source, scala]","text":"<p>partitionReplicaAssignment(   topicPartition: TopicPartition): Seq[Int]</p> <p><code>partitionReplicaAssignment</code> finds the brokers with the replicas of the given partition (aka partition replica assignment).</p> <p>Internally, <code>partitionReplicaAssignment</code> finds broker IDs of the replicas of the given partition (<code>TopicPartition</code>) in the &lt;&gt; internal registry. <p><code>partitionReplicaAssignment</code> returns an empty collection when no topic or partition are found.</p> <p>NOTE: <code>partitionReplicaAssignment</code> is used when...FIXME</p> <p>=== [[putReplicaStateIfNotExists]] <code>putReplicaStateIfNotExists</code> Method</p>"},{"location":"controller/ControllerContext/#source-scala_3","title":"[source, scala]","text":"<p>putReplicaStateIfNotExists(   replica: PartitionAndReplica,   state: ReplicaState): Unit</p> <p><code>putReplicaStateIfNotExists</code> simply adds the replica to the &lt;&gt; internal registry unless available already. <p>NOTE: <code>putReplicaStateIfNotExists</code> is used exclusively when <code>ZkReplicaStateMachine</code> is requested to &lt;&gt;. <p>=== [[checkValidReplicaStateChange]] <code>checkValidReplicaStateChange</code> Method</p>"},{"location":"controller/ControllerContext/#source-scala_4","title":"[source, scala]","text":"<p>checkValidReplicaStateChange(   replicas: Seq[PartitionAndReplica],   targetState: ReplicaState ): (Seq[PartitionAndReplica], Seq[PartitionAndReplica])</p> <p>For every replica (in the given replicas), <code>checkValidReplicaStateChange</code> &lt;&gt; with the target state (<code>ReplicaState</code>). <p>NOTE: <code>checkValidReplicaStateChange</code> is used exclusively when <code>ZkReplicaStateMachine</code> is requested to &lt;&gt;. <p>=== [[checkValidPartitionStateChange]] <code>checkValidPartitionStateChange</code> Method</p>"},{"location":"controller/ControllerContext/#source-scala_5","title":"[source, scala]","text":"<p>checkValidPartitionStateChange(   partitions: Seq[TopicPartition],   targetState: PartitionState ): (Seq[TopicPartition], Seq[TopicPartition])</p> <p>For every replica (in the given replicas), <code>checkValidPartitionStateChange</code> &lt;&gt; with the target state (<code>PartitionState</code>). <p>NOTE: <code>checkValidPartitionStateChange</code> is used exclusively when <code>ZkReplicaStateMachine</code> is requested to &lt;&gt;. <p>=== [[partitionsInStates]] Finding Partitions by Given States -- <code>partitionsInStates</code> Method</p>"},{"location":"controller/ControllerContext/#source-scala_6","title":"[source, scala]","text":"<p>partitionsInStates(   states: Set[PartitionState]): Set[TopicPartition] partitionsInStates(   topic: String, states: Set[PartitionState]): Set[TopicPartition]</p> <p><code>partitionsInStates</code> uses the &lt;&gt; internal registry to find all of the <code>TopicPartitions</code> (of the topic if defined) in the given <code>PartitionStates</code>. <p>NOTE: <code>partitionsInStates</code> is used when <code>PartitionStateMachine</code> is requested to link:kafka-controller-PartitionStateMachine.md#triggerOnlinePartitionStateChange[triggerOnlinePartitionStateChange].</p>"},{"location":"controller/ControllerContext/#controllerstats","title":"ControllerStats <p><code>ControllerStats</code> with <code>UncleanLeaderElectionsPerSec</code> meter metric and <code>KafkaTimers</code> for every ControllerState (except Idle state)</p> <p><code>stats</code> is used exclusively to create the &lt;&gt; (of &lt;&gt;) that is then used to collect the times (metrics) of &lt;&gt; (except &lt;&gt;) <ul> <li>Every <code>ControllerState</code> has the &lt;&gt; metric defined (except &lt;&gt; state)  <p>The timer metric name pattern is kafka.controller:type=ControllerStats,name=.</p>","text":""},{"location":"controller/ControllerEvent/","title":"ControllerEvent","text":"<p><code>ControllerEvent</code> is an abstraction of events in the lifecycle of KafkaController (state machine) that trigger state change.</p> <p><code>ControllerEvent</code> events are managed by ControllerEventManager.</p>"},{"location":"controller/ControllerEvent/#contract","title":"Contract","text":""},{"location":"controller/ControllerEvent/#preempt","title":"preempt <pre><code>preempt(): Unit\n</code></pre> <p>Used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"controller/ControllerEvent/#controllerstate","title":"ControllerState <pre><code>state: ControllerState\n</code></pre> <p>ControllerState of the ControllerEventManager (while processing controller events)</p> <p>Used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"controller/ControllerEvent/#implementations","title":"Implementations","text":"<ul> <li>AllocateProducerIds</li> <li>AlterPartitionReceived</li> <li>ApiPartitionReassignment</li> <li>AutoPreferredReplicaLeaderElection</li> <li>BrokerChange</li> <li>BrokerModifications</li> <li>ControlledShutdown</li> <li>ControllerChange</li> <li>Expire</li> <li>IsrChangeNotification</li> <li>LeaderAndIsrResponseReceived</li> <li>ListPartitionReassignments</li> <li>LogDirEventNotification</li> <li>MockEvent</li> <li>PartitionModifications</li> <li>PartitionReassignmentIsrChange</li> <li>Reelect</li> <li>RegisterBrokerAndReelect</li> <li>ReplicaLeaderElection</li> <li>ShutdownEventThread</li> <li>Startup</li> <li>TopicChange</li> <li>TopicDeletion</li> <li>TopicDeletionStopReplicaResponseReceived</li> <li>TopicUncleanLeaderElectionEnable</li> <li>UncleanLeaderElectionEnable</li> <li>UpdateFeatures</li> <li>UpdateMetadataResponseReceived</li> <li>ZkPartitionReassignment</li> </ul>"},{"location":"controller/ControllerEvent/#review-me","title":"Review Me","text":"<p>[[implementations]] .ControllerEvents [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | ControllerEvent | ControllerState | Description</p> <p>| &lt;&gt; | &lt;&gt; | [[AutoPreferredReplicaLeaderElection]] <p>| &lt;&gt; | &lt;&gt; | [[BrokerChange]] <p>| BrokerModifications | &lt;&gt; | [[BrokerModifications]] <p>| ControlledShutdown | &lt;&gt; | [[ControlledShutdown]] <p>| ControllerChange | &lt;&gt; | [[ControllerChange]] <p>| Expire | &lt;&gt; | [[Expire]] <p>| IsrChangeNotification | &lt;&gt; a| [[IsrChangeNotification]] Emitted when the &lt;&gt; has been notified about the znode child change <p>| &lt;&gt; | &lt;&gt; | [[LeaderAndIsrResponseReceived]] <p>| LogDirEventNotification | &lt;&gt; | [[LogDirEventNotification]] <p>| PartitionModifications | &lt;&gt; | [[PartitionModifications]] Emitted when one of the &lt;&gt; has been notified about the znode change <p>| PartitionReassignment | &lt;&gt; | [[PartitionReassignment]] <p>| PartitionReassignmentIsrChange | &lt;&gt; | [[PartitionReassignmentIsrChange]] <p>| &lt;&gt; | &lt;&gt; | [[PreferredReplicaLeaderElection]] <p>| &lt;&gt; | &lt;&gt; | [[Reelect]] <p>| RegisterBrokerAndReelect | &lt;&gt; | [[RegisterBrokerAndReelect]] <p>| ShutdownEventThread | &lt;&gt; | [[ShutdownEventThread]] <p>| &lt;&gt; | &lt;&gt; | [[Startup]] <p>| TopicChange | &lt;&gt; | [[TopicChange]] <p>| &lt;&gt; | &lt;&gt; | [[TopicDeletion]] <p>| TopicDeletionStopReplicaResponseReceived | &lt;&gt; | [[TopicDeletionStopReplicaResponseReceived]] <p>| TopicUncleanLeaderElectionEnable | &lt;&gt; | [[TopicUncleanLeaderElectionEnable]] <p>| UncleanLeaderElectionEnable | &lt;&gt; | [[UncleanLeaderElectionEnable]] Emitted when <code>KafkaController</code> is requested to &lt;&gt; <p>|===</p> <p>NOTE: <code>ControllerEvent</code> is a Scala sealed trait and so all the possible &lt;&gt; are in a single compilation unit (i.e. a file)."},{"location":"controller/ControllerEventManager/","title":"ControllerEventManager","text":"<p><code>ControllerEventManager</code> manages the event queue and the controller-event-thread (for event processing) for KafkaController.</p> <p></p>"},{"location":"controller/ControllerEventManager/#creating-instance","title":"Creating Instance","text":"<p><code>ControllerEventManager</code> takes the following to be created:</p> <ul> <li> Controller ID (i.e. the broker.id of the broker) <li> ControllerEventProcessor <li> <code>Time</code> <li>rateAndTimeMetrics</li> <li> <code>eventQueueTimeTimeoutMs</code> (default: 300000) <p><code>ControllerEventManager</code> is created when:</p> <ul> <li><code>KafkaController</code> is created</li> </ul>"},{"location":"controller/ControllerEventManager/#rateandtimemetrics","title":"rateAndTimeMetrics <pre><code>rateAndTimeMetrics: Map[ControllerState, KafkaTimer]\n</code></pre> <p><code>ControllerEventManager</code> is given <code>rateAndTimeMetrics</code> that are <code>KafkaTimers</code> per ControllerState.</p>","text":""},{"location":"controller/ControllerEventManager/#performance-metrics","title":"Performance Metrics","text":"<p><code>ControllerEventManager</code> is a KafkaMetricsGroup and registers the following performance metrics in kafka.controller:type=ControllerEventManager group.</p> <p></p>"},{"location":"controller/ControllerEventManager/#eventqueuesize","title":"EventQueueSize <p>Number of QueuedEvents in the event queue</p>","text":""},{"location":"controller/ControllerEventManager/#eventqueuetimems","title":"EventQueueTimeMs","text":""},{"location":"controller/ControllerEventManager/#event-queue","title":"Event Queue <p><code>ControllerEventManager</code> creates a <code>LinkedBlockingQueue</code> (Java) of QueuedEvents.</p>  <p>LinkedBlockingQueue</p> <p><code>LinkedBlockingQueue</code> is an optionally-bounded blocking queue based on linked nodes that orders elements in first-in-first-out fashion.</p>  <p>Use EventQueueSize metric to monitor the depth of the queue.</p> <p>The <code>LinkedBlockingQueue</code> is used to:</p> <ul> <li>put a ControllerEvent</li> <li>clearAndPut</li> <li>pollFromEventQueue</li> </ul>","text":""},{"location":"controller/ControllerEventManager/#emitting-enqueuing-controller-event","title":"Emitting (Enqueuing) Controller Event <pre><code>put(\n  event: ControllerEvent): QueuedEvent\n</code></pre> <p><code>put</code> creates a QueuedEvent with the input ControllerEvent and adds it to the end of the event queue.</p>  <p><code>put</code> is used when:</p> <ul> <li><code>ControllerBrokerRequestBatch</code> is requested to sendEvent</li> <li><code>ControllerEventManager</code> is requested to clearAndPut</li> <li>KafkaController is doing its operation (and emits events)</li> </ul>","text":""},{"location":"controller/ControllerEventManager/#starting-controllereventthread","title":"Starting ControllerEventThread <pre><code>start(): Unit\n</code></pre> <p><code>start</code> requests the ControllerEventThread to start processing ControllerEvents.</p> <p><code>start</code> is used when:</p> <ul> <li><code>KafkaController</code> is requested to start up</li> </ul>","text":""},{"location":"controller/ControllerEventManager/#controller-event-thread","title":"controller-event-thread <p><code>ControllerEventManager</code> creates a ControllerEventThread when created.</p> <p>The name of <code>ControllerEventThread</code> is controller-event-thread.</p> <p>The <code>ControllerEventThread</code> is requested to start when start and runs until close.</p>","text":""},{"location":"controller/ControllerEventManager/#controllerstate","title":"ControllerState <p><code>ControllerEventManager</code> is in one of the possible ControllerStates:</p> <ul> <li> <p>Idle when <code>ControllerEventManager</code> is created and right after <code>ControllerEventThread</code> has finished processing a controller event</p> </li> <li> <p>State transitions happen per the requested state of the <code>ControllerEvent</code> being processed (while <code>ControllerEventThread</code> is processing controller events)</p> </li> </ul>  <p>Note</p> <p>The state of <code>ControllerEventManager</code> is exactly the state of the parent KafkaController.</p>","text":""},{"location":"controller/ControllerEventManager/#pollfromeventqueue","title":"pollFromEventQueue <pre><code>pollFromEventQueue(): QueuedEvent\n</code></pre> <p><code>pollFromEventQueue</code> takes a QueuedEvent off the queue.</p>  <p><code>pollFromEventQueue</code> is used when:</p> <ul> <li><code>ControllerEventThread</code> is requested to doWork</li> </ul>","text":""},{"location":"controller/ControllerEventProcessor/","title":"ControllerEventProcessor","text":"<p><code>ControllerEventProcessor</code> is an abstraction of processors that can process and preempt controller events.</p>"},{"location":"controller/ControllerEventProcessor/#contract","title":"Contract","text":""},{"location":"controller/ControllerEventProcessor/#preempt","title":"preempt <pre><code>preempt(\n  event: ControllerEvent): Unit\n</code></pre> <p>Preempts a ControllerEvent</p> <p>Used when:</p> <ul> <li><code>QueuedEvent</code> is requested to preempt a ControllerEventProcessor</li> </ul>","text":""},{"location":"controller/ControllerEventProcessor/#process","title":"process <pre><code>process(\n  event: ControllerEvent): Unit\n</code></pre> <p>Processes a ControllerEvent</p> <p>Used when:</p> <ul> <li><code>QueuedEvent</code> is requested to process a ControllerEventProcessor</li> </ul>","text":""},{"location":"controller/ControllerEventProcessor/#implementations","title":"Implementations","text":"<ul> <li>KafkaController</li> </ul>"},{"location":"controller/ControllerEventThread/","title":"ControllerEventThread","text":"<p><code>ControllerEventThread</code> is a <code>ShutdownableThread</code> that ControllerEventManager uses to process QueuedEvents (asynchronously on a separate thread).</p> <p></p>"},{"location":"controller/ControllerEventThread/#review-me","title":"Review Me","text":"<p><code>ControllerEventThread</code> is a <code>ShutdownableThread</code> that is &lt;&gt; for &lt;&gt; (with the &lt;&gt; being controller-event-thread). <pre><code>// jstack [brokerPid]\n\"controller-event-thread\" #42 prio=5 os_prio=31 cpu=387,10ms elapsed=82679,68s tid=0x00007f920e489800 nid=0x14703 waiting on condition  [0x000070000fcea000]\n   java.lang.Thread.State: WAITING (parking)\n    at jdk.internal.misc.Unsafe.park(java.base@12.0.2/Native Method)\n    - parking to wait for  &lt;0x00000007c07f0298&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at java.util.concurrent.locks.LockSupport.park(java.base@12.0.2/LockSupport.java:194)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(java.base@12.0.2/AbstractQueuedSynchronizer.java:2081)\n    at java.util.concurrent.LinkedBlockingQueue.take(java.base@12.0.2/LinkedBlockingQueue.java:433)\n    at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:127)\n    at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:89)\n</code></pre> <p><code>ControllerEventThread</code> is &lt;&gt; with &lt;&gt;. <p>[[creating-instance]][[name]] <code>ControllerEventThread</code> takes the name of the thread to be created.</p> <p>[[logIdent]] <code>ControllerEventThread</code> uses [ControllerEventThread controllerId=[controllerId]] as the logging prefix (aka <code>logIdent</code>).</p> <p>=== [[doWork]] Processing Controller Events -- <code>doWork</code> Method</p>"},{"location":"controller/ControllerEventThread/#source-scala","title":"[source, scala]","text":""},{"location":"controller/ControllerEventThread/#dowork-unit","title":"doWork(): Unit","text":"<p><code>doWork</code> takes (and removes) a &lt;&gt; from the head of the &lt;&gt; (waiting for a <code>QueuedEvent</code> to be available if the queue is empty). <p>NOTE: The very first event in the event queue is <code>Startup</code> that <code>KafkaController</code> puts when it is link:kafka-controller-KafkaController.adoc#startup[started].</p> <p><code>doWork</code> sets &lt;&gt; (of <code>ControllerEventManager</code>) to be the &lt;&gt; of the controller event. <p><code>doWork</code> requests the &lt;&gt; to <code>update</code> (with the time between the current time and the &lt;&gt;). <p><code>doWork</code> finds the <code>KafkaTimer</code> for the current controller state (in &lt;&gt; lookup table) to measure and record the time to requests the event to &lt;&gt; using the &lt;&gt;. <p>In the end, <code>doWork</code> sets the &lt;&gt; (of <code>ControllerEventManager</code>) as <code>Idle</code>. <p>In case of any error (<code>Throwable</code>), <code>doWork</code> simply prints out the following ERROR message to the logs:</p> <pre><code>Uncaught error processing event [controllerEvent]\n</code></pre>"},{"location":"controller/ControllerState/","title":"ControllerState","text":"<p>== [[ControllerState]] ControllerState Contract</p> <p><code>ControllerState</code> is the &lt;&gt; of the &lt;&gt; that &lt;&gt; can be in. <p>Every <code>ControllerEvent</code> has an associated &lt;&gt;. When a <code>ControllerEvent</code> is processed, it triggers a state transition to the requested state. These state transitions are used for &lt;&gt; and the owning &lt;&gt;. <p><code>ControllerState</code> has the &lt;&gt; (except &lt;&gt; state). &lt;&gt; uses them to build a registry of <code>KafkaTimers</code> for every <code>ControllerState</code>. The timer metric name pattern is kafka.controller:type=ControllerStats,name=. <p>.ControllerStates and RateAndTimeMs Timer Metrics in JConsole image::images/ControllerStates-RateAndTimeMs-Timer-Metrics-in-JConsole.png[align=\"center\"]</p> <p>[[contract]] .ControllerState Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| value a| [[value]]</p>"},{"location":"controller/ControllerState/#source-scala","title":"[source, scala]","text":""},{"location":"controller/ControllerState/#value-byte","title":"value: Byte","text":"<p>Internal identifier</p> <p>| rateAndTimeMetricName a| [[rateAndTimeMetricName]]</p>"},{"location":"controller/ControllerState/#source-scala_1","title":"[source, scala]","text":""},{"location":"controller/ControllerState/#rateandtimemetricname-optionstring","title":"rateAndTimeMetricName: Option[String]","text":"<p><code>rateAndTimeMetricName</code> depends on the &lt;&gt; flag: <ul> <li> <p>[ControllerState_name]RateAndTimeMs when enabled</p> </li> <li> <p>Undefined when disabled</p> </li> </ul> <p>| hasRateAndTimeMetric a| [[hasRateAndTimeMetric]]</p>"},{"location":"controller/ControllerState/#source-scala_2","title":"[source, scala]","text":""},{"location":"controller/ControllerState/#hasrateandtimemetric-boolean","title":"hasRateAndTimeMetric: Boolean","text":"<p>Controls whether the <code>ControllerState</code> has a &lt;&gt; (<code>true</code>) or not (<code>false</code>) <p>Default: <code>true</code> |===</p> <p>[[implementations]] .ControllerStates (and Triggering ControllerEvents) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | ControllerState | Description</p> <p>| AutoLeaderBalance | [[AutoLeaderBalance]]</p> <p>| BrokerChange | [[BrokerChange]] &lt;&gt; is LeaderElectionRateAndTimeMs <p>ControllerEvents: BrokerChange, BrokerModifications</p> <p>| ControlledShutdown | [[ControlledShutdown]]</p> <p>| ControllerChange | [[ControllerChange]] ControllerEvents: RegisterBrokerAndReelect</p> <p>| ControllerShutdown | [[ControllerShutdown]]</p> <p>| Idle | [[Idle]] The initial state of &lt;&gt; (and indirectly &lt;&gt;) <p><code>Idle</code> &lt;&gt;. <p>| IsrChange | [[IsrChange]] ControllerEvents: &lt;&gt; <p>| LeaderAndIsrResponseReceived | [[LeaderAndIsrResponseReceived]]</p> <p>| LogDirChange | [[LogDirChange]] ControllerEvents: LogDirEventNotification</p> <p>| ManualLeaderBalance | [[ManualLeaderBalance]] ControllerEvents: &lt;&gt; <p>| PartitionReassignment | [[PartitionReassignment]] ControllerEvents: PartitionReassignment, PartitionReassignmentIsrChange</p> <p>| TopicChange | [[TopicChange]] ControllerEvents: TopicChange, PartitionModifications</p> <p>| TopicDeletion | [[TopicDeletion]] ControllerEvents: TopicDeletion</p> <p>| TopicUncleanLeaderElectionEnable | [[TopicUncleanLeaderElectionEnable]]</p> <p>| UncleanLeaderElectionEnable | [[UncleanLeaderElectionEnable]]</p> <p>|===</p> <p>NOTE: <code>ControllerState</code> is a Scala sealed abstract class and so all the possible &lt;&gt; are in a single compilation unit (i.e. a file)."},{"location":"controller/Election/","title":"Election","text":"<p><code>Election</code> is a utility with the algorithms for partition leader election.</p>"},{"location":"controller/Election/#leaderforoffline","title":"leaderForOffline <pre><code>leaderForOffline(\n  controllerContext: ControllerContext,\n  isLeaderRecoverySupported: Boolean,\n  partitionsWithUncleanLeaderRecoveryState: Seq[(TopicPartition, Option[LeaderAndIsr], Boolean)]\n): Seq[ElectionResult] // (1)!\nleaderForOffline(\n  partition: TopicPartition,\n  leaderAndIsrOpt: Option[LeaderAndIsr],\n  uncleanLeaderElectionEnabled: Boolean,\n  isLeaderRecoverySupported: Boolean,\n  controllerContext: ControllerContext): ElectionResult\n</code></pre> <ol> <li>Uses the other <code>leaderForOffline</code> for every tuple in <code>partitionsWithUncleanLeaderRecoveryState</code></li> </ol> <p><code>leaderForOffline</code> requests the given ControllerContext for the partition replicas and removes replicas that are not online.</p> <p>For the input <code>LeaderAndIsr</code> defined, <code>leaderForOffline</code> offlinePartitionLeaderElection (with the replica brokers) and...FIXME</p>  <p><code>leaderForOffline</code> is used when:</p> <ul> <li><code>ZkPartitionStateMachine</code> is requested to doElectLeaderForPartitions (with OfflinePartitionLeaderElectionStrategy)</li> </ul>","text":""},{"location":"controller/KafkaController/","title":"KafkaController","text":"<p><code>KafkaController</code> is created and immediately started alongside KafkaServer.</p> <p></p> <p><code>KafkaController</code> uses listeners as a notification system to react to changes in Zookeeper.</p> <p><code>KafkaController</code> is a state machine (using controller events).</p>"},{"location":"controller/KafkaController/#creating-instance","title":"Creating Instance","text":"<p><code>KafkaController</code> takes the following to be created:</p> <ul> <li> KafkaConfig <li> KafkaZkClient <li> <code>Time</code> <li> Metrics <li> <code>BrokerInfo</code> <li> <code>initialBrokerEpoch</code> <li> <code>DelegationTokenManager</code> <li> <code>BrokerFeatures</code> <li> <code>FinalizedFeatureCache</code> <li> Thread name prefix (default: undefined) <p><code>KafkaController</code> is created when:</p> <ul> <li><code>KafkaServer</code> is requested to start up</li> </ul>"},{"location":"controller/KafkaController/#controllercontext","title":"ControllerContext <p><code>KafkaController</code> creates a ControllerContext when created.</p>","text":""},{"location":"controller/KafkaController/#controllerchannelmanager","title":"ControllerChannelManager <p><code>KafkaController</code> creates a <code>ControllerChannelManager</code> when created.</p> <p><code>ControllerChannelManager</code> is used to create separate <code>ControllerBrokerRequestBatch</code>es of the KafkaController itself, the ZkReplicaStateMachine and ZkPartitionStateMachine.</p> <p><code>ControllerChannelManager</code> is requested to start up when <code>KafkaController</code> is requested to start controller election (and a broker is successfully elected as the active controller).</p> <p><code>KafkaController</code> uses the <code>ControllerChannelManager</code> to add or remove brokers when processing broker changes in Zookeeper (a new or updated znode under <code>/brokers/ids</code> path).</p> <p><code>ControllerChannelManager</code> is requested to shut down when <code>KafkaController</code> is requested to resign as the active controller.</p>","text":""},{"location":"controller/KafkaController/#performance-metrics","title":"Performance Metrics <p><code>KafkaController</code> is a KafkaMetricsGroup.</p>","text":""},{"location":"controller/KafkaController/#activecontrollercount","title":"ActiveControllerCount <p><code>1</code> if isActive. <code>0</code> otherwise.</p>","text":""},{"location":"controller/KafkaController/#offlinepartitionscount","title":"OfflinePartitionsCount","text":""},{"location":"controller/KafkaController/#preferredreplicaimbalancecount","title":"PreferredReplicaImbalanceCount","text":""},{"location":"controller/KafkaController/#controllerstate","title":"ControllerState","text":""},{"location":"controller/KafkaController/#globaltopiccount","title":"GlobalTopicCount","text":""},{"location":"controller/KafkaController/#globalpartitioncount","title":"GlobalPartitionCount","text":""},{"location":"controller/KafkaController/#topicstodeletecount","title":"TopicsToDeleteCount","text":""},{"location":"controller/KafkaController/#replicastodeletecount","title":"ReplicasToDeleteCount","text":""},{"location":"controller/KafkaController/#topicsineligibletodeletecount","title":"TopicsIneligibleToDeleteCount","text":""},{"location":"controller/KafkaController/#replicasineligibletodeletecount","title":"ReplicasIneligibleToDeleteCount","text":""},{"location":"controller/KafkaController/#activebrokercount","title":"ActiveBrokerCount","text":""},{"location":"controller/KafkaController/#fencedbrokercount","title":"FencedBrokerCount","text":""},{"location":"controller/KafkaController/#controllereventprocessor","title":"ControllerEventProcessor <p><code>KafkaController</code> is a <code>ControllerEventProcessor</code> to process and preempt controller events.</p>","text":""},{"location":"controller/KafkaController/#resigning-as-active-controller","title":"Resigning As Active Controller <pre><code>onControllerResignation(): Unit\n</code></pre> <p><code>onControllerResignation</code> starts by printing out the following DEBUG message to the logs:</p> <pre><code>Resigning\n</code></pre> <p><code>onControllerResignation</code> unsubscribes from intercepting Zookeeper events for the following znodes in order:</p> <ol> <li>Child changes to /isr_change_notification znode</li> <li>Data changes to /admin/reassign_partitions znode</li> <li>Data changes to /admin/preferred_replica_election znode</li> <li>Child changes to /log_dir_event_notification znode</li> </ol> <p><code>onControllerResignation</code> unregisterBrokerModificationsHandler.</p> <p><code>onControllerResignation</code> requests KafkaScheduler to <code>shutdown</code>.</p> <p><code>onControllerResignation</code> resets internal counters.</p> <p><code>onControllerResignation</code> unregisterPartitionReassignmentIsrChangeHandlers.</p> <p><code>onControllerResignation</code> requests the PartitionStateMachine to <code>shutdown</code>.</p> <p><code>onControllerResignation</code> unsubscribes from intercepting Zookeeper events for the following znode:</p> <ul> <li>Child changes to /brokers/topics znode</li> </ul> <p><code>onControllerResignation</code> unregisterPartitionModificationsHandlers.</p> <p><code>onControllerResignation</code> unsubscribes from intercepting Zookeeper events for the following znode:</p> <ul> <li>Child changes to /admin/delete_topics znode</li> </ul> <p><code>onControllerResignation</code> requests the ReplicaStateMachine to <code>shutdown</code>.</p> <p><code>onControllerResignation</code> unsubscribes from intercepting Zookeeper events for the following znode:</p> <ul> <li>Child changes to /brokers/ids znode</li> </ul> <p><code>onControllerResignation</code> requests the ControllerChannelManager to <code>shutdown</code>.</p> <p><code>onControllerResignation</code> requests the ControllerContext to <code>resetContext</code>.</p> <p>In the end, <code>onControllerResignation</code> prints out the following DEBUG message to the logs:</p> <pre><code>Resigned\n</code></pre>  <p><code>onControllerResignation</code> is used when:</p> <ul> <li><code>KafkaController</code> is requested to shutdown, triggerControllerMove, maybeResign, processExpire</li> </ul>","text":""},{"location":"controller/KafkaController/#processing-expire-event","title":"Processing Expire Event <pre><code>processExpire(): Unit\n</code></pre> <p><code>processExpire</code> sets the activeControllerId to <code>-1</code> followed by onControllerResignation.</p>  <p><code>processExpire</code> is used when:</p> <ul> <li><code>KafkaController</code> is requested to process Expire event</li> </ul>","text":""},{"location":"controller/KafkaController/#onbrokerstartup","title":"onBrokerStartup <pre><code>onBrokerStartup(\n  newBrokers: Seq[Int]): Unit\n</code></pre> <p><code>onBrokerStartup</code> prints out the following INFO message to the logs:</p> <pre><code>New broker startup callback for [comma-separated list of newBrokers]\n</code></pre> <p><code>onBrokerStartup</code> removes the <code>newBrokers</code> from the replicasOnOfflineDirs of the ControllerContext.</p> <p><code>onBrokerStartup</code> sends update metadata request to all the existing brokers in the cluster (of the ControllerContext).</p> <p><code>onBrokerStartup</code> sends update metadata request to the new brokers with the full set of partition states (for initialization).</p> <p><code>onBrokerStartup</code> requests the ControllerContext for the replicas on the given <code>newBrokers</code> (the entire list of partitions that it is supposed to host).</p> <p><code>onBrokerStartup</code> requests the ReplicaStateMachine to <code>handleStateChanges</code> with the replicas on the new brokers and <code>OnlineReplica</code> target state.</p> <p><code>onBrokerStartup</code> requests the PartitionStateMachine to <code>triggerOnlinePartitionStateChange</code>.</p> <p><code>onBrokerStartup</code> checks if topic deletion can be resumed. <code>onBrokerStartup</code> collects replicas (on the new brokers) that are scheduled to be deleted by requesting the TopicDeletionManager to <code>isTopicQueuedUpForDeletion</code>. If there are any, <code>onBrokerStartup</code> prints out the following INFO message to the logs and requests the TopicDeletionManager to <code>resumeDeletionForTopics</code>.</p> <pre><code>Some replicas [replicasForTopicsToBeDeleted] for topics scheduled for deletion [topicsToBeDeleted]\nare on the newly restarted brokers [newBrokers].\nSignaling restart of topic deletion for these topics\n</code></pre> <p>In the end, <code>onBrokerStartup</code> registerBrokerModificationsHandler with the new brokers.</p>  <p><code>onBrokerStartup</code> is used when:</p> <ul> <li><code>KafkaController</code> is requested to process a BrokerChange controller event</li> </ul>","text":""},{"location":"controller/KafkaController/#replicastatemachine","title":"ReplicaStateMachine <p><code>KafkaController</code> creates a ZkReplicaStateMachine when created.</p> <p><code>ZkReplicaStateMachine</code> is requested to &lt;&gt; at &lt;&gt; (when a broker is successfully &lt;&gt;) and &lt;&gt; at &lt;&gt;. <p><code>ZkReplicaStateMachine</code> is requested to &lt;&gt; at the following events: <ul> <li> <p>&lt;&gt; to transition replicas to <code>OnlineReplica</code> state  <li> <p>&lt;&gt; to transition replicas to <code>OnlineReplica</code> state  <li> <p>&lt;&gt; to transition replicas to <code>OfflineReplica</code> state  <li> <p>&lt;&gt; to transition replicas to <code>NewReplica</code> state first and then to <code>OnlineReplica</code>  <li> <p>&lt;&gt; to transition replicas to <code>OnlineReplica</code> state  <li> <p>&lt;&gt; to transition replicas to <code>OfflineReplica</code> state first and then to <code>ReplicaDeletionStarted</code>, <code>ReplicaDeletionSuccessful</code>, and <code>NonExistentReplica</code> in the end  <li> <p>&lt;&gt; to transition replicas to <code>NewReplica</code> state  <li> <p>&lt;&gt; to transition replicas to <code>OfflineReplica</code> state   <p><code>KafkaController</code> uses the <code>ZkReplicaStateMachine</code> to create the &lt;&gt;.","text":""},{"location":"controller/KafkaController/#shutting-down","title":"Shutting Down <pre><code>shutdown(): Unit\n</code></pre> <p><code>shutdown</code> requests the ControllerEventManager to close followed by onControllerResignation.</p>  <p><code>shutdown</code> is used when:</p> <ul> <li><code>KafkaServer</code> is requested to shut down.</li> </ul>","text":""},{"location":"controller/KafkaController/#controllereventmanager","title":"ControllerEventManager <p><code>KafkaController</code> creates a ControllerEventManager when created (with broker.id configuration property).</p> <p>The <code>ControllerEventManager</code> is used to create the following services:</p> <ul> <li>ControllerBrokerRequestBatch</li> <li>ZkReplicaStateMachine</li> <li>ZkPartitionStateMachine</li> <li>ControllerChangeHandler</li> <li>BrokerChangeHandler</li> <li>TopicChangeHandler</li> <li>TopicDeletionHandler</li> <li>PartitionReassignmentHandler</li> <li>PreferredReplicaElectionHandler</li> <li>IsrChangeNotificationHandler</li> <li>LogDirEventNotificationHandler</li> <li><code>BrokerModificationsHandler</code> (in registerBrokerModificationsHandler)</li> <li><code>PartitionReassignmentIsrChangeHandler</code> (in updateCurrentReassignment)</li> <li><code>PartitionModificationsHandler</code> (in registerPartitionModificationsHandlers)</li> </ul>","text":""},{"location":"controller/KafkaController/#processing-controller-events","title":"Processing Controller Events <pre><code>process(\n  event: ControllerEvent): Unit\n</code></pre> <p><code>process</code> is part of the ControllerEventProcessor abstraction.</p>  <p><code>process</code> handles the input ControllerEvent and updates the metrics.</p>    ControllerEvent Handler     AllocateProducerIds processAllocateProducerIds   AlterPartitionReceived processAlterPartition   ApiPartitionReassignment processApiPartitionReassignment   AutoPreferredReplicaLeaderElection processAutoPreferredReplicaLeaderElection   BrokerChange processBrokerChange   BrokerModifications processBrokerModification   ControlledShutdown processControlledShutdown   ControllerChange processControllerChange   Expire processExpire   IsrChangeNotification processIsrChangeNotification   LeaderAndIsrResponseReceived processLeaderAndIsrResponseReceived   ListPartitionReassignments processListPartitionReassignments   LogDirEventNotification processLogDirEventNotification   PartitionModifications processPartitionModifications   PartitionReassignmentIsrChange processPartitionReassignmentIsrChange   Reelect processReelect   RegisterBrokerAndReelect processRegisterBrokerAndReelect   ReplicaLeaderElection processReplicaLeaderElection   Startup processStartup   TopicChange processTopicChange   TopicDeletion processTopicDeletion   TopicDeletionStopReplicaResponseReceived processTopicDeletionStopReplicaResponseReceived   TopicUncleanLeaderElectionEnable processTopicUncleanLeaderElectionEnable   UncleanLeaderElectionEnable processUncleanLeaderElectionEnable   UpdateFeatures processFeatureUpdates   UpdateMetadataResponseReceived processUpdateMetadataResponseReceived   ZkPartitionReassignment processZkPartitionReassignment","text":""},{"location":"controller/KafkaController/#updatemetrics","title":"updateMetrics <pre><code>updateMetrics(): Unit\n</code></pre> <p><code>updateMetrics</code> updates the metrics (using the ControllerContext).</p>","text":""},{"location":"controller/KafkaController/#controllermovedexception","title":"ControllerMovedException <p>In case of a <code>ControllerMovedException</code>, <code>process</code> prints out the following INFO message to the logs and maybeResign.</p> <pre><code>Controller moved to another broker when processing [event].\n</code></pre>","text":""},{"location":"controller/KafkaController/#throwable","title":"Throwable <p>In case of any other error (<code>Throwable</code>), <code>process</code> simply prints out the following ERROR message to the logs:</p> <pre><code>Error processing event [event]\n</code></pre>","text":""},{"location":"controller/KafkaController/#partitionstatemachine","title":"PartitionStateMachine <p><code>KafkaController</code> creates a ZkPartitionStateMachine when created with the following:</p> <ul> <li>KafkaConfig</li> <li>StateChangeLogger</li> <li>ControllerContext</li> <li>KafkaZkClient</li> <li>A new ControllerBrokerRequestBatch</li> </ul> <p><code>KafkaController</code> uses this <code>ZkPartitionStateMachine</code> to create the TopicDeletionManager.</p> <p><code>ZkPartitionStateMachine</code> is requested to start up at onControllerFailover (when a broker is successfully elected as the controller) and shut down at controller resignation.</p> <p><code>ZkPartitionStateMachine</code> is requested to triggerOnlinePartitionStateChange at the following events:</p> <ul> <li>onBrokerStartup</li> <li>onReplicasBecomeOffline</li> <li>processUncleanLeaderElectionEnable</li> <li>processTopicUncleanLeaderElectionEnable</li> </ul> <p><code>ZkPartitionStateMachine</code> is requested to handleStateChanges at the following events:</p> <ul> <li>onReplicasBecomeOffline</li> <li>onNewPartitionCreation</li> <li>onReplicaElection</li> <li>moveReassignedPartitionLeaderIfRequired</li> <li>doControlledShutdown</li> </ul>","text":""},{"location":"controller/KafkaController/#enabletopicuncleanleaderelection","title":"enableTopicUncleanLeaderElection <pre><code>enableTopicUncleanLeaderElection(\n  topic: String): Unit\n</code></pre>  <p><code>enableTopicUncleanLeaderElection</code> does nothing on an inactive controller.</p> <p><code>enableTopicUncleanLeaderElection</code> requests the ControllerEventManager to enqueue a TopicUncleanLeaderElectionEnable event.</p>  <p><code>enableTopicUncleanLeaderElection</code> is used when:</p> <ul> <li><code>TopicConfigHandler</code> is requested to processConfigChanges</li> </ul>","text":""},{"location":"controller/KafkaController/#processtopicuncleanleaderelectionenable","title":"processTopicUncleanLeaderElectionEnable <pre><code>processTopicUncleanLeaderElectionEnable(\n  topic: String): Unit\n</code></pre>  <p><code>processTopicUncleanLeaderElectionEnable</code> is only executed on an active controller broker and does nothing otherwise.</p> <p><code>processTopicUncleanLeaderElectionEnable</code> prints out the following INFO message to the logs:</p> <pre><code>Unclean leader election has been enabled for topic [topic]\n</code></pre> <p>In the end, <code>processTopicUncleanLeaderElectionEnable</code> requests the ZkPartitionStateMachine to triggerOnlinePartitionStateChange.</p>  <p><code>processTopicUncleanLeaderElectionEnable</code> is used when:</p> <ul> <li><code>KafkaController</code> is requested to process a TopicUncleanLeaderElectionEnable event</li> </ul>","text":""},{"location":"controller/KafkaController/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>kafka.controller.KafkaController</code> logger to see what happens inside.</p> <p>Add the following line to <code>config/log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.controller.KafkaController=ALL\n</code></pre> <p>Refer to Logging</p>  <p>Note</p> <p>Please note that Kafka comes with a preconfigured <code>kafka.controller</code> logger in <code>config/log4j.properties</code>:</p> <pre><code>log4j.appender.controllerAppender=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.controllerAppender.DatePattern='.'yyyy-MM-dd-HH\nlog4j.appender.controllerAppender.File=${kafka.logs.dir}/controller.log\nlog4j.appender.controllerAppender.layout=org.apache.log4j.PatternLayout\nlog4j.appender.controllerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n\n\nlog4j.logger.kafka.controller=TRACE, controllerAppender\nlog4j.additivity.kafka.controller=false\n</code></pre> <p>That means that the logs of <code>KafkaController</code> go to <code>logs/controller.log</code> file at <code>TRACE</code> logging level and are not added to the main logs (per <code>log4j.additivity</code> being off)</p>","text":""},{"location":"controller/KafkaController/#logident","title":"logIdent <p><code>KafkaController</code> uses the following logging prefix (with the broker.id):</p> <pre><code>[Controller id=[brokerId]]\n</code></pre>","text":""},{"location":"controller/KafkaController/#review-me","title":"Review Me <p>[[state]] <code>KafkaController</code> is in one of the &lt;&gt; (that is the &lt;&gt; of the &lt;&gt;). <p><code>KafkaController</code> uses the &lt;&gt; to be notified about changes in the state of a Kafka cluster (that are reflected in changes in znodes of Apache Zookeeper) and propagate the state changes to other brokers.","text":""},{"location":"controller/KafkaController/#unsubscribing-from-child-changes-to-isr_change_notification-znode","title":"Unsubscribing from Child Changes to /isr_change_notification ZNode <pre><code>unregisterZNodeChildChangeHandler(): Unit\n</code></pre> <p><code>unregisterZNodeChildChangeHandler</code> prints out the following DEBUG message to the logs:</p> <pre><code>De-registering IsrChangeNotificationListener\n</code></pre> <p><code>unregisterZNodeChildChangeHandler</code> requests &lt;&gt; to link:kafka-ZkUtils.md#unsubscribeChildChanges[unsubscribe from intercepting changes] to <code>/isr_change_notification</code> znode with &lt;&gt;.  <p><code>unregisterZNodeChildChangeHandler</code> is used when:</p> <ul> <li><code>KafkaController</code> is requested to resign as the active controller</li> </ul>","text":""},{"location":"controller/KafkaController/#unsubscribing-from-child-changes-to-log_dir_event_notification-znode","title":"Unsubscribing from Child Changes to /log_dir_event_notification ZNode <pre><code>deregisterLogDirEventNotificationListener(): Unit\n</code></pre> <p><code>deregisterLogDirEventNotificationListener</code> prints out the following DEBUG message to the logs:</p> <pre><code>De-registering logDirEventNotificationListener\n</code></pre> <p><code>deregisterLogDirEventNotificationListener</code> requests &lt;&gt; to link:kafka-ZkUtils.md#unsubscribeChildChanges[unsubscribe from intercepting changes] to <code>/log_dir_event_notification</code> znode with &lt;&gt;.  <p><code>deregisterLogDirEventNotificationListener</code> is used when:</p> <ul> <li><code>KafkaController</code> is requested to resign as the active controller</li> </ul>","text":""},{"location":"controller/KafkaController/#unsubscribing-from-data-changes-to-adminpreferred_replica_election-znode","title":"Unsubscribing from Data Changes to /admin/preferred_replica_election ZNode <pre><code>deregisterPreferredReplicaElectionListener(): Unit\n</code></pre> <p><code>deregisterPreferredReplicaElectionListener</code> requests &lt;&gt; to link:kafka-ZkUtils.md#unsubscribeDataChanges[unsubscribe from intercepting data changes] to <code>/admin/preferred_replica_election</code> znode with &lt;&gt;. <p><code>deregisterPreferredReplicaElectionListener</code> is used when:</p> <ul> <li><code>KafkaController</code> is requested to resign as the active controller</li> </ul>","text":""},{"location":"controller/KafkaController/#unsubscribing-from-data-changes-to-adminreassign_partitions-znode","title":"Unsubscribing from Data Changes to /admin/reassign_partitions ZNode <pre><code>deregisterPartitionReassignmentListener(): Unit\n</code></pre> <p><code>deregisterPartitionReassignmentListener</code> requests &lt;&gt; to link:kafka-ZkUtils.md#unsubscribeDataChanges[unsubscribe from intercepting data changes] to <code>/admin/reassign_partitions</code> znode with &lt;&gt;. <p><code>deregisterPartitionReassignmentListener</code> is used when:</p> <ul> <li><code>KafkaController</code> is requested to resign as the active controller</li> </ul>","text":""},{"location":"controller/KafkaController/#sendupdatemetadatarequest","title":"sendUpdateMetadataRequest <pre><code>sendUpdateMetadataRequest(): Unit\n</code></pre> <p><code>sendUpdateMetadataRequest</code> requests the &lt;&gt; to &lt;&gt; and &lt;&gt;. <p>In the end, <code>sendUpdateMetadataRequest</code> requests the &lt;&gt; to &lt;&gt; with the current epoch. <p>In case of <code>IllegalStateException</code>, <code>sendUpdateMetadataRequest</code> &lt;&gt; (that &lt;&gt;).  <p><code>sendUpdateMetadataRequest</code> is used when:</p> <ul> <li> <p><code>KafkaController</code> is requested to &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, process a &lt;&gt; controller event  <li> <p><code>TopicDeletionManager</code> is requested to &lt;&gt;","text":""},{"location":"controller/KafkaController/#updateleaderepochandsendrequest","title":"updateLeaderEpochAndSendRequest <pre><code>updateLeaderEpochAndSendRequest(\n  partition: TopicPartition,\n  replicasToReceiveRequest: Seq[Int],\n  newAssignedReplicas: Seq[Int]): Unit\n</code></pre> <p>[[updateLeaderEpochAndSendRequest-updateLeaderEpoch]] <code>updateLeaderEpochAndSendRequest</code> &lt;&gt; and branches off per result: a &lt;&gt; or &lt;&gt;. <p>NOTE: <code>updateLeaderEpochAndSendRequest</code> is used when <code>KafkaController</code> is requested to &lt;&gt; and &lt;&gt;.","text":""},{"location":"controller/KafkaController/#leaderisrandcontrollerepoch","title":"LeaderIsrAndControllerEpoch <p>When &lt;&gt; returns a <code>LeaderIsrAndControllerEpoch</code>, <code>updateLeaderEpochAndSendRequest</code> requests the &lt;&gt; to &lt;&gt;. <code>updateLeaderEpochAndSendRequest</code> requests the &lt;&gt; to &lt;&gt; followed by &lt;&gt;. <p>In the end, <code>updateLeaderEpochAndSendRequest</code> prints out the following TRACE message to the logs:</p> <pre><code>Sent LeaderAndIsr request [updatedLeaderIsrAndControllerEpoch] with new assigned replica list [newAssignedReplicas] to leader [leader] for partition being reassigned [partition]\n</code></pre>","text":""},{"location":"controller/KafkaController/#no-leaderisrandcontrollerepoch","title":"No LeaderIsrAndControllerEpoch <p>When &lt;&gt; returns <code>None</code>, <code>updateLeaderEpochAndSendRequest</code> prints out the following ERROR message to the logs: <pre><code>Failed to send LeaderAndIsr request with new assigned replica list [newAssignedReplicas] to leader for partition being reassigned [partition]\n</code></pre> <p>=== [[elect]] Controller Election</p> <pre><code>elect(): Unit\n</code></pre> <p><code>elect</code> requests the &lt;&gt; for the &lt;&gt; (or assumes <code>-1</code> if not available) and saves it to the &lt;&gt; internal registry. <p><code>elect</code> stops the controller election if there is an active controller ID available and prints out the following DEBUG message to the logs:</p> <pre><code>Broker [activeControllerId] has been elected as the controller, so stopping the election process.\n</code></pre> <p>[[elect-registerControllerAndIncrementControllerEpoch]] Otherwise, with no active controller, <code>elect</code> requests the &lt;&gt; to &lt;&gt; (with the &lt;&gt;). <p><code>elect</code> saves the controller epoch and the zookeeper epoch as the &lt;&gt; and &lt;&gt; of the &lt;&gt;, respectively. <p><code>elect</code> saves the &lt;&gt; as the &lt;&gt; internal registry. <p><code>elect</code> prints out the following INFO message to the logs:</p> <pre><code>[brokerId] successfully elected as the controller. Epoch incremented to [epoch] and epoch zk version is now [epochZkVersion]\n</code></pre> <p>In the end, <code>elect</code> &lt;&gt;. <p>NOTE: <code>elect</code> is used when <code>ControllerEventThread</code> is requested to process &lt;&gt; and &lt;&gt; controller events. <p>==== [[elect-ControllerMovedException]] <code>elect</code> and ControllerMovedException</p> <p>In case of a <code>ControllerMovedException</code>, <code>elect</code> &lt;&gt; and prints out either DEBUG or WARN message to the logs per the &lt;&gt; internal registry: <pre><code>Broker [activeControllerId] was elected as controller instead of broker [brokerId]\n</code></pre> <pre><code>A controller has been elected but just resigned, this will result in another round of election\n</code></pre> <p>=== [[isActive]] Is Broker The Active Controller?</p> <pre><code>isActive: Boolean\n</code></pre> <p><code>isActive</code> indicates whether the current broker (by the broker ID) hosts the active <code>KafkaController</code> (given the &lt;&gt;) or not. <p>NOTE: <code>isActive</code> is on (<code>true</code>) after the <code>KafkaController</code> of a Kafka broker has been &lt;&gt;.","text":""},{"location":"controller/KafkaController/#note","title":"[NOTE]","text":"<p><code>isActive</code> is used (as a valve to stop processing early) when:</p> <ul> <li> <p><code>ControllerEventThread</code> is requested to &lt;&gt; (that should only be processed on the active controller, e.g. <code>AutoPreferredReplicaLeaderElection</code>, <code>UncleanLeaderElectionEnable</code>, <code>ControlledShutdown</code>, <code>LeaderAndIsrResponseReceived</code>, <code>TopicDeletionStopReplicaResponseReceived</code>, <code>BrokerChange</code>, <code>BrokerModifications</code>, <code>TopicChange</code>) <li> <p><code>KafkaController</code> is requested to &lt;&gt;"},{"location":"controller/KafkaController/#kafkaapis-is-requested-to-and","title":"* <code>KafkaApis</code> is requested to &lt;&gt;, &lt;&gt; and &lt;&gt; <p>=== [[startup]] Starting Up</p>","text":""},{"location":"controller/KafkaController/#source-scala","title":"[source, scala]","text":""},{"location":"controller/KafkaController/#startup-unit","title":"startup(): Unit <p><code>startup</code> requests the &lt;&gt; to &lt;&gt; (under the name controller-state-change-handler) that is does the following: <ul> <li> <p>On <code>afterInitializingSession</code>, the <code>StateChangeHandler</code> simply puts <code>RegisterBrokerAndReelect</code> event on the event queue of the &lt;&gt;  <li> <p>On <code>beforeInitializingSession</code>, the <code>StateChangeHandler</code> simply puts <code>Expire</code> event on the event queue of the &lt;&gt;   <p><code>startup</code> then puts <code>Startup</code> event at the end of the event queue of the &lt;&gt; and immediately requests it to &lt;&gt;. <p>NOTE: <code>startup</code> is used exclusively when <code>KafkaServer</code> is requested to &lt;&gt;. <p>=== [[registerSessionExpirationListener]] Registering SessionExpirationListener To Control Session Recreation -- <code>registerSessionExpirationListener</code> Internal Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_1","title":"[source, scala]","text":""},{"location":"controller/KafkaController/#registersessionexpirationlistener-unit","title":"registerSessionExpirationListener(): Unit <p><code>registerSessionExpirationListener</code> requests &lt;&gt; to link:kafka-ZkUtils.md#subscribeStateChanges[subscribe to state changes] with a <code>SessionExpirationListener</code> (with the <code>KafkaController</code> and &lt;&gt;). <p>NOTE: <code>SessionExpirationListener</code> puts &lt;&gt; event on the link:kafka-controller-ControllerEventManager.md#queue[event queue] of <code>ControllerEventManager</code> every time the Zookeeper session has expired and a new session has been created. <p>NOTE: <code>registerSessionExpirationListener</code> is used exclusively when &lt;&gt; event is processed (after <code>ControllerEventThread</code> is link:kafka-controller-ControllerEventThread.md#doWork[started]). <p>=== [[registerControllerChangeListener]] Registering ControllerChangeListener for /controller ZNode Changes -- <code>registerControllerChangeListener</code> Internal Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_2","title":"[source, scala]","text":""},{"location":"controller/KafkaController/#registercontrollerchangelistener-unit","title":"registerControllerChangeListener(): Unit <p><code>registerControllerChangeListener</code> requests &lt;&gt; to link:kafka-ZkUtils.md#subscribeDataChanges[subscribe to data changes] for <code>/controller</code> znode with a <code>ControllerChangeListener</code> (with the <code>KafkaController</code> and &lt;&gt;).","text":""},{"location":"controller/KafkaController/#note_1","title":"[NOTE] <p><code>ControllerChangeListener</code> emits:</p> <ol> <li>&lt;&gt; event with the current controller ID (on the link:kafka-controller-ControllerEventManager.md#queue[event queue] of <code>ControllerEventManager</code>) every time the data of a znode changes","text":""},{"location":"controller/KafkaController/#1-event-when-the-data-associated-with-a-znode-has-been-deleted","title":"1. &lt;&gt; event when the data associated with a znode has been deleted <p>NOTE: <code>registerControllerChangeListener</code> is used exclusively when &lt;&gt; event is processed (after <code>ControllerEventThread</code> is link:kafka-controller-ControllerEventThread.md#doWork[started]). <p>=== [[registerBrokerChangeListener]] <code>registerBrokerChangeListener</code> Internal Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_3","title":"[source, scala]","text":""},{"location":"controller/KafkaController/#registerbrokerchangelistener-optionseqstring","title":"registerBrokerChangeListener(): Option[Seq[String]] <p><code>registerBrokerChangeListener</code> requests &lt;&gt; to link:kafka-ZkUtils.md#subscribeChildChanges[subscribeChildChanges] for <code>/brokers/ids</code> path with &lt;&gt;. <p>NOTE: <code>registerBrokerChangeListener</code> is used exclusively when <code>KafkaController</code> does &lt;&gt;. <p>=== [[getControllerID]] Getting Active Controller ID (from JSON under /controller znode) -- <code>getControllerID</code> Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_4","title":"[source, scala]","text":""},{"location":"controller/KafkaController/#getcontrollerid-int","title":"getControllerID(): Int <p><code>getControllerID</code> returns the ID of the active Kafka controller that is associated with <code>/controller</code> znode in JSON format or <code>-1</code> otherwise.</p> <p>Internally, <code>getControllerID</code> requests &lt;&gt; for link:kafka-ZkUtils.md#readDataMaybeNull[data associated with <code>/controller</code> znode]. <p>If available, <code>getControllerID</code> parses the data (being the current controller info in JSON format) to extract <code>brokerid</code> field.</p> <pre><code>$ ./bin/zookeeper-shell.sh :2181 get /controller\n\n{\"version\":1,\"brokerid\":0,\"timestamp\":\"1543499076007\"}\ncZxid = 0x60\nctime = Thu Nov 29 14:44:36 CET 2018\nmZxid = 0x60\nmtime = Thu Nov 29 14:44:36 CET 2018\npZxid = 0x60\ncversion = 0\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0x100073f07ba0003\ndataLength = 54\nnumChildren = 0\n</code></pre> <p>Otherwise, when no <code>/controller</code> znode is available, <code>getControllerID</code> returns <code>-1</code>.</p>","text":""},{"location":"controller/KafkaController/#note_2","title":"[NOTE] <p><code>getControllerID</code> is used when:</p> <ol> <li>Processing <code>Reelect</code> controller event</li> </ol>","text":""},{"location":"controller/KafkaController/#1","title":"1. &lt;&gt; <p>=== [[registerTopicDeletionListener]] Registering TopicDeletionListener for Child Changes to /admin/delete_topics ZNode -- <code>registerTopicDeletionListener</code> Internal Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_5","title":"[source, scala]","text":""},{"location":"controller/KafkaController/#registertopicdeletionlistener-optionseqstring","title":"registerTopicDeletionListener(): Option[Seq[String]] <p><code>registerTopicDeletionListener</code> requests &lt;&gt; to link:kafka-ZkUtils.md#subscribeChildChanges[subscribeChildChanges] to <code>/admin/delete_topics</code> znode with &lt;&gt;. <p>NOTE: <code>registerTopicDeletionListener</code> is used exclusively when <code>KafkaController</code> does &lt;&gt;. <p>=== [[deregisterTopicDeletionListener]] De-Registering TopicDeletionListener for Child Changes to /admin/delete_topics ZNode -- <code>deregisterTopicDeletionListener</code> Internal Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_6","title":"[source, scala]","text":""},{"location":"controller/KafkaController/#deregistertopicdeletionlistener-unit","title":"deregisterTopicDeletionListener(): Unit <p><code>deregisterTopicDeletionListener</code> requests &lt;&gt; to link:kafka-ZkUtils.md#unsubscribeChildChanges[unsubscribeChildChanges] to <code>/admin/delete_topics</code> znode with &lt;&gt;. <p>NOTE: <code>deregisterTopicDeletionListener</code> is used exclusively when <code>KafkaController</code> &lt;&gt;. <p>=== [[initializeControllerContext]] Initializing ControllerContext -- <code>initializeControllerContext</code> Internal Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_7","title":"[source, scala]","text":""},{"location":"controller/KafkaController/#initializecontrollercontext-unit","title":"initializeControllerContext(): Unit <p><code>initializeControllerContext</code>...FIXME</p> <p>In the end, <code>initializeControllerContext</code> prints out the following INFO messages to the logs (with the current state based on the &lt;&gt;):","text":""},{"location":"controller/KafkaController/#optionswrap","title":"[options=\"wrap\"] <p>Currently active brokers in the cluster: [liveBrokerIds] Currently shutting brokers in the cluster: [shuttingDownBrokerIds] Current list of topics in the cluster: [allTopics]</p>  <p>NOTE: <code>initializeControllerContext</code> is used exclusively when <code>KafkaController</code> is requested to &lt;&gt;. <p>=== [[updateLeaderAndIsrCache]] <code>updateLeaderAndIsrCache</code> Internal Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_8","title":"[source, scala]","text":""},{"location":"controller/KafkaController/#updateleaderandisrcachepartitions-seqtopicpartition","title":"updateLeaderAndIsrCache(partitions: Seq[TopicPartition] <p>Unless given, <code>updateLeaderAndIsrCache</code> defaults to &lt;&gt; of the &lt;&gt; for the partitions. <p><code>updateLeaderAndIsrCache</code> requests the &lt;&gt; to &lt;&gt; (with the given partitions) and updates the &lt;&gt; of the &lt;&gt;. <p>NOTE: <code>updateLeaderAndIsrCache</code> is used when <code>KafkaController</code> is requested to &lt;&gt; (with no partitions) and &lt;&gt; (with partitions given). <p>=== [[onPreferredReplicaElection]] Preferred Replica Leader Election -- <code>onPreferredReplicaElection</code> Internal Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_9","title":"[source, scala] <p>onPreferredReplicaElection(   partitions: Set[TopicPartition],   electionType: ElectionType): Map[TopicPartition, Throwable]</p>  <p><code>onPreferredReplicaElection</code> prints out the following INFO message to the logs:</p> <pre><code>Starting preferred replica leader election for partitions [partitions]\n</code></pre> <p><code>onPreferredReplicaElection</code> requests the &lt;&gt; to &lt;&gt; for the partitions (with <code>OnlinePartition</code> target state and &lt;&gt;). <p>(only for &lt;&gt; that are not &lt;&gt;) In the end, <code>onPreferredReplicaElection</code> &lt;&gt;. <p>(only for &lt;&gt; that are not &lt;&gt;) In case of an error","text":""},{"location":"controller/KafkaController/#note_3","title":"[NOTE] <p><code>onPreferredReplicaElection</code> is used when <code>KafkaController</code> is requested for the following:</p> <ul> <li> <p>&lt;&gt; (with &lt;&gt; election type)  <li> <p>&lt;&gt; (with &lt;&gt; election type)","text":""},{"location":"controller/KafkaController/#any-election-type-with-the-default","title":"* &lt;&gt; (any election type with &lt;&gt; the default) <p>=== [[onControllerFailover]] <code>onControllerFailover</code> Internal Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_10","title":"[source, scala]","text":""},{"location":"controller/KafkaController/#oncontrollerfailover-unit","title":"onControllerFailover(): Unit <p><code>onControllerFailover</code> prints out the following INFO message to the logs:</p> <pre><code>Registering handlers\n</code></pre> <p><code>onControllerFailover</code> requests the &lt;&gt; to &lt;&gt;: <ul> <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt;  <p><code>onControllerFailover</code> requests the &lt;&gt; to &lt;&gt;: <ul> <li>&lt;&gt; <li>&lt;&gt;  <p><code>onControllerFailover</code> prints out the following INFO message to the logs:</p> <pre><code>Deleting log dir event notifications\n</code></pre> <p><code>onControllerFailover</code> requests the &lt;&gt; to &lt;&gt; (with the &lt;&gt; of the &lt;&gt;). <p><code>onControllerFailover</code> prints out the following INFO message to the logs:</p> <pre><code>Deleting isr change notifications\n</code></pre> <p><code>onControllerFailover</code> requests the &lt;&gt; to &lt;&gt; (with the &lt;&gt; of the &lt;&gt;). <p><code>onControllerFailover</code> prints out the following INFO message to the logs:</p> <pre><code>Initializing controller context\n</code></pre> <p><code>onControllerFailover</code> &lt;&gt;. <p><code>onControllerFailover</code> prints out the following INFO message to the logs:</p> <pre><code>Fetching topic deletions in progress\n</code></pre> <p><code>onControllerFailover</code> &lt;&gt;. <p><code>onControllerFailover</code> prints out the following INFO message to the logs:</p> <pre><code>Initializing topic deletion manager\n</code></pre> <p><code>onControllerFailover</code> requests the &lt;&gt; to &lt;&gt; (with the topics to be deleted and ineligible for deletion). <p><code>onControllerFailover</code> prints out the following INFO message to the logs:</p> <pre><code>Sending update metadata request\n</code></pre> <p><code>onControllerFailover</code> &lt;&gt; (with the &lt;&gt; of the &lt;&gt;). <p><code>onControllerFailover</code> requests the &lt;&gt; to &lt;&gt;. <p><code>onControllerFailover</code> requests the &lt;&gt; to &lt;&gt;. <p><code>onControllerFailover</code> prints out the following INFO message to the logs:</p> <pre><code>Ready to serve as the new controller with epoch [epoch]\n</code></pre> <p><code>onControllerFailover</code> &lt;&gt; (with the &lt;&gt; of the &lt;&gt;). <p><code>onControllerFailover</code> requests the &lt;&gt; to &lt;&gt;. <p><code>onControllerFailover</code> &lt;&gt; with the &lt;&gt;. <p><code>onControllerFailover</code> prints out the following INFO message to the logs:</p> <pre><code>Starting the controller scheduler\n</code></pre> <p><code>onControllerFailover</code> requests the &lt;&gt; to &lt;&gt;. <p>With &lt;&gt; enabled (default: <code>true</code>), <code>onControllerFailover</code> &lt;&gt; with the initial delay of 5 seconds. <p>With &lt;&gt; password set (default: <code>(empty)</code>), <code>onControllerFailover</code> prints out the following INFO message to the logs: <pre><code>starting the token expiry check scheduler\n</code></pre> <p><code>onControllerFailover</code> requests the &lt;&gt; to &lt;&gt; and requests it to &lt;&gt; the delete-expired-tokens task (FIXME). <p>NOTE: <code>onControllerFailover</code> is used when <code>KafkaController</code> is requested to &lt;&gt; (and a broker is successfully elected as the active controller). <p>=== [[scheduleAutoLeaderRebalanceTask]] <code>scheduleAutoLeaderRebalanceTask</code> Internal Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_11","title":"[source, scala] <p>scheduleAutoLeaderRebalanceTask(   delay: Long,   unit: TimeUnit): Unit</p>  <p><code>scheduleAutoLeaderRebalanceTask</code> simply requests the &lt;&gt; to &lt;&gt; called auto-leader-rebalance-task with the given initial delay. <p>The <code>auto-leader-rebalance-task</code> simply requests the &lt;&gt; to &lt;&gt; an &lt;&gt; controller event. <p>NOTE: <code>scheduleAutoLeaderRebalanceTask</code> is used when <code>KafkaController</code> is requested to &lt;&gt; and &lt;&gt; <p>=== [[processAutoPreferredReplicaLeaderElection]] <code>processAutoPreferredReplicaLeaderElection</code> Internal Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_12","title":"[source, scala]","text":""},{"location":"controller/KafkaController/#processautopreferredreplicaleaderelection-unit","title":"processAutoPreferredReplicaLeaderElection(): Unit <p>NOTE: <code>processAutoPreferredReplicaLeaderElection</code> does nothing (and simply returns) unless the Kafka broker (<code>KafkaController</code>) is an &lt;&gt;. <p><code>processAutoPreferredReplicaLeaderElection</code> prints out the following INFO message to the logs:</p> <pre><code>Processing automatic preferred replica leader election\n</code></pre> <p><code>processAutoPreferredReplicaLeaderElection</code> &lt;&gt;. <p>In the end, <code>processAutoPreferredReplicaLeaderElection</code> &lt;&gt; with the initial delay based on &lt;&gt; configuration property (default: <code>300</code> seconds). <p>NOTE: <code>processAutoPreferredReplicaLeaderElection</code> is used exclusively when <code>KafkaController</code> is requested to &lt;&gt; a &lt;&gt; event. <p>=== [[checkAndTriggerAutoLeaderRebalance]] <code>checkAndTriggerAutoLeaderRebalance</code> Internal Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_13","title":"[source, scala]","text":""},{"location":"controller/KafkaController/#checkandtriggerautoleaderrebalance-unit","title":"checkAndTriggerAutoLeaderRebalance(): Unit <p><code>checkAndTriggerAutoLeaderRebalance</code> prints out the following TRACE message to the logs:</p> <pre><code>Checking need to trigger auto leader balancing\n</code></pre> <p>[[checkAndTriggerAutoLeaderRebalance-preferredReplicasForTopicsByBrokers]] <code>checkAndTriggerAutoLeaderRebalance</code>...FIXME</p> <p><code>checkAndTriggerAutoLeaderRebalance</code> prints out the following DEBUG message to the logs:</p> <pre><code>Preferred replicas by broker [preferredReplicasForTopicsByBrokers]\n</code></pre> <p>For every broker with one or more partition leaders, <code>checkAndTriggerAutoLeaderRebalance</code>...FIXME</p> <p><code>checkAndTriggerAutoLeaderRebalance</code> prints out the following DEBUG message to the logs:</p> <pre><code>Topics not in preferred replica for broker [leaderBroker] [topicsNotInPreferredReplica]\n</code></pre> <p>[[checkAndTriggerAutoLeaderRebalance-imbalanceRatio]] <code>checkAndTriggerAutoLeaderRebalance</code> calculates an imbalance ratio of the broker which is the number of <code>topicsNotInPreferredReplica</code> divided by the total number of partitions (<code>topicPartitionsForBroker</code>).</p> <p><code>checkAndTriggerAutoLeaderRebalance</code> prints out the following TRACE message to the logs:</p> <pre><code>Leader imbalance ratio for broker [leaderBroker] is [imbalanceRatio]\n</code></pre> <p>[[checkAndTriggerAutoLeaderRebalance-candidatePartitions]] With the imbalance ratio greater than the desired ratio (per &lt;&gt; configuration property with the default: <code>10%</code>), <code>checkAndTriggerAutoLeaderRebalance</code> &lt;&gt; for...FIXME (with &lt;&gt; election type). <p>NOTE: <code>checkAndTriggerAutoLeaderRebalance</code> is used exclusively when <code>KafkaController</code> is requested to &lt;&gt;. <p>=== [[onBrokerLogDirFailure]] Handling Log Directory Failures for Brokers -- <code>onBrokerLogDirFailure</code> Internal Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_14","title":"[source, scala] <p>onBrokerLogDirFailure(   brokerIds: Seq[Int]): Unit</p>  <p><code>onBrokerLogDirFailure</code> prints out the following INFO message to the logs:</p> <pre><code>Handling log directory failure for brokers [brokerIds]\n</code></pre> <p><code>onBrokerLogDirFailure</code> requests the &lt;&gt; for the &lt;&gt; and then requests the &lt;&gt; to &lt;&gt; for the replicas to enter <code>OnlineReplica</code> state. <p>NOTE: <code>onBrokerLogDirFailure</code> is used exclusively when <code>KafkaController</code> is requested to &lt;&gt;. <p>=== [[enableDefaultUncleanLeaderElection]] <code>enableDefaultUncleanLeaderElection</code> Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_15","title":"[source, scala]","text":""},{"location":"controller/KafkaController/#enabledefaultuncleanleaderelection-unit","title":"enableDefaultUncleanLeaderElection(): Unit <p>NOTE: <code>enableDefaultUncleanLeaderElection</code> does nothing (and simply returns) unless the Kafka broker (<code>KafkaController</code>) is an &lt;&gt;. <p><code>enableDefaultUncleanLeaderElection</code> simply requests the &lt;&gt; to link:kafka-controller-ControllerEventManager.md#put[enqueue] a link:kafka-controller-ControllerEvent-UncleanLeaderElectionEnable.md[UncleanLeaderElectionEnable] event. <p>NOTE: <code>enableDefaultUncleanLeaderElection</code> is used when <code>DynamicLogConfig</code> is requested to link:kafka-server-DynamicLogConfig.md#reconfigure[reconfigure] (for link:kafka-log-LogConfig.md#unclean.leader.election.enable[unclean.leader.election.enable] configuration property).</p> <p>=== [[electPreferredLeaders]] Preferred Replica Leader Election -- <code>electPreferredLeaders</code> Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_16","title":"[source, scala] <p>electPreferredLeaders(   partitions: Set[TopicPartition],   callback: ElectPreferredLeadersCallback = { (_, _) =&gt; }): Unit</p>  <p><code>electPreferredLeaders</code> simply requests the &lt;&gt; to &lt;&gt; an &lt;&gt; event (with &lt;&gt; election type) <p>NOTE: <code>electPreferredLeaders</code> is used exclusively when <code>ReplicaManager</code> is requested to &lt;&gt;. <p>=== [[processUncleanLeaderElectionEnable]] <code>processUncleanLeaderElectionEnable</code> Internal Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_17","title":"[source, scala]","text":""},{"location":"controller/KafkaController/#processuncleanleaderelectionenable-unit","title":"processUncleanLeaderElectionEnable(): Unit <p>NOTE: <code>processUncleanLeaderElectionEnable</code> does nothing (and simply returns) unless the Kafka broker (<code>KafkaController</code>) is an &lt;&gt;. <p><code>processUncleanLeaderElectionEnable</code> prints out the following INFO message to the logs:</p> <pre><code>Unclean leader election has been enabled by default\n</code></pre> <p><code>processUncleanLeaderElectionEnable</code> requests the &lt;&gt; to link:kafka-controller-PartitionStateMachine.md#triggerOnlinePartitionStateChange[triggerOnlinePartitionStateChange]. <p>NOTE: <code>processUncleanLeaderElectionEnable</code> is used exclusively when <code>KafkaController</code> is requested to &lt;&gt;. <p>=== [[processBrokerChange]] Processing BrokerChange Controller Event (On controller-event-thread) -- <code>processBrokerChange</code> Internal Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_18","title":"[source, scala]","text":""},{"location":"controller/KafkaController/#processbrokerchange-unit","title":"processBrokerChange(): Unit <p>NOTE: <code>processBrokerChange</code> does nothing (and simply returns) unless the Kafka broker (<code>KafkaController</code>) is an &lt;&gt;. <p><code>processBrokerChange</code> requests the &lt;&gt; for the &lt;&gt; and compares the broker list with the &lt;&gt; (of the &lt;&gt;). <p>At this point in time, <code>processBrokerChange</code> knows what brokers are new, dead or bounced.</p> <p><code>processBrokerChange</code> prints out the following INFO message to the logs:</p>","text":""},{"location":"controller/KafkaController/#optionswrap_1","title":"[options=\"wrap\"]","text":""},{"location":"controller/KafkaController/#newly-added-brokers-ids-deleted-brokers-ids-bounced-brokers-ids-all-live-brokers-ids","title":"Newly added brokers: [ids], deleted brokers: [ids], bounced brokers: [ids], all live brokers: [ids] <p><code>processBrokerChange</code> notifies (updates) the &lt;&gt;: <ul> <li> <p>For every newly-added broker, <code>processBrokerChange</code> requests to &lt;&gt;  <li> <p>For bounced brokers, <code>processBrokerChange</code> requests to &lt;&gt; first followed by &lt;&gt;  <li> <p>For every deleted broker, <code>processBrokerChange</code> requests to &lt;&gt;   <p><code>processBrokerChange</code> updates the &lt;&gt;: <ul> <li> <p>For newly-added brokers (if there were any), <code>processBrokerChange</code> requests to &lt;&gt; followed by &lt;&gt;  <li> <p>For bounced brokers (if there were any), <code>processBrokerChange</code> first requests to &lt;&gt; followed by &lt;&gt; and then requests to &lt;&gt; followed by &lt;&gt;  <li> <p>For deleted brokers (if there were any), <code>processBrokerChange</code> requests to &lt;&gt; followed by &lt;&gt;   <p>In the end, only when they were any updates (new, dead or bounced brokers), <code>processBrokerChange</code> prints out the following INFO message to the logs:</p> <pre><code>Updated broker epochs cache: [liveBrokerIdAndEpochs]\n</code></pre> <p>NOTE: <code>processBrokerChange</code> is used exclusively when <code>KafkaController</code> is requested to &lt;&gt; (on the &lt;&gt;). <p>=== [[processLogDirEventNotification]] Processing LogDirEventNotification Controller Event (On controller-event-thread) -- <code>processLogDirEventNotification</code> Internal Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_19","title":"[source, scala]","text":""},{"location":"controller/KafkaController/#processlogdireventnotification-unit","title":"processLogDirEventNotification(): Unit <p>NOTE: <code>processLogDirEventNotification</code> does nothing (and simply returns) unless the Kafka broker (<code>KafkaController</code>) is an &lt;&gt;. <p><code>processLogDirEventNotification</code> requests the &lt;&gt; for the &lt;&gt; (sequence numbers). <p><code>processLogDirEventNotification</code> requests the &lt;&gt; for the &lt;&gt; and then &lt;&gt;. <p>In the end, <code>processLogDirEventNotification</code> requests the &lt;&gt; to &lt;&gt;. <p>NOTE: <code>processLogDirEventNotification</code> is used exclusively when <code>KafkaController</code> is requested to &lt;&gt;. <p>=== [[processStartup]] <code>processStartup</code> Internal Method</p>","text":""},{"location":"controller/KafkaController/#source-scala_20","title":"[source, scala]","text":""},{"location":"controller/KafkaController/#processstartup-unit","title":"processStartup(): Unit <p><code>processStartup</code> requests the &lt;&gt; to &lt;&gt; (with the &lt;&gt;). <p>In the end, <code>processStartup</code> starts &lt;&gt;. <p>NOTE: <code>processStartup</code> is used exclusively when <code>KafkaController</code> is requested to &lt;&gt; (on the &lt;&gt;). <p>=== [[internal-properties]] Internal Properties</p> <p>| activeControllerId a| [[activeControllerId]] The ID of the active <code>KafkaController</code></p> <ul> <li>Initialized to <code>-1</code></li> </ul> <p>| brokerRequestBatch a| [[brokerRequestBatch]] &lt;&gt; (with the &lt;&gt;, &lt;&gt;, and &lt;&gt;) <p>| controllerChangeHandler a| [[controllerChangeHandler]] A <code>ZNodeChangeHandler</code> (for the <code>KafkaController</code> and the &lt;&gt;) that listens to change events on <code>/controller</code> znode. <p><code>controllerChangeHandler</code> &lt;&gt; as follows: <ul> <li> <p><code>ControllerChange</code> when the znode is created or the znode data changed</p> </li> <li> <p><code>Reelect</code> when the znode is deleted</p> </li> </ul> <p>| eventManager a| [[eventManager]] &lt;&gt; (with &lt;&gt; of the &lt;&gt;, the &lt;&gt; as the &lt;&gt;` and the &lt;&gt; as the &lt;&gt;) <p><code>eventManager</code> is used to create other internal components to allow them for emitting controller events at state changes:</p> <ul> <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt;  <p><code>eventManager</code> is &lt;&gt; when <code>KafkaController</code> is requested to &lt;&gt;. <p><code>eventManager</code> is &lt;&gt; when <code>KafkaController</code> is requested to &lt;&gt;. <p>| kafkaScheduler | [[kafkaScheduler]] &lt;&gt; with 1 daemon thread with kafka-scheduler prefix <p>| stateChangeLogger | [[stateChangeLogger]] link:kafka-controller-StateChangeLogger.md[StateChangeLogger] with the &lt;&gt; and <code>inControllerContext</code> flag enabled <p>| tokenCleanScheduler | [[tokenCleanScheduler]] &lt;&gt; with 1 daemon thread with delegation-token-cleaner prefix <p>| topicDeletionManager | [[topicDeletionManager]] &lt;&gt; |===","text":""},{"location":"controller/LeaderAndIsrRequest/","title":"LeaderAndIsrRequest","text":"<p><code>LeaderAndIsrRequest</code> is a controller request with <code>LeaderAndIsr</code> API key and the following properties:</p> <ul> <li> Version <li> Controller ID (broker.id of the controller broker) <li> Controller Epoch <li> Broker Epoch <li> <code>PartitionStates</code> by <code>TopicPartition</code> (<code>Map&lt;TopicPartition, PartitionState&gt;</code>) <li> Topic IDs <li> Live leaders <p><code>LeaderAndIsrRequest</code> is created (using LeaderAndIsrRequest.Builder) when:</p> <ul> <li><code>AbstractRequest</code> is requested to parse a request (with the LeaderAndIsr API key)</li> <li><code>LeaderAndIsrRequest</code> is requested to parse a byte buffer</li> <li><code>LeaderAndIsrRequest.Builder</code> is requested to build a LeaderAndIsrRequest (when <code>ControllerBrokerRequestBatch</code> is requested to sendRequestsToBrokers)</li> </ul>"},{"location":"controller/LeaderAndIsrRequest/#leaderandisrrequestbuilder","title":"LeaderAndIsrRequest.Builder <p><code>LeaderAndIsrRequest</code> comes with a concrete AbstractRequest.Builder factory object that can build a <code>LeaderAndIsrRequest</code>.</p> <p><code>LeaderAndIsrRequest.Builder</code> is used when:</p> <ul> <li><code>AbstractControllerBrokerRequestBatch</code> is requested to send out LeaderAndIsr requests to leader and follower brokers</li> </ul>","text":""},{"location":"controller/PartitionLeaderElectionAlgorithms/","title":"PartitionLeaderElectionAlgorithms","text":"<p><code>PartitionLeaderElectionAlgorithms</code> is a utility with the algorithms for partition leader election.</p>"},{"location":"controller/PartitionLeaderElectionAlgorithms/#offlinepartitionleaderelection","title":"offlinePartitionLeaderElection <pre><code>offlinePartitionLeaderElection(\n  assignment: Seq[Int],\n  isr: Seq[Int],\n  liveReplicas: Set[Int],\n  uncleanLeaderElectionEnabled: Boolean,\n  controllerContext: ControllerContext): Option[Int]\n</code></pre> <p><code>offlinePartitionLeaderElection</code> finds the first broker ID (among the <code>liveReplicas</code>) that is among the <code>isr</code>.</p> <p>If not found and <code>uncleanLeaderElectionEnabled</code> flag is enabled, <code>offlinePartitionLeaderElection</code> finds the first live replica broker (from the <code>assignment</code> that is among <code>liveReplicas</code>). When successful and a live replica broker is found, <code>offlinePartitionLeaderElection</code> marks the occurrence of this unclean leader election event in kafka.controller:type=ControllerStats,name=UncleanLeaderElectionsPerSec metric.</p> <p>In the end, <code>offlinePartitionLeaderElection</code> returns a broker ID (if a new partition leader is found) or <code>None</code>.</p>  <p><code>offlinePartitionLeaderElection</code> is used when:</p> <ul> <li><code>Election</code> is requested to leaderForOffline</li> </ul>","text":""},{"location":"controller/PartitionStateMachine/","title":"PartitionStateMachine","text":"<p><code>PartitionStateMachine</code> is an abstraction of partition state machines that can handleStateChanges.</p>"},{"location":"controller/PartitionStateMachine/#contract","title":"Contract","text":""},{"location":"controller/PartitionStateMachine/#handlestatechanges","title":"handleStateChanges <pre><code>handleStateChanges(\n  partitions: Seq[TopicPartition],\n  targetState: PartitionState\n): Map[TopicPartition, Either[Throwable, LeaderAndIsr]] // (1)!\nhandleStateChanges(\n  partitions: Seq[TopicPartition],\n  targetState: PartitionState,\n  leaderElectionStrategy: Option[PartitionLeaderElectionStrategy]\n): Map[TopicPartition, Either[Throwable, LeaderAndIsr]]\n</code></pre> <ol> <li>Uses an undefined <code>leaderElectionStrategy</code> (<code>None</code>)</li> </ol> <p>Handles state changes of partitions (partition state changes)</p> <p>Used when:</p> <ul> <li><code>KafkaController</code> is requested to onNewPartitionCreation, onReplicasBecomeOffline, onReplicaElection, moveReassignedPartitionLeaderIfRequired, doControlledShutdown</li> <li><code>PartitionStateMachine</code> is requested to triggerOnlineStateChangeForPartitions</li> <li><code>TopicDeletionManager</code> is requested to onTopicDeletion</li> </ul>","text":""},{"location":"controller/PartitionStateMachine/#implementations","title":"Implementations","text":"<ul> <li>ZkPartitionStateMachine</li> </ul>"},{"location":"controller/PartitionStateMachine/#creating-instance","title":"Creating Instance","text":"<p><code>PartitionStateMachine</code> takes the following to be created:</p> <ul> <li> ControllerContext <p>Abstract Class</p> <p><code>PartitionStateMachine</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete PartitionStateMachines.</p>"},{"location":"controller/PartitionStateMachine/#starting-up-on-active-controller","title":"Starting Up (on Active Controller) <pre><code>startup(): Unit\n</code></pre>  <p><code>startup</code> prints out the following INFO message to the logs:</p> <pre><code>Initializing partition state\n</code></pre> <p><code>startup</code> initializePartitionState.</p> <p><code>startup</code> prints out the following INFO message to the logs:</p> <pre><code>Triggering online partition state changes\n</code></pre> <p><code>startup</code> triggerOnlinePartitionStateChange.</p> <p>In the end, <code>startup</code> prints out the following DEBUG message to the logs:</p> <pre><code>Started partition state machine with initial state -&gt; [partitionStates]\n</code></pre>  <p><code>startup</code> is used when:</p> <ul> <li><code>KafkaController</code> is requested to onControllerFailover (when a broker is successfully elected as the controller)</li> </ul>","text":""},{"location":"controller/PartitionStateMachine/#initializepartitionstate","title":"initializePartitionState <pre><code>initializePartitionState(): Unit\n</code></pre>  <p><code>initializePartitionState</code>...FIXME</p>","text":""},{"location":"controller/PartitionStateMachine/#triggeronlinepartitionstatechange","title":"triggerOnlinePartitionStateChange <pre><code>triggerOnlinePartitionStateChange(): Map[TopicPartition, Either[Throwable, LeaderAndIsr]]\ntriggerOnlinePartitionStateChange(\n  topic: String): Unit // (1)!\n</code></pre> <ol> <li>Uses the partitions of the given topic only</li> </ol>  <p><code>triggerOnlinePartitionStateChange</code> requests the ControllerContext for all the partitions in the following states (possibly limited to the given topic):</p> <ul> <li><code>NewPartition</code></li> <li><code>OfflinePartition</code></li> </ul> <p>In the end, <code>triggerOnlinePartitionStateChange</code> triggers online state change for the partitions.</p>  <p><code>triggerOnlinePartitionStateChange</code> is used when:</p> <ul> <li><code>KafkaController</code> is requested to onBrokerStartup, onReplicasBecomeOffline, processUncleanLeaderElectionEnable, processTopicUncleanLeaderElectionEnable</li> <li><code>PartitionStateMachine</code> is requested to start up</li> </ul>","text":""},{"location":"controller/PartitionStateMachine/#triggeronlinestatechangeforpartitions","title":"triggerOnlineStateChangeForPartitions <pre><code>triggerOnlineStateChangeForPartitions(\n  partitions: Set[TopicPartition]\n): Map[TopicPartition, Either[Throwable, LeaderAndIsr]]\n</code></pre>  <p><code>triggerOnlineStateChangeForPartitions</code> filters out the partitions of the topics to be deleted from the given <code>partitions</code>.</p> <p><code>triggerOnlineStateChangeForPartitions</code> tries to move the partitions to <code>OnlinePartition</code> state with OfflinePartitionLeaderElectionStrategy (with <code>allowUnclean</code> flag off).</p>","text":""},{"location":"controller/PartitionStateMachine/#review-me","title":"Review Me <p>=== [[PartitionLeaderElectionStrategy]] PartitionLeaderElectionStrategy</p> <p>.PartitionLeaderElectionStrategies [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| ControlledShutdownPartitionLeaderElectionStrategy a| [[ControlledShutdownPartitionLeaderElectionStrategy]]</p> <p>| OfflinePartitionLeaderElectionStrategy a| [[OfflinePartitionLeaderElectionStrategy]] Accepts <code>allowUnclean</code> flag</p> <p>Handled by <code>ZkPartitionStateMachine</code> when requested to link:kafka-controller-ZkPartitionStateMachine.adoc#doElectLeaderForPartitions[doElectLeaderForPartitions]</p> <p>Used when:</p> <ul> <li> <p><code>KafkaController</code> is requested to link:kafka-controller-KafkaController.adoc#onNewPartitionCreation[onNewPartitionCreation] (with the <code>allowUnclean</code> flag off), link:kafka-controller-KafkaController.adoc#onReplicaElection[onReplicaElection] (with the <code>allowUnclean</code> flag on for the admin client)</p> </li> <li> <p><code>PartitionStateMachine</code> is requested to &lt;&gt; (with the <code>allowUnclean</code> flag off)   <p>| PreferredReplicaPartitionLeaderElectionStrategy a| [[PreferredReplicaPartitionLeaderElectionStrategy]] <code>KafkaController</code> is requested for &lt;&gt; that in turn triggers <code>ZkPartitionStateMachine</code> to &lt;&gt; <p>| ReassignPartitionLeaderElectionStrategy a| [[ReassignPartitionLeaderElectionStrategy]]</p> <p>|===</p> <p>=== [[shutdown]] Shutting Down -- <code>shutdown</code> Method</p>","text":""},{"location":"controller/PartitionStateMachine/#source-scala","title":"[source, scala]","text":""},{"location":"controller/PartitionStateMachine/#shutdown-unit","title":"shutdown(): Unit <p><code>shutdown</code> simply prints out the following INFO message to the logs:</p> <pre><code>Stopped partition state machine\n</code></pre> <p>NOTE: <code>shutdown</code> is used exclusively when is requested to &lt;&gt; <p>=== [[initializePartitionState]] <code>initializePartitionState</code> Internal Method</p>","text":""},{"location":"controller/PartitionStateMachine/#source-scala_1","title":"[source, scala]","text":""},{"location":"controller/PartitionStateMachine/#initializepartitionstate-unit","title":"initializePartitionState(): Unit <p><code>initializePartitionState</code> requests the &lt;&gt; for &lt;&gt; (across all the brokers in the Kafka cluster). <p>For every <code>TopicPartition</code>, <code>initializePartitionState</code> requests the &lt;&gt; for the <code>LeaderIsrAndControllerEpoch</code> metadata (using the &lt;&gt; internal registry). <p><code>initializePartitionState</code> &lt;&gt; of a <code>TopicPartition</code> as follows: <ul> <li> <p><code>OnlinePartition</code> when the &lt;&gt; says that the &lt;&gt; (for the leader ISR and the <code>TopicPartition</code>)  <li> <p><code>OfflinePartition</code> when the &lt;&gt; says that the &lt;&gt; (for the leader ISR and the <code>TopicPartition</code>)  <li> <p><code>NewPartition</code> when the &lt;&gt; has no metadata about the <code>TopicPartition</code>   <p>NOTE: <code>initializePartitionState</code> is used exclusively when <code>PartitionStateMachine</code> is requested to &lt;&gt;.","text":""},{"location":"controller/QueuedEvent/","title":"QueuedEvent","text":"<p><code>QueuedEvent</code> is a ControllerEvent with the time it was enqueued to ControllerEventManager.</p>"},{"location":"controller/QueuedEvent/#creating-instance","title":"Creating Instance","text":"<p><code>QueuedEvent</code> takes the following to be created:</p> <ul> <li> ControllerEvent <li> Enqueue time (in millis) <p><code>QueuedEvent</code> is created when:</p> <ul> <li><code>ControllerEventManager</code> is requested to enqueue a ControllerEvent</li> </ul>"},{"location":"controller/QueuedEvent/#processing-controllereventprocessor","title":"Processing ControllerEventProcessor <pre><code>process(\n  processor: ControllerEventProcessor): Unit\n</code></pre> <p><code>process</code> requests the input ControllerEventProcessor to process the ControllerEvent.</p>  <p><code>process</code> is used when:</p> <ul> <li><code>ControllerEventThread</code> is requested to doWork</li> </ul>","text":""},{"location":"controller/QueuedEvent/#string-textual-representation","title":"String (Textual) Representation <pre><code>toString: String\n</code></pre> <p><code>toString</code> is part of the java.lang.Object abstraction.</p>  <p><code>toString</code> returns the following string representation (with the ControllerEvent and the enqueue time):</p> <pre><code>QueuedEvent(event=[event], enqueueTimeMs=[enqueueTimeMs])\n</code></pre>","text":""},{"location":"controller/ReplicaStateMachine/","title":"ReplicaStateMachine","text":""},{"location":"controller/ReplicaStateMachine/#review-me","title":"Review Me","text":"<p><code>ReplicaStateMachine</code> is the &lt;&gt; of &lt;&gt; that can &lt;&gt;, be &lt;&gt; and &lt;&gt; in the end. <p>[[contract]] .ReplicaStateMachine Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| handleStateChanges a| [[handleStateChanges]]</p>"},{"location":"controller/ReplicaStateMachine/#source-scala","title":"[source, scala]","text":"<p>handleStateChanges(   replicas: Seq[PartitionAndReplica],   targetState: ReplicaState): Unit</p> <p>Handles state changes of replicas (replica state changes)</p> <p>Used when:</p> <ul> <li> <p><code>KafkaController</code> is requested to &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, and &lt;&gt; <li> <p><code>ReplicaStateMachine</code> is requested to &lt;&gt; (when <code>KafkaController</code> is requested to &lt;&gt; on a broker elected as the controller) <li> <p><code>TopicDeletionManager</code> is requested to &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, and &lt;&gt; <p>|===</p> <p>[[implementations]] NOTE: &lt;&gt; is the default and only known implementation of the &lt;&gt; in Apache Kafka. <p>[[creating-instance]][[controllerContext]] <code>ReplicaStateMachine</code> takes a single &lt;&gt; to be created. <p>NOTE: <code>ReplicaStateMachine</code> is a Scala abstract class and cannot be &lt;&gt; directly. It is created indirectly for the &lt;&gt;. <p>=== [[shutdown]] Shutting Down -- <code>shutdown</code> Method</p>"},{"location":"controller/ReplicaStateMachine/#source-scala_1","title":"[source, scala]","text":""},{"location":"controller/ReplicaStateMachine/#shutdown-unit","title":"shutdown(): Unit","text":"<p><code>shutdown</code> simply prints out the following INFO message to the logs:</p> <pre><code>Stopped replica state machine\n</code></pre> <p>NOTE: <code>shutdown</code> is used exclusively when <code>KafkaController</code> is requested to &lt;&gt;. <p>=== [[startup]] Starting Up (On Active Controller) -- <code>startup</code> Method</p>"},{"location":"controller/ReplicaStateMachine/#source-scala_2","title":"[source, scala]","text":""},{"location":"controller/ReplicaStateMachine/#startup-unit","title":"startup(): Unit","text":"<p><code>startup</code> prints out the following INFO message to the logs:</p> <pre><code>Initializing replica state\n</code></pre> <p><code>startup</code> &lt;&gt;. <p><code>startup</code> prints out the following INFO message to the logs:</p> <pre><code>Triggering online replica state changes\n</code></pre> <p><code>startup</code> requests the &lt;&gt; for &lt;&gt;. <p><code>startup</code> &lt;&gt; (with the online replicas and <code>OnlineReplica</code> target state). <p><code>startup</code> prints out the following INFO message to the logs:</p> <pre><code>Triggering offline replica state changes\n</code></pre> <p><code>startup</code> &lt;&gt; (with the offline replicas and <code>OfflineReplica</code> target state). <p>In the end, <code>startup</code> prints out the following DEBUG message to the logs:</p> <pre><code>Started replica state machine with initial state -&gt; [replicaState]\n</code></pre> <p>NOTE: <code>startup</code> is used exclusively when <code>KafkaController</code> is requested to &lt;&gt; (when a broker is successfully elected as the controller). <p>=== [[initializeReplicaState]] Initializing Partition Replica States using ControllerContext -- <code>initializeReplicaState</code> Internal Method</p>"},{"location":"controller/ReplicaStateMachine/#source-scala_3","title":"[source, scala]","text":""},{"location":"controller/ReplicaStateMachine/#initializereplicastate-unit","title":"initializeReplicaState(): Unit","text":"<p><code>initializeReplicaState</code> requests the &lt;&gt; for &lt;&gt;. <p>For every partition, <code>initializeReplicaState</code> requests the &lt;&gt; for the &lt;&gt; (the broker IDs of partition replicas). <p>For every partition replica, <code>initializeReplicaState</code> requests the &lt;&gt; to &lt;&gt; to <code>OnlineReplica</code> or <code>ReplicaDeletionIneligible</code> per &lt;&gt;. <p>NOTE: <code>initializeReplicaState</code> is used exclusively when <code>ReplicaStateMachine</code> is requested to &lt;&gt;."},{"location":"controller/TopicDeletionManager/","title":"TopicDeletionManager","text":""},{"location":"controller/TopicDeletionManager/#review-me","title":"Review Me","text":"<p><code>TopicDeletionManager</code> is &lt;&gt; exclusively when <code>KafkaController</code> is link:kafka-controller-KafkaController.adoc#topicDeletionManager[created]. <p>.TopicDeletionManager is Created Alongside KafkaController image::images/TopicDeletionManager.png[align=\"center\"]</p> <p><code>TopicDeletionManager</code> is controlled by link:kafka-properties.adoc#delete.topic.enable[delete.topic.enable] Kafka property and does nothing when it is turned off (i.e. <code>false</code>).</p> <p>[[internal-registries]] .TopicDeletionManager's Internal Properties (e.g. Registries and Counters) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| [[partitionsToBeDeleted]] <code>partitionsToBeDeleted</code> | <code>TopicAndPartitions</code> to be deleted</p> <p>| [[topicsToBeDeleted]] <code>topicsToBeDeleted</code> | The names of the topics to be deleted</p> <p>| [[topicsIneligibleForDeletion]] <code>topicsIneligibleForDeletion</code> | The names of the topics that must not be deleted (i.e. are ineligible for deletion) |===</p> <p>[[logIdent]] <code>TopicDeletionManager</code> uses [Topic Deletion Manager [brokerId]] as the logging prefix (aka <code>logIdent</code>).</p> <p>[[logging]] [TIP] ==== Enable <code>INFO</code> logging level for <code>kafka.controller.TopicDeletionManager</code> logger to see what happens inside.</p> <p>Add the following line to <code>config/log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.controller.TopicDeletionManager=INFO\n</code></pre>"},{"location":"controller/TopicDeletionManager/#refer-to-linkkafka-loggingadoclogging","title":"Refer to link:kafka-logging.adoc[Logging].","text":"<p>=== [[startReplicaDeletion]] <code>startReplicaDeletion</code> Internal Method</p>"},{"location":"controller/TopicDeletionManager/#source-scala","title":"[source, scala]","text":""},{"location":"controller/TopicDeletionManager/#startreplicadeletionreplicasfortopicstobedeleted-setpartitionandreplica-unit","title":"startReplicaDeletion(replicasForTopicsToBeDeleted: Set[PartitionAndReplica]): Unit","text":"<p><code>startReplicaDeletion</code>...FIXME</p> <p>NOTE: <code>startReplicaDeletion</code> is used when...FIXME</p> <p>=== [[enqueueTopicsForDeletion]] <code>enqueueTopicsForDeletion</code> Method</p>"},{"location":"controller/TopicDeletionManager/#source-scala_1","title":"[source, scala]","text":""},{"location":"controller/TopicDeletionManager/#enqueuetopicsfordeletiontopics-setstring-unit","title":"enqueueTopicsForDeletion(topics: Set[String]): Unit","text":"<p><code>enqueueTopicsForDeletion</code>...FIXME</p> <p>NOTE: <code>enqueueTopicsForDeletion</code> is used when...FIXME</p> <p>=== [[failReplicaDeletion]] <code>failReplicaDeletion</code> Method</p>"},{"location":"controller/TopicDeletionManager/#source-scala_2","title":"[source, scala]","text":""},{"location":"controller/TopicDeletionManager/#failreplicadeletionreplicas-setpartitionandreplica-unit","title":"failReplicaDeletion(replicas: Set[PartitionAndReplica]): Unit","text":"<p><code>failReplicaDeletion</code>...FIXME</p> <p>NOTE: <code>failReplicaDeletion</code> is used when...FIXME</p> <p>=== [[creating-instance]] Creating TopicDeletionManager Instance</p> <p><code>TopicDeletionManager</code> takes the following when created:</p> <ul> <li>[[controller]] link:kafka-controller-KafkaController.adoc[KafkaController]</li> <li>[[eventManager]] link:kafka-controller-ControllerEventManager.adoc[ControllerEventManager]</li> </ul> <p><code>TopicDeletionManager</code> initializes the &lt;&gt;. <p>=== [[markTopicIneligibleForDeletion]] <code>markTopicIneligibleForDeletion</code> Method</p>"},{"location":"controller/TopicDeletionManager/#source-scala_3","title":"[source, scala]","text":""},{"location":"controller/TopicDeletionManager/#marktopicineligiblefordeletiontopics-setstring-unit","title":"markTopicIneligibleForDeletion(topics: Set[String]): Unit","text":"<p>(only with link:kafka-properties.adoc#delete.topic.enable[delete.topic.enable] Kafka property enabled) <code>markTopicIneligibleForDeletion</code> computes the intersection between &lt;&gt; and the input <code>topics</code> sets and adds the intersection to &lt;&gt; set. <p>If there are any topics in the intersection, <code>markTopicIneligibleForDeletion</code> prints out the following INFO message to the logs:</p> <pre><code>Halted deletion of topics [newTopicsToHaltDeletion]\n</code></pre>"},{"location":"controller/TopicDeletionManager/#note","title":"[NOTE]","text":"<p><code>markTopicIneligibleForDeletion</code> is used when:</p> <ul> <li> <p><code>KafkaController</code> link:kafka-controller-KafkaController.adoc#initiateReassignReplicasForTopicPartition[initiateReassignReplicasForTopicPartition]</p> </li> <li> <p><code>TopicDeletion</code> controller event is link:kafka-controller-ControllerEvent-TopicDeletion.adoc#partitionReassignmentInProgress[processed] (for topics to be deleted with partitions in <code>controllerContext.partitionsBeingReassigned</code> list)</p> </li> </ul>"},{"location":"controller/TopicDeletionManager/#topicdeletionmanager-does-and","title":"* <code>TopicDeletionManager</code> does &lt;&gt; and &lt;&gt; <p>=== [[reset]] Reseting -- <code>reset</code> Method</p>","text":""},{"location":"controller/TopicDeletionManager/#source-scala_4","title":"[source, scala]","text":""},{"location":"controller/TopicDeletionManager/#reset-unit","title":"reset(): Unit","text":"<p>(only with link:kafka-properties.adoc#delete.topic.enable[delete.topic.enable] Kafka property enabled) <code>reset</code> removes all elements from the following internal registries:</p> <ul> <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <p>NOTE: <code>reset</code> does nothing when link:kafka-properties.adoc#delete.topic.enable[delete.topic.enable] Kafka property is <code>false</code>.</p> <p>NOTE: <code>reset</code> is used exclusively when <code>KafkaController</code> link:kafka-controller-KafkaController.adoc#onControllerResignation[resigns as the active controller].</p> <p>=== [[onTopicDeletion]] <code>onTopicDeletion</code> Internal Method</p>"},{"location":"controller/TopicDeletionManager/#source-scala_5","title":"[source, scala]","text":""},{"location":"controller/TopicDeletionManager/#ontopicdeletiontopics-setstring-unit","title":"onTopicDeletion(topics: Set[String]): Unit","text":"<p><code>onTopicDeletion</code>...FIXME</p> <p>NOTE: <code>onTopicDeletion</code> is used when...FIXME</p> <p>=== [[completeDeleteTopic]] <code>completeDeleteTopic</code> Internal Method</p>"},{"location":"controller/TopicDeletionManager/#source-scala_6","title":"[source, scala]","text":""},{"location":"controller/TopicDeletionManager/#completedeletetopictopic-string-unit","title":"completeDeleteTopic(topic: String): Unit","text":"<p><code>completeDeleteTopic</code>...FIXME</p> <p>NOTE: <code>completeDeleteTopic</code> is used when...FIXME</p> <p>=== [[init]] Initializing -- <code>init</code> Method</p>"},{"location":"controller/TopicDeletionManager/#source-scala_7","title":"[source, scala]","text":"<p>init(   initialTopicsToBeDeleted: Set[String],   initialTopicsIneligibleForDeletion: Set[String]): Unit</p> <p><code>init</code>...FIXME</p> <p>NOTE: <code>init</code> is used when...FIXME</p> <p>=== [[tryTopicDeletion]] <code>tryTopicDeletion</code> Method</p>"},{"location":"controller/TopicDeletionManager/#source-scala_8","title":"[source, scala]","text":""},{"location":"controller/TopicDeletionManager/#trytopicdeletion-unit","title":"tryTopicDeletion(): Unit","text":"<p><code>tryTopicDeletion</code>...FIXME</p> <p>NOTE: <code>tryTopicDeletion</code> is used when...FIXME</p> <p>=== [[isTopicQueuedUpForDeletion]] <code>isTopicQueuedUpForDeletion</code> Method</p>"},{"location":"controller/TopicDeletionManager/#source-scala_9","title":"[source, scala]","text":""},{"location":"controller/TopicDeletionManager/#istopicqueuedupfordeletiontopic-string-boolean","title":"isTopicQueuedUpForDeletion(topic: String): Boolean","text":"<p><code>isTopicQueuedUpForDeletion</code>...FIXME</p> <p>NOTE: <code>isTopicQueuedUpForDeletion</code> is used when...FIXME</p> <p>=== [[resumeDeletionForTopics]] <code>resumeDeletionForTopics</code> Method</p>"},{"location":"controller/TopicDeletionManager/#source-scala_10","title":"[source, scala]","text":""},{"location":"controller/TopicDeletionManager/#resumedeletionfortopicstopics-setstring-setempty-unit","title":"resumeDeletionForTopics(topics: Set[String] = Set.empty): Unit","text":"<p><code>resumeDeletionForTopics</code>...FIXME</p> <p>NOTE: <code>resumeDeletionForTopics</code> is used when...FIXME</p> <p>=== [[completeReplicaDeletion]] <code>completeReplicaDeletion</code> Method</p>"},{"location":"controller/TopicDeletionManager/#source-scala_11","title":"[source, scala]","text":""},{"location":"controller/TopicDeletionManager/#completereplicadeletionreplicas-setpartitionandreplica-unit","title":"completeReplicaDeletion(replicas: Set[PartitionAndReplica]): Unit","text":"<p><code>completeReplicaDeletion</code>...FIXME</p> <p>NOTE: <code>completeReplicaDeletion</code> is used when...FIXME</p> <p>=== [[markTopicForDeletionRetry]] <code>markTopicForDeletionRetry</code> Internal Method</p>"},{"location":"controller/TopicDeletionManager/#source-scala_12","title":"[source, scala]","text":""},{"location":"controller/TopicDeletionManager/#marktopicfordeletionretrytopic-string-unit","title":"markTopicForDeletionRetry(topic: String): Unit","text":"<p><code>markTopicForDeletionRetry</code>...FIXME</p> <p>NOTE: <code>markTopicForDeletionRetry</code> is used when...FIXME</p> <p>=== [[retryDeletionForIneligibleReplicas]] <code>retryDeletionForIneligibleReplicas</code> Internal Method</p>"},{"location":"controller/TopicDeletionManager/#source-scala_13","title":"[source, scala]","text":""},{"location":"controller/TopicDeletionManager/#retrydeletionforineligiblereplicastopic-string-unit","title":"retryDeletionForIneligibleReplicas(topic: String): Unit","text":"<p><code>retryDeletionForIneligibleReplicas</code>...FIXME</p> <p>NOTE: <code>retryDeletionForIneligibleReplicas</code> is used when...FIXME</p>"},{"location":"controller/TopicUncleanLeaderElectionEnable/","title":"TopicUncleanLeaderElectionEnable","text":"<p><code>TopicUncleanLeaderElectionEnable</code> is a ControllerEvent that KafkaController uses to trigger processTopicUncleanLeaderElectionEnable.</p>"},{"location":"controller/TopicUncleanLeaderElectionEnable/#creating-instance","title":"Creating Instance","text":"<p><code>TopicUncleanLeaderElectionEnable</code> takes the following to be created:</p> <ul> <li> Topic Name <p><code>TopicUncleanLeaderElectionEnable</code> is created when:</p> <ul> <li><code>KafkaController</code> is requested to enableTopicUncleanLeaderElection (on an active controller)</li> </ul>"},{"location":"controller/TopicUncleanLeaderElectionEnable/#controllerstate","title":"ControllerState <pre><code>state: ControllerState\n</code></pre> <p><code>state</code> is part of the ControllerEvent abstraction.</p>  <p><code>state</code> is <code>TopicUncleanLeaderElectionEnable</code>.</p>","text":""},{"location":"controller/TopicUncleanLeaderElectionEnable/#kafkacontroller","title":"KafkaController","text":""},{"location":"controller/ZkPartitionStateMachine/","title":"ZkPartitionStateMachine","text":"<p><code>ZkPartitionStateMachine</code> is a PartitionStateMachine of a KafkaController.</p> <p></p> <p>When requested to handle partition state changes, <code>ZkPartitionStateMachine</code> uses the ControllerBrokerRequestBatch to propagate them to all brokers in a cluster.</p>"},{"location":"controller/ZkPartitionStateMachine/#creating-instance","title":"Creating Instance","text":"<p><code>ZkPartitionStateMachine</code> takes the following to be created:</p> <ul> <li> KafkaConfig <li> <code>StateChangeLogger</code> <li> ControllerContext <li> KafkaZkClient <li> ControllerBrokerRequestBatch <p><code>ZkPartitionStateMachine</code> is created along with a KafkaController.</p>"},{"location":"controller/ZkPartitionStateMachine/#handling-state-changes-of-partitions","title":"Handling State Changes of Partitions <pre><code>handleStateChanges(\n  partitions: Seq[TopicPartition],\n  targetState: PartitionState,\n  partitionLeaderElectionStrategyOpt: Option[PartitionLeaderElectionStrategy]\n): Map[TopicPartition, Either[Throwable, LeaderAndIsr]]\n</code></pre> <p><code>handleStateChanges</code> is part of the PartitionStateMachine abstraction.</p>  <p><code>handleStateChanges</code> does nothing and returns an empty collection when executed with no partitions.</p> <p><code>handleStateChanges</code> requests the ControllerBrokerRequestBatch to prepare a new batch.</p> <p><code>handleStateChanges</code> doHandleStateChanges (that may give some errors that are returned in the end).</p> <p>In the end, <code>handleStateChanges</code> requests the ControllerBrokerRequestBatch to send controller requests to brokers.</p>","text":""},{"location":"controller/ZkPartitionStateMachine/#dohandlestatechanges","title":"doHandleStateChanges <pre><code>doHandleStateChanges(\n  partitions: Seq[TopicPartition],\n  targetState: PartitionState,\n  partitionLeaderElectionStrategyOpt: Option[PartitionLeaderElectionStrategy]\n): Map[TopicPartition, Either[Throwable, LeaderAndIsr]]\n</code></pre>  <p><code>doHandleStateChanges</code> requests the ControllerContext to putPartitionStateIfNotExists to <code>NonExistentPartition</code> for every partition (in <code>partitions</code>).</p> <p><code>doHandleStateChanges</code> requests the ControllerContext to checkValidPartitionStateChange with the given target <code>PartitionState</code> (that splits the partitions into valid and invalid partitions).</p> <p><code>doHandleStateChanges</code> logInvalidTransition for every invalid partition.</p> <p><code>doHandleStateChanges</code> branches off per the target state:</p> <ul> <li>NewPartition</li> <li>OnlinePartition</li> <li>OfflinePartition or NonExistentPartition</li> </ul>","text":""},{"location":"controller/ZkPartitionStateMachine/#newpartition","title":"NewPartition <p>For <code>NewPartition</code> target state, <code>doHandleStateChanges</code> goes over the valid partitions and for every partition prints out the following TRACE message to the logs and requests the &lt;&gt; to &lt;&gt; to <code>NewPartition</code> state. <pre><code>Changed partition [partition] state from [state] to NewPartition with assigned replicas [partitionReplicaAssignment]\n</code></pre>","text":""},{"location":"controller/ZkPartitionStateMachine/#onlinepartition","title":"OnlinePartition <p><code>doHandleStateChanges</code> finds uninitialized partitions (among the valid partitions with <code>NewPartition</code> state).</p> <p><code>doHandleStateChanges</code> finds partitions to elect a leader (among the valid partitions with <code>OfflinePartition</code> or <code>OnlinePartition</code> state).</p> <p>For uninitialized partitions, <code>doHandleStateChanges</code> initializeLeaderAndIsrForPartitions, prints out the following INFO message to the logs and requests the ControllerContext to putPartitionState to <code>OnlinePartition</code> state.</p> <pre><code>Changed partition [partition] from [state] to OnlinePartition with state [leaderAndIsr]\n</code></pre> <p>For partitions to elect a leader, <code>doHandleStateChanges</code> electLeaderForPartitions with the input PartitionLeaderElectionStrategy.</p> <p>For every partition with leader election successful, <code>doHandleStateChanges</code> prints out the following INFO message to the logs and requests the ControllerContext to putPartitionState to <code>OnlinePartition</code> state.</p> <pre><code>Changed partition [partition] from [state] to OnlinePartition with state [leaderAndIsr]\n</code></pre> <p>In the end, <code>doHandleStateChanges</code> returns the partitions with election failed.</p>","text":""},{"location":"controller/ZkPartitionStateMachine/#offlinepartition-or-nonexistentpartition","title":"OfflinePartition or NonExistentPartition <p>For <code>OfflinePartition</code> target state, <code>doHandleStateChanges</code> goes over the valid partitions and for every partition prints out the following TRACE message to the logs and requests the &lt;&gt; to &lt;&gt; to <code>OfflinePartition</code> state. <pre><code>Changed partition [partition] state from [state] to OfflinePartition\n</code></pre> <p>For <code>NonExistentPartition</code> target state, <code>doHandleStateChanges</code> goes over the valid partitions and for every partition prints out the following TRACE message to the logs and requests the &lt;&gt; to &lt;&gt; to <code>NonExistentPartition</code> state. <pre><code>Changed partition [partition] state from [state] to NonExistentPartition\n</code></pre>","text":""},{"location":"controller/ZkPartitionStateMachine/#electleaderforpartitions","title":"electLeaderForPartitions <pre><code>electLeaderForPartitions(\n  partitions: Seq[TopicPartition],\n  partitionLeaderElectionStrategy: PartitionLeaderElectionStrategy\n): Map[TopicPartition, Either[Throwable, LeaderAndIsr]]\n</code></pre>  <p><code>electLeaderForPartitions</code> doElectLeaderForPartitions until all the given partitions have partition leaders elected successfully or not.</p> <p>For any partition to retry a leader election, <code>electLeaderForPartitions</code> prints out the following INFO message to the logs:</p> <pre><code>Retrying leader election with strategy [partitionLeaderElectionStrategy] for partitions [remaining]\n</code></pre>","text":""},{"location":"controller/ZkPartitionStateMachine/#doelectleaderforpartitions","title":"doElectLeaderForPartitions <pre><code>doElectLeaderForPartitions(\n  partitions: Seq[TopicPartition],\n  partitionLeaderElectionStrategy: PartitionLeaderElectionStrategy\n): (Map[TopicPartition, Either[Exception, LeaderAndIsr]], Seq[TopicPartition])\n</code></pre> <p><code>doElectLeaderForPartitions</code> requests the KafkaZkClient for the partition states (with <code>LeaderAndIsr</code> information).</p> <p>For every partition state response, <code>doElectLeaderForPartitions</code> decodes the response (if possible) and adds it to <code>validLeaderAndIsrs</code> internal registry (of <code>(TopicPartition, LeaderAndIsr)</code> pairs) or to failed elections (of <code>TopicPartition, Either[Exception, LeaderAndIsr]</code>s).</p> <p><code>doElectLeaderForPartitions</code> branches off per the input PartitionLeaderElectionStrategy that gives partitions with and without leaders elected.</p> <ul> <li> <p>For OfflinePartitionLeaderElectionStrategy, <code>doElectLeaderForPartitions</code> collectUncleanLeaderElectionState with the valid partitions for election followed by leaderForOffline.</p> </li> <li> <p>For ReassignPartitionLeaderElectionStrategy, <code>doElectLeaderForPartitions</code> leaderForReassign</p> </li> <li> <p>For PreferredReplicaPartitionLeaderElectionStrategy, <code>doElectLeaderForPartitions</code> leaderForPreferredReplica</p> </li> <li> <p>For ControlledShutdownPartitionLeaderElectionStrategy, <code>doElectLeaderForPartitions</code> leaderForControlledShutdown</p> </li> </ul> <p><code>doElectLeaderForPartitions</code> adds the partitions with no leader elected to failed elections.</p> <p><code>doElectLeaderForPartitions</code> requests the KafkaZkClient to updateLeaderAndIsr (with the adjusted leader and ISRs).</p> <p>For every successfully-updated partition (in Zookeeper), <code>doElectLeaderForPartitions</code> requests the following:</p> <ol> <li> <p>The ControllerContext to partitionFullReplicaAssignment and record the partition leadership</p> </li> <li> <p>The ControllerBrokerRequestBatch to addLeaderAndIsrRequestForBrokers to every live replica broker (with <code>isNew</code> flag off)</p> </li> </ol> <p><code>doElectLeaderForPartitions</code> prints out the following DEBUG message to the logs for every partition with no leader elected:</p> <pre><code>Controller failed to elect leader for partition [partition].\nAttempted to write state [partition], but failed with bad ZK version.\nThis will be retried.\n</code></pre>","text":""},{"location":"controller/ZkPartitionStateMachine/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>kafka.controller.ZkPartitionStateMachine</code> logger to see what happens inside.</p> <p>Add the following line to <code>config/log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.controller.ZkPartitionStateMachine=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"controller/ZkPartitionStateMachine/#logident","title":"logIdent <p><code>ZkPartitionStateMachine</code> uses the following logging prefix (with the broker.id):</p> <pre><code>[PartitionStateMachine controllerId=[brokerId]]\n</code></pre>","text":""},{"location":"controller/ZkPartitionStateMachine/#review-me","title":"Review Me","text":""},{"location":"controller/ZkPartitionStateMachine/#initializeleaderandisrforpartitions","title":"initializeLeaderAndIsrForPartitions <pre><code>initializeLeaderAndIsrForPartitions(\n  partitions: Seq[TopicPartition]): Seq[TopicPartition]\n</code></pre> <p><code>initializeLeaderAndIsrForPartitions</code> starts by requesting the &lt;&gt; for the &lt;&gt; for every partition (in the given <code>partitions</code>). <p>From the partition replica assignments, <code>initializeLeaderAndIsrForPartitions</code> makes sure that the replicas are all &lt;&gt; only (per the &lt;&gt;) so all other partitions are filtered out (excluded). <p><code>initializeLeaderAndIsrForPartitions</code> splits the partitions (with online replicas only) into two sets with and without replicas (<code>partitionsWithLiveReplicas</code> and <code>partitionsWithoutLiveReplicas</code>, respectively).</p> <p>For every partition without live (online) replicas, <code>initializeLeaderAndIsrForPartitions</code> &lt;&gt;: <pre><code>Controller [controllerId] epoch [epoch] failed to change state for partition [partition] from NewPartition to OnlinePartition\n</code></pre> <pre><code>Controller [controllerId] epoch [epoch] encountered error during state change of partition [partition] from New to Online, assigned replicas are [[replicas]], live brokers are [[liveBrokerIds]]. No assigned replica is alive.\n</code></pre> <p><code>initializeLeaderAndIsrForPartitions</code> converts the partitions with live (online) replicas into <code>leaderIsrAndControllerEpochs</code> (<code>LeaderIsrAndControllerEpoch</code> with <code>LeaderAndIsr</code>) and for every pair <code>initializeLeaderAndIsrForPartitions</code> requests the &lt;&gt; to &lt;&gt;. <p>For every successful response (from &lt;&gt;), <code>initializeLeaderAndIsrForPartitions</code> requests the following: <p>. The &lt;&gt; to record the <code>leaderIsrAndControllerEpoch</code> for the partition (in the &lt;&gt; registry) <p>. The &lt;&gt; to &lt;&gt; (with <code>isNew</code> flag on) <p>In the end, <code>initializeLeaderAndIsrForPartitions</code> returns the partitions that were successfully initialized.</p> <p>In case of <code>ControllerMovedException</code> (while...FIXME), <code>initializeLeaderAndIsrForPartitions</code>...FIXME</p> <p>In case of any other error (<code>Exception</code>) (while...FIXME), <code>initializeLeaderAndIsrForPartitions</code>...FIXME</p> <p>NOTE: <code>initializeLeaderAndIsrForPartitions</code> is used exclusively when <code>ZkPartitionStateMachine</code> is requested to &lt;&gt; (for &lt;&gt;).","text":""},{"location":"controller/ZkReplicaStateMachine/","title":"ZkReplicaStateMachine","text":"<p><code>ZkReplicaStateMachine</code> is a ReplicaStateMachine to handle changes of the state of partition replicas.</p> <p><code>ZkReplicaStateMachine</code> uses ControllerBrokerRequestBatch to propagate replica state changes to all brokers in a Kafka cluster.</p>"},{"location":"controller/ZkReplicaStateMachine/#creating-instance","title":"Creating Instance","text":"<p><code>ZkReplicaStateMachine</code> takes the following to be created:</p> <ul> <li> KafkaConfig <li> <code>StateChangeLogger</code> <li> ControllerContext <li> KafkaZkClient <li> ControllerBrokerRequestBatch <p><code>ZkReplicaStateMachine</code> is created along with a KafkaController.</p>"},{"location":"controller/ZkReplicaStateMachine/#handling-replica-state-changes","title":"Handling Replica State Changes <pre><code>handleStateChanges(\n  replicas: Seq[PartitionAndReplica],\n  targetState: ReplicaState): Unit\n</code></pre> <p><code>handleStateChanges</code> is part of the ReplicaStateMachine abstraction.</p>   <p>Note</p> <p><code>handleStateChanges</code> is a noop and does nothing when the input <code>replicas</code> collection is empty.</p>  <p><code>handleStateChanges</code> requests the ControllerBrokerRequestBatch for a new batch.</p> <p><code>handleStateChanges</code> groups the <code>replicas</code> by the replica ID and doHandleStateChanges for every replica ID (with the <code>ReplicaState</code>).</p> <p>In the end, <code>handleStateChanges</code> requests the ControllerBrokerRequestBatch to sendRequestsToBrokers.</p>","text":""},{"location":"controller/ZkReplicaStateMachine/#dohandlestatechanges","title":"doHandleStateChanges <pre><code>doHandleStateChanges(\n  replicaId: Int,\n  replicas: Seq[PartitionAndReplica],\n  targetState: ReplicaState): Unit\n</code></pre> <p>For every replica (in the <code>replicas</code>), <code>doHandleStateChanges</code> requests the ControllerBrokerRequestBatch to putReplicaStateIfNotExists (with <code>NonExistentReplica</code> state)</p> <p><code>doHandleStateChanges</code> requests the ControllerBrokerRequestBatch to checkValidReplicaStateChange (that gives valid and invalid replicas).</p> <p>For every invalid replica, <code>doHandleStateChanges</code> logInvalidTransition.</p> <p><code>doHandleStateChanges</code> branches off per the input target state (<code>ReplicaState</code>):</p> <ul> <li><code>NewReplica</code></li> <li><code>OnlineReplica</code></li> <li><code>OfflineReplica</code></li> <li><code>ReplicaDeletionStarted</code></li> <li><code>ReplicaDeletionIneligible</code></li> <li><code>ReplicaDeletionSuccessful</code></li> <li><code>NonExistentReplica</code></li> </ul>","text":""},{"location":"controller/ZkReplicaStateMachine/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>kafka.controller.ZkReplicaStateMachine</code> logger to see what happens inside.</p> <p>Add the following line to <code>config/log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.controller.ZkReplicaStateMachine=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"controller/ZkReplicaStateMachine/#logident","title":"logIdent <p><code>ZkReplicaStateMachine</code> uses the following logging prefix (with the broker.id):</p> <pre><code>[ReplicaStateMachine controllerId=[brokerId]]\n</code></pre>","text":""},{"location":"controller/controller-election/","title":"Controller Election","text":"<p>Controller Election is a process to elect a Kafka broker as the controller in a Kafka cluster.</p> <p>Kafka Controller Election process relies heavily on the features of Apache ZooKeeper that acts as the source of truth and guarantees that only one broker can ever be elected (due to how ephemeral nodes work).</p> <p>Nodes and ephemeral nodes</p> <p>ZooKeeper also has the notion of ephemeral nodes. These znodes exists as long as the session that created the znode is active. When the session ends the znode is deleted.</p> <p>Leader Election</p> <p>A simple way of doing leader election with ZooKeeper is to use the SEQUENCE|EPHEMERAL flags when creating znodes that represent \"proposals\" of clients. The idea is to have a znode, say \"/election\"</p> <p>When <code>ControllerEventThread</code> is requested to process Startup and Reelect controller events, <code>KafkaController</code> (instance that runs on every Kafka broker) is requested to elect.</p> <p>Tip</p> <p>Consult Demo: Kafka Controller Election to learn about the process.</p> <p>Given that all the state is in ZooKeeper use <code>zookeeper-shell</code> script to know which broker is the active controller.</p> <pre><code>$ ./bin/zookeeper-shell.sh :2181 get /controller\nConnecting to :2181\n\n{\"version\":1,\"brokerid\":0,\"timestamp\":\"1543491973573\"}\ncZxid = 0x48\nctime = Thu Nov 29 12:46:13 CET 2018\nmZxid = 0x48\nmtime = Thu Nov 29 12:46:13 CET 2018\npZxid = 0x48\ncversion = 0\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0x100073f07ba0001\ndataLength = 54\nnumChildren = 0\n</code></pre> <p>If you receive <code>Node does not exist: /controller</code> error message, that means that no Kafka broker has been elected as the active controller yet (or that you use an incorrect ZooKeeper server to talk to).</p> <p>You could also use <code>nc</code> to talk to ZooKeeper in a more direct way (that allows for <code>dump</code> command).</p> <pre><code>$ nc localhost 2181\ndump\nSessionTracker dump:\nSession Sets (2):\n0 expire at Fri Jan 02 10:57:03 CET 1970:\n1 expire at Fri Jan 02 10:57:06 CET 1970:\n  0x100073f07ba0001\nephemeral nodes dump:\nSessions with Ephemerals (1):\n0x100073f07ba0001:\n  /controller\n  /brokers/ids/0\n</code></pre> <p>From Notable changes in 0.10.1.0 in the official documentation of Apache Kafka:</p> <p>The recommended way to detect if a given broker is the controller is via the <code>kafka.controller:type=KafkaController,name=ActiveControllerCount</code> metric.</p> <p></p>"},{"location":"controller/controller-election/#forcing-controller-re-election","title":"Forcing Controller Re-election","text":"<p>Controller re-election may be forced by running the following command:</p> <pre><code>bin/zookeeper-shell.sh localhost delete /controller\n</code></pre> <p>Tip</p> <p>Monitor logs/controller.log to see what happens.</p>"},{"location":"controller/controller-election/#controller-id-registered-in-zookeeper","title":"Controller ID Registered (in ZooKeeper)","text":"<p>The election process stops when there is a controller ID registered in Zookeeper (using <code>KafkaZkClient</code> that gets the ID of the active controller and the ID is any number but <code>-1</code>).</p>"},{"location":"controller/controller-election/#no-controller-id-registered-in-zookeeper","title":"No Controller ID Registered (in ZooKeeper)","text":"<p>If there is no controller ID registered, every <code>KafkaController</code> instance tries to register itself as the controller (in Zookeeper) and increment the controller epoch (using KafkaZkClient).</p> <p>In the end, the active <code>KafkaController</code> is requested to onControllerFailover.</p>"},{"location":"demo/","title":"Demos","text":"<p>The following demos are available:</p> <ul> <li>ACL Authorization</li> <li>Controller Election</li> <li>Kafka and kcat in Docker</li> <li>Secure Inter-Broker Communication</li> <li>Securing Communication Between Clients and Brokers Using SSL</li> <li>SSL Authentication</li> <li>Transactional Kafka Producer</li> </ul>"},{"location":"demo/acl-authorization/","title":"Demo: ACL Authorization","text":"<p>This demo shows how to use ACL authorization in Apache Kafka (using AclAuthorizer) to restrict topic operations.</p> <p>In ACL words, the demo shows how to allow <code>Write</code> and <code>Read</code> operations to <code>ANY</code> topic to certain users.</p> <p>You'll be using there users for different ACLs:</p> <ul> <li> <p><code>CN=root</code> - a user with all topic operations allowed</p> </li> <li> <p><code>CN=producer</code> - a user with Write operation allowed</p> </li> <li> <p><code>CN=consumer</code> - a user with Read operation allowed</p> </li> </ul>"},{"location":"demo/acl-authorization/#before-you-begin","title":"Before You Begin","text":"<p>The demo is a follow-up to Demo: SSL Authentication. Please finish it first before this demo.</p>"},{"location":"demo/acl-authorization/#enable-acl-authorization","title":"Enable ACL Authorization","text":"<p>Enable ACL authorization in a Kafka cluster.</p> <p>Add the following configuration property to <code>config/server-ssl.properties</code>:</p> <pre><code>authorizer.class.name=kafka.security.authorizer.AclAuthorizer\n</code></pre> <p>Restart the Kafka broker and observe the logs. You should find the following <code>ClusterAuthorizationException</code> at the very end of the logs:</p> <pre><code>ClusterAuthorizationException: Request Request(processor=0, connectionId=127.0.0.1:9093-127.0.0.1:62402-0, session=Session(User:CN=localhost,/127.0.0.1), listenerName=ListenerName(SSL), securityProtocol=SSL, buffer=null) is not authorized.\n</code></pre> <p>That's because <code>User:CN=localhost</code> user is not authorized to execute an action (since by default no one is allowed to execute any action).</p>"},{"location":"demo/acl-authorization/#review-authorization-logs","title":"Review Authorization Logs","text":"<p>Access denials are logged at INFO level to <code>logs/kafka-authorizer.log</code> by default.</p> <p>You should find the following INFO message (which corresponds to the <code>ClusterAuthorizationException</code> earlier):</p> <pre><code>Principal = User:CN=localhost is Denied Operation = ClusterAction from host = 127.0.0.1 on resource = Cluster:LITERAL:kafka-cluster for request = UpdateMetadata with resourceRefCount = 1\n</code></pre>"},{"location":"demo/acl-authorization/#enable-debug-logging-level-for-kafkaauthorizerlogger","title":"Enable DEBUG Logging Level for kafka.authorizer.logger","text":"<p>Enable allowed accesses that are logged at DEBUG level. Edit <code>config/log4j.properties</code> and change the logging level of <code>kafka.authorizer.logger</code> to DEBUG:</p> <pre><code>log4j.logger.kafka.authorizer.logger=DEBUG, authorizerAppender\nlog4j.additivity.kafka.authorizer.logger=false\n</code></pre> <p>Restart the broker.</p>"},{"location":"demo/acl-authorization/#create-user-cnroot","title":"Create User CN=root","text":"<p>You will now \"create\" a user identified as <code>CN=root</code>.</p> <p>Generate the keys and certificate of the user.</p> <pre><code>keytool \\\n  -genkey \\\n  -keystore root.keystore \\\n  -alias root \\\n  -dname CN=root \\\n  -keyalg RSA \\\n  -validity 365 \\\n  -storepass 123456\n</code></pre> <p>Export the certificate of the user from the keystore.</p> <pre><code>keytool \\\n  -certreq \\\n  -keystore root.keystore \\\n  -alias root \\\n  -file root.unsigned.crt \\\n  -storepass 123456\n</code></pre> <p>Sign the certificate signing request with the root CA.</p> <pre><code>$ openssl x509 \\\n  -req \\\n  -CA ca.crt \\\n  -CAkey ca.key \\\n  -in root.unsigned.crt \\\n  -out root.crt \\\n  -days 365 \\\n  -CAcreateserial \\\n  -passin pass:1234\nSignature ok\nsubject=CN = root\nGetting CA Private Key\n</code></pre> <p>Import the certificate of the CA into the user keystore.</p> <pre><code>$ keytool \\\n  -importcert \\\n  -file ca.crt \\\n  -alias ca \\\n  -keystore root.keystore \\\n  -storepass 123456 \\\n  -noprompt\nCertificate was added to keystore\n</code></pre> <p>Import the signed certificate into the user keystore. Make sure to use the same <code>-alias</code> as you used ealier.</p> <pre><code>$ keytool \\\n  -importcert \\\n  -file root.crt \\\n  -alias root \\\n  -keystore root.keystore \\\n  -storepass 123456\nCertificate reply was installed in keystore\n</code></pre>"},{"location":"demo/acl-authorization/#define-super-users","title":"Define Super Users","text":"<p>Super users are allowed to perform any operation on any resource in a Kafka cluster.</p> <p>Define the broker (as <code>User:CN=localhost</code>) and <code>CN=root</code> as super users.</p> <p>Add the following configuration property to <code>config/server-ssl.properties</code>. Note that the delimiter is semicolon (<code>;</code>) since user names may contain comma.</p> <pre><code>super.users=User:CN=localhost;User:CN=root\n</code></pre> <p>Restart the broker.</p> <p>There should be no exceptions in the logs.</p> <p>Moreover, <code>logs/kafka-authorizer.log</code> should have the following DEBUG messages:</p> <pre><code>principal = User:CN=localhost is a super user, allowing operation without checking acls.\nPrincipal = User:CN=localhost is Allowed Operation = ClusterAction from host = 127.0.0.1 on resource = Cluster:LITERAL:kafka-cluster for request = UpdateMetadata with resourceRefCount = 1\n</code></pre>"},{"location":"demo/acl-authorization/#optional-allow-everyone-if-no-acl-found","title":"(Optional) Allow Everyone If No ACL Found","text":"<p>This step is optional.</p> <p>For a less-secure broker configuration, you could add the following configuration property to <code>config/server-ssl.properties</code>:</p> <pre><code>allow.everyone.if.no.acl.found=true\n</code></pre> <p>That would make access more open to any client (with a valid and trusted certificate).</p> <p>For the demo, enable it so you won't run into the following <code>GroupAuthorizationException</code> later:</p> <pre><code>GroupAuthorizationException: Not authorized to access group: console-consumer-...\n</code></pre> <p>Edit <code>config/server-ssl.properties</code> and restart the broker.</p>"},{"location":"demo/acl-authorization/#list-acls","title":"List ACLs","text":"<p>Use kafka-acls utility to list the access control list (ACL). There should be none.</p> <p>Create <code>root.properties</code> as a minimal configuration of a Kafka client to identify itself as <code>CN=root</code>.</p> <pre><code>security.protocol=SSL\nssl.truststore.location=/tmp/kafka-ssl-demo/client.truststore\nssl.truststore.password=123456\nssl.keystore.location=/tmp/kafka-ssl-demo/root.keystore\nssl.keystore.password=123456\nssl.key.password=123456\n</code></pre> <p>Use <code>--command-config</code> option to specify the SSL configuration.</p> <pre><code>kafka-acls.sh \\\n  --bootstrap-server :9093 \\\n  --list \\\n  --command-config /tmp/kafka-ssl-demo/root.properties\n</code></pre>"},{"location":"demo/acl-authorization/#create-user-cnproducer","title":"Create User CN=producer","text":"<p>You will now \"create\" a <code>CN=producer</code> user (that will have Write operation allowed).</p> <p>Generate the keys and certificate of a Kafka client to be authenticated as CN=producer.</p> <pre><code>keytool \\\n  -genkey \\\n  -keystore producer.keystore \\\n  -alias producer \\\n  -dname CN=producer \\\n  -keyalg RSA \\\n  -validity 365 \\\n  -storepass 123456\n</code></pre> <p>Export the user certificate from the keystore.</p> <pre><code>keytool \\\n  -certreq \\\n  -keystore producer.keystore \\\n  -alias producer \\\n  -file producer.unsigned.crt \\\n  -storepass 123456\n</code></pre> <p>Sign the certificate signing request with the root CA.</p> <pre><code>$ openssl x509 \\\n  -req \\\n  -CA ca.crt \\\n  -CAkey ca.key \\\n  -in producer.unsigned.crt \\\n  -out producer.crt \\\n  -days 365 \\\n  -CAcreateserial \\\n  -passin pass:1234\nSignature ok\nsubject=CN = producer\nGetting CA Private Key\n</code></pre> <p>Import the certificate of the CA into the user keystore.</p> <pre><code>$ keytool \\\n  -import \\\n  -file ca.crt \\\n  -keystore producer.keystore \\\n  -alias ca \\\n  -storepass 123456 \\\n  -noprompt\nCertificate was added to keystore\n</code></pre> <p>Import the signed certificate into the user keystore. Make sure to use the same <code>-alias</code> as you used ealier.</p> <pre><code>$ keytool \\\n  -import \\\n  -file producer.crt \\\n  -keystore producer.keystore \\\n  -alias producer \\\n  -storepass 123456 \\\n  -noprompt\nCertificate reply was installed in keystore\n</code></pre>"},{"location":"demo/acl-authorization/#create-user-cnconsumer","title":"Create User CN=consumer","text":"<p>You will now \"create\" a <code>CN=consumer</code> user (that will have Read operation allowed only).</p> <p>Generate the keys and certificate of a Kafka client to be authenticated as CN=consumer.</p> <pre><code>keytool \\\n  -genkey \\\n  -keystore consumer.keystore \\\n  -alias consumer \\\n  -dname CN=consumer \\\n  -keyalg RSA \\\n  -validity 365 \\\n  -storepass 123456\n</code></pre> <p>Export the user certificate from the keystore.</p> <pre><code>keytool \\\n  -certreq \\\n  -keystore consumer.keystore \\\n  -alias consumer \\\n  -file consumer.unsigned.crt \\\n  -storepass 123456\n</code></pre> <p>Sign the certificate signing request with the root CA.</p> <pre><code>$ openssl x509 \\\n  -req \\\n  -CA ca.crt \\\n  -CAkey ca.key \\\n  -in consumer.unsigned.crt \\\n  -out consumer.crt \\\n  -days 365 \\\n  -CAcreateserial \\\n  -passin pass:1234\nSignature ok\nsubject=CN = consumer\nGetting CA Private Key\n</code></pre> <p>Import the certificate of the CA into the user keystore.</p> <pre><code>$ keytool \\\n  -import \\\n  -file ca.crt \\\n  -alias ca \\\n  -keystore consumer.keystore \\\n  -storepass 123456 \\\n  -noprompt\nCertificate was added to keystore\n</code></pre> <p>Import the signed certificate into the user keystore. Make sure to use the same <code>-alias</code> as you used ealier.</p> <pre><code>$ keytool \\\n  -import \\\n  -file consumer.crt \\\n  -alias consumer \\\n  -keystore consumer.keystore \\\n  -storepass 123456\nCertificate reply was installed in keystore\n</code></pre>"},{"location":"demo/acl-authorization/#restrict-topic-operations-write-for-cnproduce","title":"Restrict Topic Operations -- Write for CN=produce","text":"<p>Use kafka-acls utility to restrict <code>Write</code> operation on any topic to <code>CN=produce</code> user (and super users).</p> <pre><code>kafka-acls.sh \\\n  --bootstrap-server :9093 \\\n  --add \\\n  --allow-principal User:CN=producer \\\n  --operation Write \\\n  --topic '*' \\\n  --command-config /tmp/kafka-ssl-demo/root.properties\n</code></pre> <p>List the ACLs using <code>kafka-acls</code> utility.</p> <pre><code>$ kafka-acls.sh \\\n  --bootstrap-server :9093 \\\n  --list \\\n  --command-config /tmp/kafka-ssl-demo/root.properties\nCurrent ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`:\n  (principal=User:CN=producer, host=*, operation=WRITE, permissionType=ALLOW)\n</code></pre>"},{"location":"demo/acl-authorization/#restrict-topic-operations-read-for-cnconsumer","title":"Restrict Topic Operations -- Read for CN=consumer","text":"<p>Use kafka-acls utility to restrict <code>Write</code> operation on any topic to <code>CN=consumer</code> user (and super users).</p> <pre><code>kafka-acls.sh \\\n  --bootstrap-server :9093 \\\n  --add \\\n  --allow-principal User:CN=consumer \\\n  --operation Read \\\n  --topic '*' \\\n  --command-config /tmp/kafka-ssl-demo/root.properties\n</code></pre> <p>List the ACLs using <code>kafka-acls</code> utility.</p> <pre><code>$ kafka-acls.sh \\\n  --bootstrap-server :9093 \\\n  --list \\\n  --command-config /tmp/kafka-ssl-demo/root.properties\nCurrent ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`:\n  (principal=User:CN=producer, host=*, operation=WRITE, permissionType=ALLOW)\n  (principal=User:CN=consumer, host=*, operation=READ, permissionType=ALLOW)\n</code></pre>"},{"location":"demo/acl-authorization/#send-messages","title":"Send Messages","text":"<p>Create <code>producer.properties</code> file as a minimal configuration of a Kafka client to use SSL authentication and identify itself as <code>CN=producer</code>:</p> <pre><code>security.protocol=SSL\nssl.truststore.location=/tmp/kafka-ssl-demo/client.truststore\nssl.truststore.password=123456\nssl.keystore.location=/tmp/kafka-ssl-demo/producer.keystore\nssl.keystore.password=123456\nssl.key.password=123456\n</code></pre> <p>Use <code>kafka-console-producer.sh</code> utility to send a message to the Kafka broker as <code>CN=producer</code>:</p> <pre><code>kafka-console-producer.sh \\\n  --broker-list :9093 \\\n  --topic ssl \\\n  --producer.config /tmp/kafka-ssl-demo/producer.properties\n</code></pre> <p>In <code>logs/kafka-authorizer.log</code> you should find the following:</p> <pre><code>DEBUG operation = Write on resource = Topic:LITERAL:ssl from host = 127.0.0.1 is Allow based on acl = User:CN=producer has Allow permission for operations: Write from hosts: * (kafka.authorizer.logger)\nDEBUG Principal = User:CN=producer is Allowed Operation = Describe from host = 127.0.0.1 on resource = Topic:LITERAL:ssl for request = Metadata with resourceRefCount = 1 (kafka.authorizer.logger)\nDEBUG operation = Write on resource = Topic:LITERAL:ssl from host = 127.0.0.1 is Allow based on acl = User:CN=producer has Allow permission for operations: Write from hosts: * (kafka.authorizer.logger)\nDEBUG Principal = User:CN=producer is Allowed Operation = Write from host = 127.0.0.1 on resource = Topic:LITERAL:ssl for request = Produce with resourceRefCount = 1 (kafka.authorizer.logger)\n</code></pre>"},{"location":"demo/acl-authorization/#consume-messages","title":"Consume Messages","text":"<p>Create <code>consumer.properties</code> file as a minimal configuration of a Kafka client to use SSL authentication and identify itself as <code>CN=consumer</code>:</p> <pre><code>security.protocol=SSL\nssl.truststore.location=/tmp/kafka-ssl-demo/client.truststore\nssl.truststore.password=123456\nssl.keystore.location=/tmp/kafka-ssl-demo/consumer.keystore\nssl.keystore.password=123456\nssl.key.password=123456\n</code></pre> <p>Use <code>kafka-console-consumer.sh</code> utility to consume messages as <code>CN=consumer</code>:</p> <pre><code>kafka-console-consumer.sh \\\n  --bootstrap-server :9093 \\\n  --topic ssl \\\n  --consumer.config /tmp/kafka-ssl-demo/consumer.properties\n</code></pre> <p>In <code>logs/kafka-authorizer.log</code> you should find the following:</p> <pre><code>DEBUG operation = Read on resource = Topic:LITERAL:ssl from host = 127.0.0.1 is Allow based on acl = User:CN=consumer has Allow permission for operations: Read from hosts: * (kafka.authorizer.logger)\nDEBUG Principal = User:CN=consumer is Allowed Operation = Read from host = 127.0.0.1 on resource = Topic:LITERAL:ssl for request = Fetch with resourceRefCount = 1 (kafka.authorizer.logger)\n</code></pre> <p>That's all for the demo. Thanks for reading!</p>"},{"location":"demo/controller-election/","title":"Demo: Controller Election","text":"<p>Use the following setup with one Zookeeper server and two Kafka brokers to observe the Kafka controller election.</p> <p>Start Zookeeper server.</p> <pre><code>$ ./bin/zookeeper-server-start.sh config/zookeeper.properties\n...\nINFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)\n</code></pre> <p>Add the following line to <code>config/log4j.properties</code> to enable <code>DEBUG</code> logging level for <code>kafka.controller.KafkaController</code> logger.</p> <pre><code>log4j.logger.kafka.controller.KafkaController=DEBUG\n</code></pre> <p>Start a Kafka broker.</p> <pre><code>$ ./bin/kafka-server-start.sh config/server.properties \\\n    --override broker.id=100 \\\n    --override log.dirs=/tmp/kafka-logs-100 \\\n    --override port=9192\n...\nINFO Registered broker 100 at path /brokers/ids/100 with addresses: EndPoint(192.168.1.4,9192,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.utils.ZkUtils)\nINFO Kafka version : 1.0.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser)\nINFO Kafka commitId : 852297efd99af04d (org.apache.kafka.common.utils.AppInfoParser)\nINFO [KafkaServer id=100] started (kafka.server.KafkaServer)\n</code></pre> <p>Start another Kafka broker (with different properties)</p> <pre><code>$ ./bin/kafka-server-start.sh config/server.properties \\\n    --override broker.id=200 \\\n    --override log.dirs=/tmp/kafka-logs-200 \\\n    --override port=9292\n...\nINFO Registered broker 200 at path /brokers/ids/200 with addresses: EndPoint(192.168.1.4,9292,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.utils.ZkUtils)\nINFO Kafka version : 1.0.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser)\nINFO Kafka commitId : 852297efd99af04d (org.apache.kafka.common.utils.AppInfoParser)\nINFO [KafkaServer id=200] started (kafka.server.KafkaServer)\n</code></pre> <p>Connect to Zookeeper using Zookeeper CLI (command-line interface).</p> <p>Tip</p> <p>Use the official distribution of Apache Zookeeper.</p> <p>The zookeeper shell shipped with Kafka works with no support for command line history (because jline jar is missing, cf. KAFKA-2385).</p> <pre><code>$ ./bin/zkCli.sh -server localhost:2181\n</code></pre> <p>Once connected, execute <code>get /controller</code> to get the data associated with <code>/controller</code> znode where the active Kafka controller stores the controller ID.</p> <pre><code>[zk: localhost:2181(CONNECTED) 0] get /controller\n{\"version\":1,\"brokerid\":100,\"timestamp\":\"1506423376977\"}\ncZxid = 0x191\nctime = Tue Sep 26 12:56:16 CEST 2017\nmZxid = 0x191\nmtime = Tue Sep 26 12:56:16 CEST 2017\npZxid = 0x191\ncversion = 0\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0x15ebdd241840002\ndataLength = 56\nnumChildren = 0\n</code></pre> <p>Tip</p> <p>Clear the consoles of the two Kafka brokers so you have the election logs only.</p> <p>Delete <code>/controller</code> znode and observe the controller election.</p> <pre><code>[zk: localhost:2181(CONNECTED) 2] delete /controller\n</code></pre> <p>You should see the following in the logs in the consoles of the two Kafka brokers.</p> <pre><code>DEBUG [Controller id=100] Resigning (kafka.controller.KafkaController)\nDEBUG [Controller id=100] De-registering IsrChangeNotificationListener (kafka.controller.KafkaController)\nDEBUG [Controller id=100] De-registering logDirEventNotificationListener (kafka.controller.KafkaController)\nINFO [Controller id=100] Resigned (kafka.controller.KafkaController)\nDEBUG [Controller id=100] Broker 200 has been elected as the controller, so stopping the election process. (kafka.controller.KafkaController)\n</code></pre> <p>and</p> <pre><code>INFO Creating /controller (is it secure? false) (kafka.utils.ZKCheckedEphemeral)\nINFO Result of znode creation is: OK (kafka.utils.ZKCheckedEphemeral)\nINFO [Controller id=200] 200 successfully elected as the controller (kafka.controller.KafkaController)\nINFO [Controller id=200] Starting become controller state transition (kafka.controller.KafkaController)\nINFO [Controller id=200] Initialized controller epoch to 39 and zk version 38 (kafka.controller.KafkaController)\nINFO [Controller id=200] Incremented epoch to 40 (kafka.controller.KafkaController)\nDEBUG [Controller id=200] Registering IsrChangeNotificationListener (kafka.controller.KafkaController)\nDEBUG [Controller id=200] Registering logDirEventNotificationListener (kafka.controller.KafkaController)\nINFO [Controller id=200] Partitions being reassigned: Map() (kafka.controller.KafkaController)\nINFO [Controller id=200] Partitions already reassigned: Set() (kafka.controller.KafkaController)\nINFO [Controller id=200] Resuming reassignment of partitions: Map() (kafka.controller.KafkaController)\nINFO [Controller id=200] Currently active brokers in the cluster: Set(100, 200) (kafka.controller.KafkaController)\nINFO [Controller id=200] Currently shutting brokers in the cluster: Set() (kafka.controller.KafkaController)\nINFO [Controller id=200] Current list of topics in the cluster: Set(my-topic2, NEW, my-topic, my-topic1) (kafka.controller.KafkaController)\nINFO [Controller id=200] List of topics to be deleted:  (kafka.controller.KafkaController)\nINFO [Controller id=200] List of topics ineligible for deletion:  (kafka.controller.KafkaController)\nINFO [Controller id=200] Ready to serve as the new controller with epoch 40 (kafka.controller.KafkaController)\nINFO [Controller id=200] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController)\nINFO [Controller id=200] Partitions that completed preferred replica election:  (kafka.controller.KafkaController)\nINFO [Controller id=200] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController)\nINFO [Controller id=200] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController)\nINFO [Controller id=200] Starting preferred replica leader election for partitions  (kafka.controller.KafkaController)\nINFO [Controller id=200] Starting the controller scheduler (kafka.controller.KafkaController)\n</code></pre>"},{"location":"demo/kafka-and-kcat-in-docker/","title":"Demo: Kafka and kcat in Docker","text":"<p>This demo uses Docker to run Apache Kafka and kcat utility.</p>"},{"location":"demo/kafka-and-kcat-in-docker/#kafka-docker","title":"kafka-docker","text":"<p>Pull kafka-docker project (or create a <code>docker-compose.yml</code> file yourself).</p>"},{"location":"demo/kafka-and-kcat-in-docker/#running-kafka-cluster","title":"Running Kafka Cluster","text":"<p>Start Zookeeper and Kafka containers.</p> <pre><code>docker-compose up\n</code></pre> <pre><code>$ docker-compose ps\n          Name                        Command               State                                  Ports\n----------------------------------------------------------------------------------------------------------------------------------------\nkafka-docker_kafka_1       start-kafka.sh                   Up      0.0.0.0:62687-&gt;9092/tcp\nkafka-docker_zookeeper_1   /bin/sh -c /usr/sbin/sshd  ...   Up      0.0.0.0:2181-&gt;2181/tcp,:::2181-&gt;2181/tcp, 22/tcp, 2888/tcp, 3888/tcp\n</code></pre>"},{"location":"demo/kafka-and-kcat-in-docker/#docker-network","title":"Docker Network","text":"<p>The above creates a Docker network <code>kafka-docker_default</code> (if ran from <code>kafka-docker</code> directory as described in the official documentation of docker-compose).</p> <pre><code>$ docker network ls\nNETWORK ID     NAME                   DRIVER    SCOPE\nb8b255710858   bridge                 bridge    local\n3c9c3a969ef2   cda                    bridge    local\n398f9f3196aa   host                   host      local\n68611503fde8   kafka-docker_default   bridge    local\ndb43a5e50281   none                   null      local\n</code></pre>"},{"location":"demo/kafka-and-kcat-in-docker/#kcat","title":"kcat","text":"<p>Connect <code>kcat</code> container to the network (using <code>--network</code> option as described in the official documentation of docker-compose).</p>"},{"location":"demo/kafka-and-kcat-in-docker/#metadata-listing","title":"Metadata Listing","text":"<pre><code>docker run -it --rm \\\n  --network kafka-docker_default \\\n  edenhill/kcat:1.7.0 \\\n  -b kafka-docker_kafka_1:9092 -L\n</code></pre> <pre><code>Metadata for all topics (from broker -1: kafka-docker_kafka_1:9092/bootstrap):\n 1 brokers:\n  broker 1001 at 09cc8de4d067:9092 (controller)\n 0 topics:\n</code></pre>"},{"location":"demo/kafka-and-kcat-in-docker/#producer","title":"Producer","text":"<pre><code>docker run -it --rm \\\n  --network kafka-docker_default \\\n  --name producer \\\n  edenhill/kcat:1.7.0 \\\n  -b kafka-docker_kafka_1:9092 -P -t t1\n</code></pre> <p>Caution</p> <p>For some reason the above command couldn't send messages whenever I pressed ENTER but expected <code>Ctrl+D</code> instead (that terminates the shell and the container). Switching to confluentinc/cp-kafkacat made things working fine.</p> <pre><code>docker run -it --rm \\\n  --network kafka-docker_default \\\n  --name producer \\\n  confluentinc/cp-kafkacat \\\n  kafkacat \\\n  -b kafka-docker_kafka_1:9092 -P -t t1\n</code></pre>"},{"location":"demo/kafka-and-kcat-in-docker/#consumer","title":"Consumer","text":"<pre><code>docker run -it --rm \\\n  --network kafka-docker_default \\\n  --name consumer \\\n  edenhill/kcat:1.7.0 \\\n  -b kafka-docker_kafka_1:9092 -C -t t1\n</code></pre>"},{"location":"demo/kafka-and-kcat-in-docker/#clean-up","title":"Clean Up","text":"<pre><code>docker-compose down\n</code></pre>"},{"location":"demo/partition-leader-election/","title":"Demo: Using kafka-leader-election for Partition Leader Election","text":""},{"location":"demo/partition-leader-election/#review-me","title":"Review Me","text":"<p>The demo shows how to use link:kafka-tools-kafka-leader-election.adoc[kafka-leader-election.sh] command-line utility for link:kafka-partition-leader-election.adoc[Partition Leader Election].</p> <p>The demo is made up of the following steps:</p> <p>. &lt;&gt; <p>. &lt;&gt; <p>. &lt;&gt; <p>. &lt;&gt; <p>=== [[step-1]] Start Kafka Cluster</p> <p>You'll be using https://github.com/wurstmeister/kafka-docker[kafka-docker] project to run Apache Kafka in Docker (using Docker Compose).</p> <pre><code>docker-compose up -d --scale=kafka=3\n</code></pre> <p>The goal is to run 3 brokers as easily as possible (let me know if you know a simpler way. Thanks!).</p> <p>=== [[step-2]] Create Topic</p> <p>In this step you'll create a topic with 3 partitions (one for every broker in the cluster) and 2 replicas.</p> <p>Log in to <code>kafka-docker_kafka_1</code> broker first.</p> <pre><code>docker exec -it kafka-docker_kafka_1 /bin/bash\n</code></pre> <pre><code>kafka-topics.sh \\\n    --bootstrap-server :9092 \\\n    --create \\\n    --topic demo-kafka-leader-election \\\n    --partitions 3 \\\n    --replication-factor 2\n</code></pre> <p>Review the leader and ISR brokers. Yours may be different.</p> <pre><code>$ kafka-topics.sh \\\n    --bootstrap-server :9092 \\\n    --describe \\\n    --topic demo-kafka-leader-election\nTopic: demo-kafka-leader-election   PartitionCount: 3   ReplicationFactor: 2    Configs: segment.bytes=1073741824\n    Topic: demo-kafka-leader-election   Partition: 0    Leader: 1002    Replicas: 1002,1001 Isr: 1002,1001\n    Topic: demo-kafka-leader-election   Partition: 1    Leader: 1001    Replicas: 1001,1003 Isr: 1001,1003\n    Topic: demo-kafka-leader-election   Partition: 2    Leader: 1003    Replicas: 1003,1002 Isr: 1003,1002\n</code></pre> <p>=== [[step-3]] Shutting Down Leader Broker</p> <p>In this step, you're going to shut down a leader broker to simulate a broker failure. That triggers a leader election and promote the other broker (in the ISR list) to become the leader.</p> <pre><code>docker stop kafka-docker_kafka_2\n</code></pre> <p>You should have 2 brokers up and running.</p> <pre><code>$ docker ps\nCONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS                                                NAMES\nf7224b2bdf90        kafka-docker_kafka       \"start-kafka.sh\"         2 minutes ago       Up 2 minutes        0.0.0.0:32785-&gt;9092/tcp                              kafka-docker_kafka_3\n52ac4b64648e        wurstmeister/zookeeper   \"/bin/sh -c '/usr/sb\u2026\"   2 minutes ago       Up 2 minutes        22/tcp, 2888/tcp, 3888/tcp, 0.0.0.0:2181-&gt;2181/tcp   kafka-docker_zookeeper_1\nad05db816dc1        kafka-docker_kafka       \"start-kafka.sh\"         2 minutes ago       Up 2 minutes        0.0.0.0:32784-&gt;9092/tcp                              kafka-docker_kafka_1\n</code></pre> <p>Review the leaders.</p> <pre><code>$ kafka-topics.sh \\\n    --bootstrap-server :9092 \\\n    --describe \\\n    --topic demo-kafka-leader-election\nTopic: demo-kafka-leader-election   PartitionCount: 3   ReplicationFactor: 2    Configs: segment.bytes=1073741824\n    Topic: demo-kafka-leader-election   Partition: 0    Leader: 1001    Replicas: 1002,1001 Isr: 1001\n    Topic: demo-kafka-leader-election   Partition: 1    Leader: 1001    Replicas: 1001,1003 Isr: 1001,1003\n    Topic: demo-kafka-leader-election   Partition: 2    Leader: 1003    Replicas: 1003,1002 Isr: 1003\n</code></pre> <p>=== [[step-4]] Trigger Preferred Leader Election</p> <p>In this step, you will trigger link:kafka-tools-kafka-leader-election.adoc#preferred-partition-leader-election[preferred leader election] for the partitions with the other leader elected (non-preferred leader).</p> <p>Let's bring the stopped broker up.</p> <pre><code>$ docker start kafka-docker_kafka_2\nkafka-docker_kafka_2\n</code></pre> <p>Review the leaders.</p> <pre><code>$ kafka-topics.sh \\\n    --bootstrap-server :9092 \\\n    --describe \\\n    --topic demo-kafka-leader-election\nTopic: demo-kafka-leader-election   PartitionCount: 3   ReplicationFactor: 2    Configs: segment.bytes=1073741824\n    Topic: demo-kafka-leader-election   Partition: 0    Leader: 1001    Replicas: 1002,1001 Isr: 1001,1002\n    Topic: demo-kafka-leader-election   Partition: 1    Leader: 1001    Replicas: 1001,1003 Isr: 1001,1003\n    Topic: demo-kafka-leader-election   Partition: 2    Leader: 1003    Replicas: 1003,1002 Isr: 1003,1002\n</code></pre> <p>Time to fix the partition <code>0</code> so the broker <code>1002</code> is the leader again.</p> <pre><code>kafka-leader-election.sh \\\n  --bootstrap-server :9092 \\\n  --topic demo-kafka-leader-election \\\n  --partition 0 \\\n  --election-type preferred\n</code></pre> <p>You should see the following message for a successful election.</p> <pre><code>Successfully completed leader election (PREFERRED) for partitions demo-kafka-leader-election-0\n</code></pre> <p>Review the leaders.</p> <pre><code>$ kafka-topics.sh \\\n    --bootstrap-server :9092 \\\n    --describe \\\n    --topic demo-kafka-leader-election\nTopic: demo-kafka-leader-election   PartitionCount: 3   ReplicationFactor: 2    Configs: segment.bytes=1073741824\n    Topic: demo-kafka-leader-election   Partition: 0    Leader: 1002    Replicas: 1002,1001 Isr: 1001,1002\n    Topic: demo-kafka-leader-election   Partition: 1    Leader: 1001    Replicas: 1001,1003 Isr: 1001,1003\n    Topic: demo-kafka-leader-election   Partition: 2    Leader: 1003    Replicas: 1003,1002 Isr: 1003,1002\n</code></pre>"},{"location":"demo/secure-inter-broker-communication/","title":"Demo: Secure Inter-Broker Communication","text":"<p>The demo shows how to set up a secure communication between brokers (and disable the unsecure plaintext listener altogether). That will make Kafka brokers available via TLS/SSL only.</p>"},{"location":"demo/secure-inter-broker-communication/#before-you-begin","title":"Before You Begin","text":"<p>The demo is a follow-up to Demo: Securing Communication Between Clients and Brokers Using SSL. Please finish it first before this demo.</p>"},{"location":"demo/secure-inter-broker-communication/#configure-broker-to-trust-certificate-authority","title":"Configure Broker to Trust Certificate Authority","text":"<p>Import the certificate of the certificate authority (CA) to a broker truststore so the brokers can trust it (when a broker tries to connect using SSL).</p> <pre><code>$ keytool \\\n  -import \\\n  -file ca.crt \\\n  -keystore server.truststore \\\n  -alias ca \\\n  -storepass 123456 \\\n  -noprompt\nCertificate was added to keystore\n</code></pre> <p>Use <code>keytool</code> to print out the certificates in the client keystore.</p> <pre><code>keytool -list -v -keystore server.truststore -storepass 123456\n</code></pre> <p>There should be 1 entry for the CA.</p> <pre><code>$ keytool -list -v -keystore server.truststore -storepass 123456\nKeystore type: PKCS12\nKeystore provider: SUN\n\nYour keystore contains 1 entry\n\nAlias name: ca\n# ...removed for brevity\n</code></pre>"},{"location":"demo/secure-inter-broker-communication/#enable-ssl-for-inter-broker-communication","title":"Enable SSL for Inter-Broker Communication","text":"<p>Edit <code>config/server-ssl.properties</code> and add the following configuration properties to enable SSL for inter-broker communication:</p> <pre><code>security.inter.broker.protocol=SSL\nssl.truststore.location=/tmp/kafka-ssl-demo/server.truststore\nssl.truststore.password=123456\n</code></pre> <p>Start the broker(s).</p> <pre><code>./bin/kafka-server-start.sh config/server-ssl.properties\n</code></pre> <p>Tip</p> <p>Use <code>export KAFKA_OPTS=-Djavax.net.debug=all</code> to debug SSL issues.</p> <p>Verify the SSL configuration of the broker. The following uses the Cryptography and SSL/TLS Toolkit (OpenSSL) and the client tool.</p> <pre><code>openssl s_client -connect localhost:9093\n</code></pre>"},{"location":"demo/secure-inter-broker-communication/#disable-plaintext-unsecure-listener","title":"Disable Plaintext Unsecure Listener","text":"<p>Edit <code>config/server-ssl.properties</code> and change <code>listeners</code> property to use <code>SSL://:9093</code> only:</p> <pre><code>listeners=SSL://:9093\n</code></pre> <p>Start the broker(s).</p> <pre><code>./bin/kafka-server-start.sh config/server-ssl.properties\n</code></pre> <p>Tip</p> <p>Use <code>export KAFKA_OPTS=-Djavax.net.debug=all</code> to debug SSL-related issues.</p> <p>Verify the SSL configuration of the broker. The following uses the Cryptography and SSL/TLS Toolkit (OpenSSL) and the client tool.</p> <pre><code>openssl s_client -connect localhost:9093\n</code></pre> <p>Enter <code>Ctrl-C</code> to close the session.</p> <p>That's all for the demo. I hope you enjoyed it!</p>"},{"location":"demo/securing-communication-between-clients-and-brokers/","title":"Demo: Securing Communication Between Clients and Brokers Using SSL","text":"<p>The demo shows how to configure secure communication between Kafka clients (consumers and producers) and brokers using Transport Layer Security (TLS) (and its now-deprecated predecessor, Secure Sockets Layer (SSL)).</p> <p>Note</p> <p>TLS is a newer variant of SSL, but because of the term <code>SSL</code> is used in many Kafka configuration properties the document will use <code>SSL</code> (over <code>TLS</code>) to refer to the protocol used for secure communication.</p>"},{"location":"demo/securing-communication-between-clients-and-brokers/#create-directory-of-ssl-files","title":"Create Directory of SSL Files","text":"<p>Create a directory for SSL files.</p> <pre><code>mkdir -p /tmp/kafka-ssl-demo\n</code></pre> <p>All the following commands are executed in the directory.</p> <pre><code>cd /tmp/kafka-ssl-demo\n</code></pre>"},{"location":"demo/securing-communication-between-clients-and-brokers/#create-certificate-authority-ca","title":"Create Certificate Authority (CA)","text":"<p>Certificate Authority</p> <p>From Wikipedia:</p> <p>In cryptography, a certificate authority or certification authority (CA) is an entity that issues digital certificates.</p> <p>A digital certificate certifies the ownership of a public key by the named subject of the certificate.</p> <p>A CA acts as a trusted third party \u2014 trusted both by the subject (owner) of the certificate and by the party relying upon the certificate. The format of these certificates is specified by the X.509 standard.</p> <p>Simply put, a certificate authority is used to sign certificates (with public keys).</p> <p>Generate a private key and a self-signed certificate for the CA.</p> <pre><code>// openssl genrsa -out ca.key\n// openssl req -new -x509 -key ca.key -out ca.crt\n$ openssl req \\\n  -new \\\n  -x509 \\\n  -days 365 \\\n  -keyout ca.key \\\n  -out ca.crt \\\n  -subj \"/C=PL/L=Warsaw/CN=Certificate Authority\" \\\n  -passout pass:1234\nGenerating a RSA private key\n............................+++++\n.......................+++++\nwriting new private key to 'ca.key'\n-----\n</code></pre> <p>You should have the following files in the directory:</p> <ul> <li><code>ca.key</code> - the private key of the certificate authority</li> <li><code>ca.crt</code> - the certificate (with the public key) of the certificate authority</li> </ul> <pre><code>$ tree\n.\n\u251c\u2500\u2500 ca.crt\n\u2514\u2500\u2500 ca.key\n</code></pre> <p>Use the following command to print certificate contents:</p> <pre><code>$ openssl x509 -text -noout -in ca.crt\nCertificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number:\n            5f:a1:fb:63:86:36:d1:e4:c3:f4:46:93:2e:0f:f8:34:26:6c:79:39\n        Signature Algorithm: sha256WithRSAEncryption\n        Issuer: C = PL, L = Warsaw, CN = Certificate Authority\n        Subject: C = PL, L = Warsaw, CN = Certificate Authority\n# ... removed for clarity\n</code></pre> <p>Since <code>ca.crt</code> is a self-signed certificate, the <code>Issuer</code> and <code>Subject</code> sections use the same subject.</p>"},{"location":"demo/securing-communication-between-clients-and-brokers/#generate-ssl-keys-and-certificate-for-kafka-brokers","title":"Generate SSL Keys and Certificate for Kafka Brokers","text":"<p>Generate the keys and certificate for every Kafka broker in a cluster. This time you'll be using <code>keytool</code> utility that comes with Java Platform, Standard Edition (refer to keytool).</p> <pre><code>// openssl genrsa -out server.key\n$ keytool \\\n  -genkey \\\n  -keystore server.keystore \\\n  -alias localhost \\\n  -dname CN=localhost \\\n  -keyalg RSA \\\n  -validity 365 \\\n  -ext san=dns:localhost \\\n  -storepass 123456\n</code></pre> <p>Tip</p> <p>If you want to use a Subject Alternative Name (SAN) in your certificate, add the following options to the <code>keytool</code> command line:</p> <ul> <li>Fully-qualified domain name (FQDN): <code>\u2013ext san=dns:servername.domain.com</code></li> <li>IP address: <code>\u2013ext san=ip:192.168.10.1</code></li> </ul> <p>DistinguishedName or SubjectAltNames should be the fully-qualified domain name of a broker.</p> <p>You should now have one more file in the directory:</p> <ul> <li><code>server.keystore</code> - the keystore with the private key and the certificate of the broker</li> </ul> <pre><code>$ tree\n.\n\u251c\u2500\u2500 ca.crt\n\u251c\u2500\u2500 ca.key\n\u2514\u2500\u2500 server.keystore\n</code></pre> <p>Use <code>keytool</code> to print out the content of the keystore.</p> <pre><code>keytool -list -v -keystore server.keystore -storepass 123456\n</code></pre> <p>The keystore should contain 1 entry for the alias <code>localhost</code>.</p> <pre><code>$ keytool -list -v -keystore server.keystore -storepass 123456\nKeystore type: PKCS12\nKeystore provider: SUN\n\nYour keystore contains 1 entry\n\nAlias name: localhost\n# ... removed for clarity\n</code></pre>"},{"location":"demo/securing-communication-between-clients-and-brokers/#sign-broker-certificate-using-ca","title":"Sign Broker Certificate (Using CA)","text":"<p>Create a certificate signing request (CSR).</p> <p>Export the server certificate from <code>server.keystore</code>.</p> <pre><code>// openssl req -new -key server.key -out server.csr\n$ keytool \\\n  -certreq \\\n  -keystore server.keystore \\\n  -alias localhost \\\n  -file server.unsigned.crt \\\n  -storepass 123456\n</code></pre> <p>You should now have one more file in the directory:</p> <ul> <li><code>server.unsigned.crt</code> - a certificate signing request of the server certificate</li> </ul> <pre><code>$ tree\n.\n\u251c\u2500\u2500 ca.crt\n\u251c\u2500\u2500 ca.key\n\u251c\u2500\u2500 server.keystore\n\u2514\u2500\u2500 server.unsigned.crt\n</code></pre> <p>Sign the certificate signing request (<code>server.unsigned.crt</code>) with the root CA.</p> <pre><code>$ openssl x509 \\\n  -req \\\n  -CA ca.crt \\\n  -CAkey ca.key \\\n  -in server.unsigned.crt \\\n  -out server.crt \\\n  -days 365 \\\n  -CAcreateserial \\\n  -passin pass:1234\nSignature ok\nsubject=CN = localhost\nGetting CA Private Key\n</code></pre> <p>You should have the following files in the directory:</p> <ul> <li><code>ca.srl</code></li> <li><code>server.crt</code> - the signed certificate of the broker</li> </ul> <pre><code>$ tree\n.\n\u251c\u2500\u2500 ca.crt\n\u251c\u2500\u2500 ca.key\n\u251c\u2500\u2500 ca.srl\n\u251c\u2500\u2500 server.crt\n\u251c\u2500\u2500 server.keystore\n\u2514\u2500\u2500 server.unsigned.crt\n</code></pre>"},{"location":"demo/securing-communication-between-clients-and-brokers/#import-certificates-to-broker-keystore","title":"Import Certificates to Broker Keystore","text":"<p>Create a SSL keystore for the Kafka broker. Each broker gets its own unique keystore.</p> <p>Import the certificate of the CA into the broker keystore.</p> <pre><code>$ keytool \\\n  -import \\\n  -file ca.crt \\\n  -keystore server.keystore \\\n  -alias ca \\\n  -storepass 123456 \\\n  -noprompt\nCertificate was added to keystore\n</code></pre> <p>Import the signed certificate into the broker keystore. Make sure to use the same <code>-alias</code> as you used ealier.</p> <pre><code>$ keytool \\\n  -import \\\n  -file server.crt \\\n  -keystore server.keystore \\\n  -alias localhost \\\n  -storepass 123456 \\\n  -noprompt\nCertificate reply was installed in keystore\n</code></pre> <p>Use <code>keytool</code> to print out the certificates in the broker keystore.</p> <pre><code>keytool -list -v -keystore server.keystore -storepass 123456\n</code></pre> <p>There should be 2 entries (one for the CA and another for the broker itself).</p> <pre><code>$ keytool -list -v -keystore server.keystore -storepass 123456\n\nKeystore type: PKCS12\nKeystore provider: SUN\n\nYour keystore contains 2 entries\n\nAlias name: ca\n# ... removed from clarity\nAlias name: localhost\n# ... removed from clarity\n</code></pre>"},{"location":"demo/securing-communication-between-clients-and-brokers/#configure-ssl-on-kafka-broker","title":"Configure SSL on Kafka Broker","text":"<p>Create <code>config/server-ssl.properties</code> (based on <code>config/server.properties</code>) and add the following configuration properties to enable SSL:</p> <pre><code>listeners=PLAINTEXT://:9092,SSL://:9093\nssl.keystore.location=/tmp/kafka-ssl-demo/server.keystore\nssl.keystore.password=123456\nssl.key.password=123456\n</code></pre> <p>Start the broker(s).</p> <pre><code>./bin/kafka-server-start.sh config/server-ssl.properties\n</code></pre> <p>Tip</p> <p>Use <code>export KAFKA_OPTS=-Djavax.net.debug=all</code> to debug SSL issues.</p> <p>Verify the SSL configuration of the broker. The following uses the Cryptography and SSL/TLS Toolkit (OpenSSL) and the client tool.</p> <pre><code>openssl s_client -connect localhost:9093\n</code></pre> <p>The tool should print out the certificate chain of the broker (a chain of the subjects and the issuers). At the end, you should find the following <code>Verify return code</code>:</p> <pre><code>Verify return code: 19 (self signed certificate in certificate chain)\n</code></pre> <p>Enter <code>Ctrl-C</code> to close the session.</p> <p>Use the client tool with <code>-CAfile</code> option to trust the CA certificate.</p> <pre><code>openssl s_client -connect localhost:9093 -CAfile /tmp/kafka-ssl-demo/ca.crt\n</code></pre> <p>With the change, you should find the following <code>Verify return code</code>:</p> <pre><code>Verify return code: 0 (ok)\n</code></pre> <p>Enter <code>Ctrl-C</code> to close the session.</p>"},{"location":"demo/securing-communication-between-clients-and-brokers/#import-ca-certificate-to-client-truststore","title":"Import CA Certificate to Client Truststore","text":"<p>Add the CA certificate <code>ca.crt</code> to a client truststore for the clients to trust this CA.</p> <pre><code>$ keytool \\\n  -import \\\n  -file ca.crt \\\n  -keystore client.truststore \\\n  -alias ca \\\n  -storepass 123456 \\\n  -noprompt\nCertificate was added to keystore\n</code></pre> <p>Use <code>keytool</code> to print out the certificates in the client keystore.</p> <pre><code>keytool -list -v -keystore client.truststore -storepass 123456\n</code></pre> <p>There should be 1 entry for the CA.</p> <pre><code>$ keytool -list -v -keystore client.truststore -storepass 123456\n\nKeystore type: PKCS12\nKeystore provider: SUN\n\nYour keystore contains 1 entry\n\nAlias name: ca\n# ... removed for brevity\n</code></pre>"},{"location":"demo/securing-communication-between-clients-and-brokers/#configure-ssl-for-kafka-clients","title":"Configure SSL for Kafka Clients","text":"<p>Create <code>/tmp/kafka-ssl-demo/client-ssl.properties</code> as a minimal configuration of a Kafka client to use SSL:</p> <pre><code>security.protocol=SSL\nssl.truststore.location=/tmp/kafka-ssl-demo/client.truststore\nssl.truststore.password=123456\n</code></pre> <p>Use <code>kafka-console-producer.sh</code> utility to send records to Kafka brokers over SSL:</p> <pre><code>./bin/kafka-console-producer.sh \\\n  --broker-list :9093 \\\n  --topic ssl \\\n  --producer.config /tmp/kafka-ssl-demo/client-ssl.properties\n</code></pre> <p>That's all for the demo. I hope you enjoyed it!</p>"},{"location":"demo/ssl-authentication/","title":"Demo: SSL Authentication","text":"<p>The demo shows how to use SSL/TLS for authentication so no connection can be established between Kafka clients (consumers and producers) and brokers unless a valid and trusted certificate is provided.</p>"},{"location":"demo/ssl-authentication/#before-you-begin","title":"Before You Begin","text":"<p>The demo is a follow-up to Demo: Secure Inter-Broker Communication. Please finish it first before this demo.</p>"},{"location":"demo/ssl-authentication/#generate-certificate-for-client-authentication","title":"Generate Certificate for Client Authentication","text":"<p>Generate the keys and certificate of a Kafka client to be authenticated as jacek.</p> <pre><code>keytool \\\n  -genkey \\\n  -keystore jacek.keystore \\\n  -alias jacek \\\n  -dname CN=jacek \\\n  -keyalg RSA \\\n  -validity 365 \\\n  -storepass 123456\n</code></pre> <p>You should now have one more file in the directory:</p> <ul> <li><code>jacek.keystore</code> - the keystore with the private key and the certificate of the user</li> </ul> <p>Use <code>keytool</code> to print out the content of the keystore.</p> <pre><code>keytool -list -v -keystore jacek.keystore -storepass 123456\n</code></pre> <p>The keystore should contain 1 entry for the alias <code>jacek</code>.</p>"},{"location":"demo/ssl-authentication/#sign-client-certificate-using-ca","title":"Sign Client Certificate (Using CA)","text":"<p>Create a certificate signing request (CSR).</p> <p>Export the client certificate from <code>jacek.keystore</code>.</p> <pre><code>keytool \\\n  -certreq \\\n  -keystore jacek.keystore \\\n  -alias jacek \\\n  -file jacek.unsigned.crt \\\n  -storepass 123456\n</code></pre> <p>Sign the certificate signing request (<code>jacek.unsigned.crt</code>) with the root CA.</p> <pre><code>$ openssl x509 \\\n  -req \\\n  -CA ca.crt \\\n  -CAkey ca.key \\\n  -in jacek.unsigned.crt \\\n  -out jacek.crt \\\n  -days 365 \\\n  -CAcreateserial \\\n  -passin pass:1234\nSignature ok\nsubject=CN = jacek\nGetting CA Private Key\n</code></pre> <p>You should have the following file in the directory:</p> <ul> <li><code>jacek.crt</code> - the signed certificate of the user</li> </ul>"},{"location":"demo/ssl-authentication/#import-certificates-to-client-keystore","title":"Import Certificates to Client Keystore","text":"<p>Create a SSL keystore for the Kafka client. Each client gets its own unique keystore.</p> <p>Import the certificate of the CA into the client keystore.</p> <pre><code>$ keytool \\\n  -import \\\n  -file ca.crt \\\n  -keystore jacek.keystore \\\n  -alias ca \\\n  -storepass 123456 \\\n  -noprompt\nCertificate was added to keystore\n</code></pre> <p>Import the signed certificate into the client keystore. Make sure to use the same <code>-alias</code> as you used ealier.</p> <pre><code>$ keytool \\\n  -import \\\n  -file jacek.crt \\\n  -keystore jacek.keystore \\\n  -alias jacek \\\n  -storepass 123456 \\\n  -noprompt\nCertificate reply was installed in keystore\n</code></pre> <p>Use <code>keytool</code> to print out the certificates in the client keystore.</p> <pre><code>keytool -list -v -keystore jacek.keystore -storepass 123456\n</code></pre> <p>There should be 2 entries (one for the CA and another for the client itself).</p>"},{"location":"demo/ssl-authentication/#require-client-authorization-using-ssl-on-kafka-brokers","title":"Require Client Authorization Using SSL on Kafka Brokers","text":"<p>Enable SSL authentication (require client authentication using SSL certificates).</p> <p>Edit <code>config/server-ssl.properties</code> and add the following configuration property:</p> <pre><code>ssl.client.auth=required\n</code></pre> <p>Start the broker(s).</p> <pre><code>./bin/kafka-server-start.sh config/server-ssl.properties\n</code></pre> <p>Tip</p> <p>Use <code>export KAFKA_OPTS=-Djavax.net.debug=all</code> to debug SSL-related issues.</p> <p>Verify the SSL configuration of the broker. The following uses the Cryptography and SSL/TLS Toolkit (OpenSSL) and the client tool.</p> <pre><code>openssl s_client -connect localhost:9093\n</code></pre> <p>The client tool will quit immediately since the broker requires clients to provide valid certificates. You should find the following INFO message in the broker logs:</p> <pre><code>[SocketServer brokerId=0] Failed authentication with /0:0:0:0:0:0:0:1 (SSL handshake failed)\n</code></pre>"},{"location":"demo/ssl-authentication/#configure-ssl-authentication-for-kafka-client","title":"Configure SSL Authentication for Kafka Client","text":"<p>Use the following <code>jacek-client.properties</code> as a minimal configuration of a Kafka client to use SSL authentication:</p> <pre><code>security.protocol=SSL\nssl.truststore.location=/tmp/kafka-ssl-demo/client.truststore\nssl.truststore.password=123456\nssl.keystore.location=/tmp/kafka-ssl-demo/jacek.keystore\nssl.keystore.password=123456\nssl.key.password=123456\n</code></pre> <p>Use <code>kafka-console-producer.sh</code> utility to send records to Kafka brokers over SSL:</p> <pre><code>kafka-console-producer.sh \\\n  --broker-list :9093 \\\n  --topic ssl \\\n  --producer.config /tmp/kafka-ssl-demo/jacek-client.properties\n</code></pre> <p>Tip</p> <p>Use <code>export KAFKA_OPTS=-Djavax.net.debug=all</code> to debug SSL issues. Learn more in the source code of openjdk's sun.security.ssl.SSLLogger.</p> <p>That's all for the demo. I hope you enjoyed it!</p>"},{"location":"demo/transactional-kafka-producer/","title":"Demo: Transactional Kafka Producer","text":"<p>This demo shows the internals of transactional KafkaProducer that is a Kafka producer with transaction.id defined.</p>"},{"location":"demo/transactional-kafka-producer/#kafkaproducer","title":"KafkaProducer","text":""},{"location":"demo/transactional-kafka-producer/#start-up","title":"Start Up","text":"<p>Use <code>sbt console</code> for interactive environment (or IntelliJ IDEA).</p> <pre><code>import org.apache.kafka.clients.producer.KafkaProducer\nimport org.apache.kafka.clients.producer.ProducerConfig\nimport org.apache.kafka.common.serialization.StringSerializer\n\nimport java.util.Properties\nval props = new Properties()\nprops.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName)\nprops.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName)\nprops.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \":9092\")\nprops.put(ProducerConfig.CLIENT_ID_CONFIG, \"txn-demo\")\n// Define transaction.id\nval transactionalId = \"my-custom-txnId\"\nprops.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId)\nval producer = new KafkaProducer[String, String](props)\n</code></pre>"},{"location":"demo/transactional-kafka-producer/#initialize-transactions","title":"Initialize Transactions","text":"<pre><code>producer.initTransactions\n</code></pre> <p>Once initialized, the transactional producer must not be initialized again.</p> <pre><code>scala&gt; producer.initTransactions\norg.apache.kafka.common.KafkaException: TransactionalId my-custom-txnId: Invalid transition attempted from state READY to state INITIALIZING\n  at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:1078)\n  at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:1071)\n  at org.apache.kafka.clients.producer.internals.TransactionManager.lambda$initializeTransactions$1(TransactionManager.java:337)\n  at org.apache.kafka.clients.producer.internals.TransactionManager.handleCachedTransactionRequestResult(TransactionManager.java:1200)\n  at org.apache.kafka.clients.producer.internals.TransactionManager.initializeTransactions(TransactionManager.java:334)\n  at org.apache.kafka.clients.producer.internals.TransactionManager.initializeTransactions(TransactionManager.java:329)\n  at org.apache.kafka.clients.producer.KafkaProducer.initTransactions(KafkaProducer.java:596)\n  ... 31 elided\n</code></pre>"},{"location":"demo/transactional-kafka-producer/#start-transaction","title":"Start Transaction","text":"<p>Next up is starting a transaction using KafkaProducer.beginTransaction</p> <pre><code>producer.beginTransaction\n</code></pre>"},{"location":"demo/transactional-kafka-producer/#transactional-sends","title":"Transactional sends","text":"<pre><code>import org.apache.kafka.clients.producer.ProducerRecord\nval topic = \"txn-demo\"\nval record = new ProducerRecord[String, String](topic, \"Hello from transactional producer\")\nproducer.send(record)\n</code></pre> <pre><code>producer.send(\n  new ProducerRecord[String, String](topic, \"Another hello from txn producer\"))\n</code></pre>"},{"location":"demo/transactional-kafka-producer/#logs","title":"Logs","text":""},{"location":"demo/transactional-kafka-producer/#kafka-producer","title":"Kafka Producer","text":"<p>You should see the following INFO messages in the logs of the Kafka producer:</p> <pre><code>INFO [Producer clientId=producer-my-custom-txnId, transactionalId=my-custom-txnId] Invoking InitProducerId for the first time in order to acquire a producer ID (org.apache.kafka.clients.producer.internals.TransactionManager)\nINFO [Producer clientId=producer-my-custom-txnId, transactionalId=my-custom-txnId] Discovered transaction coordinator localhost:9092 (id: 0 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)\nINFO [Producer clientId=producer-my-custom-txnId, transactionalId=my-custom-txnId] ProducerId set to 0 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)\n</code></pre>"},{"location":"demo/transactional-kafka-producer/#kafka-cluster","title":"Kafka Cluster","text":"<p>You should see the following INFO message in the logs of a Kafka cluster:</p> <pre><code>INFO [TransactionCoordinator id=0] Initialized transactionalId my-custom-txnId with producerId 0 and producer epoch 0 on partition __transaction_state-20 (kafka.coordinator.transaction.TransactionCoordinator)\n</code></pre> <p>The calculation to determine the transactional partition (<code>__transaction_state-20</code>) is as follows:</p> <pre><code>Math.abs(transactionalId.hashCode) % 50\n</code></pre>"},{"location":"demo/transactional-kafka-producer/#start-up-consumer","title":"Start Up Consumer","text":""},{"location":"demo/transactional-kafka-producer/#kcat","title":"kcat","text":"<pre><code>kcat -C -b localhost -t txn-demo\n</code></pre> <p>You should see no records produced yet (since the transaction has not been committed yet).</p>"},{"location":"demo/transactional-kafka-producer/#kafka-console-consumer","title":"kafka-console-consumer","text":"<pre><code>./bin/kafka-console-consumer.sh \\\n  --bootstrap-server :9092 \\\n  --topic txn-demo \\\n  --from-beginning\n</code></pre> <p>Unlike <code>kcat</code>, <code>kafka-console-consumer</code> uses read_uncommitted isolation level and so there should be records printed out to the console.</p> <p>Use <code>--isolation-level</code> option to set isolation.level configuration property.</p>"},{"location":"demo/transactional-kafka-producer/#commit-transaction","title":"Commit Transaction","text":"<p>Let's commit the transaction using KafkaProducer.commitTransaction.</p> <pre><code>producer.commitTransaction\n</code></pre> <p>Immediately after committing the transaction you should see the record printed out by the Kafka consumer.</p>"},{"location":"dynamic-broker-configuration/","title":"Dynamic Broker Configuration","text":"<p>KIP-226 - Dynamic Broker Configuration</p>"},{"location":"dynamic-broker-configuration/BrokerConfigHandler/","title":"BrokerConfigHandler","text":""},{"location":"dynamic-broker-configuration/BrokerConfigHandler/#processconfigchanges","title":"processConfigChanges <pre><code>processConfigChanges(\n  brokerId: String,\n  properties: Properties): Unit\n</code></pre> <p><code>processConfigChanges</code> is part of the ConfigHandler abstraction.</p>  <p><code>processConfigChanges</code>...FIXME</p>","text":""},{"location":"dynamic-broker-configuration/BrokerReconfigurable/","title":"BrokerReconfigurable","text":"<p><code>BrokerReconfigurable</code> is an abstraction of dynamic reconfigurables that can be reconfigured at runtime.</p>"},{"location":"dynamic-broker-configuration/BrokerReconfigurable/#contract","title":"Contract","text":""},{"location":"dynamic-broker-configuration/BrokerReconfigurable/#reconfigurable-configs","title":"Reconfigurable Configs <pre><code>reconfigurableConfigs: Set[String]\n</code></pre> <p>Used when:</p> <ul> <li><code>DynamicBrokerConfig</code> is requested to addBrokerReconfigurable and processReconfiguration</li> </ul>","text":""},{"location":"dynamic-broker-configuration/BrokerReconfigurable/#validatereconfiguration","title":"validateReconfiguration <pre><code>validateReconfiguration(\n  newConfig: KafkaConfig): Unit\n</code></pre> <p>Validates the updated KafkaConfig</p> <p>Used when:</p> <ul> <li><code>DynamicBrokerConfig</code> is requested to processReconfiguration</li> </ul>","text":""},{"location":"dynamic-broker-configuration/BrokerReconfigurable/#reconfiguring-broker","title":"Reconfiguring Broker <pre><code>reconfigure(\n  oldConfig: KafkaConfig,\n  newConfig: KafkaConfig): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>DynamicBrokerConfig</code> is requested to updateCurrentConfig</li> </ul>","text":""},{"location":"dynamic-broker-configuration/BrokerReconfigurable/#implementations","title":"Implementations","text":"<ul> <li><code>DynamicListenerConfig</code></li> <li>DynamicLogConfig</li> <li>DynamicThreadPool</li> <li>LogCleaner</li> <li><code>SocketServer</code></li> </ul>"},{"location":"dynamic-broker-configuration/ConfigHandler/","title":"ConfigHandler","text":"<p><code>ConfigHandler</code> is an abstraction of config change handlers that can process configuration changes.</p>"},{"location":"dynamic-broker-configuration/ConfigHandler/#contract","title":"Contract","text":""},{"location":"dynamic-broker-configuration/ConfigHandler/#processconfigchanges","title":"processConfigChanges <pre><code>processConfigChanges(\n  entityName: String,\n  value: Properties): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>ConfigChangedNotificationHandler</code> is requested to <code>processEntityConfigChangeVersion1</code>, <code>processEntityConfigChangeVersion2</code></li> <li><code>ZkConfigManager</code> is requested to start up</li> <li><code>BrokerMetadataPublisher</code> is requested to <code>publish</code></li> </ul>","text":""},{"location":"dynamic-broker-configuration/ConfigHandler/#implementations","title":"Implementations","text":"<ul> <li>BrokerConfigHandler</li> <li><code>ClientIdConfigHandler</code></li> <li><code>IpConfigHandler</code></li> <li>TopicConfigHandler</li> <li><code>UserConfigHandler</code></li> </ul>"},{"location":"dynamic-broker-configuration/DynamicBrokerConfig/","title":"DynamicBrokerConfig","text":""},{"location":"dynamic-broker-configuration/DynamicBrokerConfig/#initializing","title":"Initializing <pre><code>initialize(\n  zkClientOpt: Option[KafkaZkClient]): Unit\n</code></pre>  <p><code>initialize</code> creates a new KafkaConfig (based on the given current KafkaConfig).</p> <p>With a <code>KafkaZkClient</code> specified (KafkaServer):</p> <ol> <li><code>initialize</code> creates a AdminZkClient</li> <li> <p><code>initialize</code> uses the <code>AdminZkClient</code> to fetchEntityConfig for all <code>brokers</code> and then updateDefaultConfig</p> <pre><code>./bin/zkCli.sh -server localhost:2181 get /config/brokers\n</code></pre> </li> <li> <p><code>initialize</code> uses the <code>AdminZkClient</code> to fetchEntityConfig for this broker (<code>brokers</code> entity with broker.id) and then updateBrokerConfig</p> <pre><code>./bin/zkCli.sh -server localhost:2181 get /config/brokers/[broker.id]\n</code></pre> </li> </ol>  <p>Note</p> <p><code>KafkaZkClient</code> is specified for KafkaServer (unlike BrokerServer that makes sense since <code>BrokerServer</code> is for Zookeeper-less Kafka deployment).</p>   <p><code>initialize</code> is used when:</p> <ul> <li><code>BrokerServer</code> is requested to start up</li> <li><code>KafkaServer</code> is requested to start up</li> </ul>","text":""},{"location":"dynamic-broker-configuration/DynamicBrokerConfig/#addreconfigurables","title":"addReconfigurables <pre><code>addReconfigurables(\n  kafkaServer: KafkaBroker): Unit\n</code></pre> <p><code>addReconfigurables</code>...FIXME</p>  <p><code>addReconfigurables</code> is used when:</p> <ul> <li><code>BrokerServer</code> is requested to start up</li> <li><code>KafkaServer</code> is requested to start up</li> </ul>","text":""},{"location":"dynamic-broker-configuration/DynamicBrokerConfig/#addbrokerreconfigurable","title":"addBrokerReconfigurable <pre><code>addBrokerReconfigurable(\n  reconfigurable: BrokerReconfigurable): Unit\n</code></pre> <p><code>addBrokerReconfigurable</code>...FIXME</p>  <p><code>addBrokerReconfigurable</code> is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"dynamic-broker-configuration/DynamicBrokerConfig/#updatedefaultconfig","title":"updateDefaultConfig <pre><code>updateDefaultConfig(\n  persistentProps: Properties,\n  doLog: Boolean = true): Unit\n</code></pre> <p><code>updateDefaultConfig</code>...FIXME</p>  <p><code>updateDefaultConfig</code> is used when:</p> <ul> <li><code>BrokerConfigHandler</code> is requested to processConfigChanges</li> <li><code>DynamicBrokerConfig</code> is requested to initialize</li> </ul>","text":""},{"location":"dynamic-broker-configuration/DynamicBrokerConfig/#processreconfiguration","title":"processReconfiguration <pre><code>processReconfiguration(\n  newProps: Map[String, String],\n  validateOnly: Boolean,\n  doLog: Boolean = false): (KafkaConfig, List[BrokerReconfigurable])\n</code></pre> <p><code>processReconfiguration</code>...FIXME</p>  <p><code>processReconfiguration</code> is used when:</p> <ul> <li><code>DynamicBrokerConfig</code> is requested to validate and updateCurrentConfig</li> </ul>","text":""},{"location":"dynamic-broker-configuration/DynamicBrokerConfig/#validate","title":"validate <pre><code>validate(\n  props: Properties,\n  perBrokerConfig: Boolean): Unit\n</code></pre> <p><code>validate</code>...FIXME</p>  <p><code>validate</code> is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"dynamic-broker-configuration/DynamicBrokerConfig/#updatecurrentconfig","title":"updateCurrentConfig <pre><code>updateCurrentConfig(\n  doLog: Boolean): Unit\n</code></pre> <p><code>updateCurrentConfig</code>...FIXME</p>  <p><code>updateCurrentConfig</code> is used when:</p> <ul> <li><code>DynamicBrokerConfig</code> is requested to updateBrokerConfig and updateDefaultConfig</li> </ul>","text":""},{"location":"dynamic-broker-configuration/DynamicBrokerConfig/#updatebrokerconfig","title":"updateBrokerConfig <pre><code>updateBrokerConfig(\n  brokerId: Int,\n  persistentProps: Properties,\n  doLog: Boolean = true): Unit\n</code></pre> <p><code>updateBrokerConfig</code>...FIXME</p>  <p><code>updateBrokerConfig</code> is used when:</p> <ul> <li><code>BrokerConfigHandler</code> is requested to processConfigChanges</li> <li><code>DynamicBrokerConfig</code> is requested to initialize</li> </ul>","text":""},{"location":"dynamic-broker-configuration/DynamicBrokerConfig/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>kafka.server.DynamicBrokerConfig</code> logger to see what happens inside.</p> <p>Add the following line to <code>config/log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.server.DynamicBrokerConfig=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"dynamic-broker-configuration/DynamicLogConfig/","title":"DynamicLogConfig","text":"<p><code>DynamicLogConfig</code> is a BrokerReconfigurable of LogManager.</p>"},{"location":"dynamic-broker-configuration/DynamicLogConfig/#creating-instance","title":"Creating Instance","text":"<p><code>DynamicLogConfig</code> takes the following to be created:</p> <ul> <li> LogManager <li> KafkaBroker <p><code>DynamicLogConfig</code> is created when:</p> <ul> <li><code>DynamicBrokerConfig</code> is requested to register Reconfigurables</li> </ul>"},{"location":"dynamic-broker-configuration/DynamicLogConfig/#reconfigurable-configs","title":"Reconfigurable Configs <pre><code>reconfigurableConfigs: Set[String]\n</code></pre> <p><code>reconfigurableConfigs</code> is part of the BrokerReconfigurable abstraction.</p>  <p><code>reconfigurableConfigs</code> returns the values of the TopicConfigSynonyms.</p>","text":""},{"location":"dynamic-broker-configuration/DynamicLogConfig/#reconfiguring-broker","title":"Reconfiguring Broker <pre><code>reconfigure(\n  oldConfig: KafkaConfig,\n  newConfig: KafkaConfig): Unit\n</code></pre> <p><code>reconfigure</code> is part of the BrokerReconfigurable abstraction.</p>  <p><code>reconfigure</code> requests the LogManager for the currentDefaultConfig (and the value of unclean.leader.election.enable configuration property explicitly).</p> <p><code>reconfigure</code> updates reconfigurable configuration properties only.</p> <p><code>reconfigure</code> requests the LogManager to reconfigureDefaultLogConfig with the new broker configs.</p> <p><code>reconfigure</code> updateLogsConfig (with the new broker configs).</p> <p>In the end, <code>reconfigure</code> requests the KafkaController to  enableDefaultUncleanLeaderElection when unclean.leader.election.enable is currently enabled while it was not before.</p>","text":""},{"location":"dynamic-broker-configuration/DynamicThreadPool/","title":"DynamicThreadPool","text":"<p><code>DynamicThreadPool</code> is a BrokerReconfigurable</p>"},{"location":"dynamic-broker-configuration/DynamicThreadPool/#reconfigure","title":"reconfigure <pre><code>reconfigure(\n  oldConfig: KafkaConfig,\n  newConfig: KafkaConfig): Unit\n</code></pre> <p><code>reconfigure</code> is part of the BrokerReconfigurable abstraction.</p>  <p><code>reconfigure</code>...FIXME</p>","text":""},{"location":"dynamic-broker-configuration/TopicConfigHandler/","title":"TopicConfigHandler","text":"<p><code>TopicConfigHandler</code> is a ConfigHandler.</p>"},{"location":"dynamic-broker-configuration/TopicConfigHandler/#creating-instance","title":"Creating Instance","text":"<p><code>TopicConfigHandler</code> takes the following to be created:</p> <ul> <li> LogManager <li> KafkaConfig <li> <code>QuotaManagers</code> <li> Optional KafkaController <p><code>TopicConfigHandler</code> is created when:</p> <ul> <li><code>BrokerServer</code> is requested to startup (and create dynamicConfigHandlers)</li> <li><code>KafkaServer</code> is requested to startup (and create dynamicConfigHandlers)</li> </ul>"},{"location":"dynamic-broker-configuration/TopicConfigHandler/#processconfigchanges","title":"processConfigChanges <pre><code>processConfigChanges(\n  topic: String,\n  topicConfig: Properties): Unit\n</code></pre> <p><code>processConfigChanges</code> is part of the ConfigHandler abstraction.</p>  <p><code>processConfigChanges</code>...FIXME</p>","text":""},{"location":"dynamic-broker-configuration/TopicConfigHandler/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>kafka.server.TopicConfigHandler</code> logger to see what happens inside.</p> <p>Add the following line to <code>config/log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.server.TopicConfigHandler=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"dynamic-broker-configuration/ZkConfigManager/","title":"ZkConfigManager","text":""},{"location":"dynamic-broker-configuration/ZkConfigManager/#creating-instance","title":"Creating Instance","text":"<p><code>ZkConfigManager</code> takes the following to be created:</p> <ul> <li> KafkaZkClient <li> ConfigHandlers by name <p><code>ZkConfigManager</code> is created when:</p> <ul> <li><code>KafkaServer</code> is requested to start up (and initializes the dynamicConfigManager)</li> </ul>"},{"location":"dynamic-broker-configuration/ZkConfigManager/#starting-up","title":"Starting Up <pre><code>startup(): Unit\n</code></pre> <p><code>startup</code> begins watching for config changes by requesting the ZkNodeChangeNotificationListener to initialize.</p> <p><code>startup</code>...FIXME</p>  <p><code>startup</code> is used when:</p> <ul> <li><code>KafkaServer</code> is requested to start up</li> </ul>","text":""},{"location":"dynamic-broker-configuration/ZkNodeChangeNotificationListener/","title":"ZkNodeChangeNotificationListener","text":""},{"location":"dynamic-broker-configuration/ZkNodeChangeNotificationListener/#creating-instance","title":"Creating Instance","text":"<p><code>ZkNodeChangeNotificationListener</code> takes the following to be created:</p> <ul> <li> KafkaZkClient <li> Node Root <li> Node Prefix <li> <code>NotificationHandler</code> <li> Change Expiration (in millis, default: <code>15 * 60 * 1000</code>) <li> <code>Time</code> <p><code>ZkNodeChangeNotificationListener</code> is created when:</p> <ul> <li><code>DelegationTokenManager</code> is requested to <code>startup</code></li> <li><code>ZkConfigManager</code> is created</li> <li><code>ZkAclChangeStore</code> is requested to <code>createListener</code></li> </ul>"},{"location":"features/","title":"Features","text":"<p>The following is a list of the features of Apache Kafka that make it so amazing (even awesomesauce \ud83d\ude0f):</p> <ul> <li>KRaft</li> <li>Log Cleanup</li> <li>others (listed in the menu on the left)</li> </ul>"},{"location":"kraft/","title":"Kafka Raft (KRaft)","text":"<p>Kafka Raft (KRaft) is a Raft-based distributed consensus algorithm in Apache Kafka to manage cluster metadata.</p> <p>There are two types of servers in KRaft mode:</p> <ul> <li>broker</li> <li>controller</li> </ul> <p>A single Kafka server can be one or both (<code>broker,controller</code>) at the same time based on process.roles configuration property.</p> <p>A Kafka server is KafkaRaftServer in KRaft mode. When created, KafkaRaftServer creates a SharedServer that is then used to create a BrokerServer or a ControllerServer or both (based on process.roles configuration property).</p> <p>In KRaft mode, controllers store cluster metadata in the internal __cluster_metadata topic.</p> <p>KRaft allows running a Kafka cluster without Apache ZooKeeper in so-called Kafka Raft metadata mode (KRaft mode).</p> <p>With KRaft, a Kafka cluster uses its own Kafka infrastructure for metadata (with no need for a separate system, e.g. Zookeeper).</p> <p>In KRaft mode, the storage log directories on a node must be formatted using kafka-storage utility. This requirement prevents system administrators from accidentally enabling KRaft mode by simply making a configuration change. Requiring formatting also prevents mistakes, since Kafka no longer has to guess if an empty storage directory is a newly directory or one where a system error prevented any data from showing up.</p> <p>KRaft was proposed as KIP-500 and went live in Kafka 2.8.0.</p> <p>KRaft is production ready since Apache Kafka 3.3.1.</p>"},{"location":"kraft/#metadata-topic","title":"Metadata Topic","text":"<p>Controllers use an internal <code>__cluster_metadata</code> topic to store the cluster metadata.</p> <p>This topic contains a single partition.</p> <p>Metadata changes are persisted to the <code>__cluster_metadata</code> log before applying them to the other nodes in the cluster. This means waiting for the metadata log's last stable offset to advance to the offset of the change.</p> <p>Changes that we haven't yet persisted are referred to as \"uncommitted.\"  The active controller may have several of these uncommitted changes in flight at any given time.  In essence, the controller's in-memory state is always a little bit in the future compared to the current state.  This allows the active controller to continue doing things while it waits for the previous changes to be committed to the Raft log.</p>"},{"location":"kraft/#node-ids","title":"Node IDs","text":"<p>Node IDs must be set in the configuration file using <code>node.id</code> configuration. No automatic node ID assignment is available.</p> <p>In a co-located configuration, a single process may take both the <code>controller</code> and <code>broker</code> roles. The node ID for both of these roles is defined by <code>node.id</code>. However, this is mainly for configuration convenience. Semantically, we view the co-located process as representing two distinct nodes. Each node has its own listeners and its own set of APIs which it exposes. The APIs exposed by a controller node will not be the same as those exposed by a broker node.</p>"},{"location":"kraft/#controllers","title":"Controllers","text":"<p>KRaft Controllers are responsible for storing the metadata of the cluster in the metadata log.</p> <p>Controllers participate in the metadata quorum.</p> <p>In Kraft mode, the active controller is selected among a potentially smaller pool of nodes specifically configured to act as controllers.  Typically three or five nodes in a cluster will be selected to be controllers.</p> <p>The leader of the controller quorum will be the active controller. The followers will function as hot standbys, ready to take over when the active leader fails or resigns. The cluster metadata is stored in memory on all of the controllers.</p> <p>The active controller makes changes to the metadata by appending records to the log. Each record has a <code>null</code> key with some value.</p> <p>System administrators will be able to choose whether to run separate controller nodes, or whether to run controller nodes which are co-located with broker nodes. Kafka supports running a controller in the same JVM as a broker, in order to save memory and enable single-process test deployments.</p> <p>The addresses and ports of the controller nodes must be configured on each broker, so that the broker can contact the controller quorum when starting up. As long as at least one of the provided controller addresses is valid, the broker will be able to learn about the current metadata quorum and start up.</p> <p>Controllers listen on a separate endpoint from brokers. The endpoints should be firewalled off from clients to prevent clients from disrupting the cluster by flooding the controller ports with requests.</p> <p>Controllers do not appear in the MetadataResponses given to clients.</p> <p>Controllers do not host topics.</p>"},{"location":"kraft/#brokers","title":"Brokers","text":"<p>KRaft Brokers</p> <p>Brokers register with the active controller using a <code>BrokerRegistrationRequest</code>. The active controller assigns the broker a new broker epoch, based on the next available offset in the log. The new epoch is guaranteed to be higher than any previous epoch that has been used for the given broker id.</p> <p>In its periodic heartbeats, the broker asks the controller if it can transition into the controlled shutdown state. It does this by setting the WantShutDown boolean. This motivates the controller to move all of the leaders off of that broker. Once they are all moved, the controller responds to the heartbeat with <code>ShouldShutDown = true</code>. At that point, the broker knows it's safe to begin the shutdown process proper.</p> <p>The brokers are always fetching new metadata from the controller.</p>"},{"location":"kraft/#metaproperties","title":"meta.properties","text":"<p>When a storage directory is in use by a cluster running in KRaft mode, it will have a new version of the <code>meta.properties</code> file. The version is <code>1</code>. <code>meta.properties</code> file is a plain text file where each line has the format <code>key=value</code>.</p> <p>The file contains <code>node.id</code> of the owner node.</p> <p>The process will raise an error at startup if either the <code>meta.properties</code> file does not exist or if the <code>node.id</code> does not match what the value from the configuration file.</p>"},{"location":"kraft/#metrics","title":"Metrics","text":"Full Name Description <code>kafka.controller:type=KafkaController,name=MetadataLag</code> The offset delta between the latest metadata record this controller has replayed and the last stable offset of the metadata topic. <code>kafka.controller:type=KafkaServer,name=MetadataLag</code> The offset delta between the latest metadata record this broker has replayed and the last stable offset of the metadata topic. <code>kafka.controller:type=KafkaController,name=MetadataCommitLatencyMs</code> The latency of committing a message to the metadata topic.  Relevant on the active controller. <code>kafka.controller:type=KafkaController,name=MetadataCommitRatePerSec</code> The number of metadata messages per second committed to the metadata topic. <code>kafka.controller:type=KafkaController,name=MetadataSnapshotOffsetLag</code> The offset delta between the latest stable offset of the metadata topic and the offset of the last snapshot (or the last stable offset itself, if there are no snapshots)"},{"location":"kraft/#metadata-quorum","title":"Raft-Based Metadata Quorum","text":"<p>controller.quorum.voters</p> <p>It is possible to reconfigure the metadata quorum over time. For example, if we start with a metadata quorum of host1, host2, host3, we could replace host3 with host4 without disrupting any of the brokers. Then we could roll the brokers to apply the new metadata quorum bootstrap configuration of host1, host2, host4 on each one.</p>"},{"location":"kraft/#snapshots","title":"Snapshots","text":"<p>Periodically, controllers consolidate all the metadata deltas into a snapshot.</p> <p>Like the metadata log, the snapshot is made up of records.  However, unlike the log, in which there may be multiple records describing a single entity, the snapshot will only contain the minimum number of records needed to describe all the entities.</p> <p>Snapshots are local to each replica. Any snapshot must be usable as a starting point for loading the entire state of metadata. In other words, a new controller node must be able to load the a snapshot, and then apply all the edits which follow it, and come up-to-date.</p>"},{"location":"kraft/#kraft-metadata-transactions","title":"KRaft Metadata Transactions","text":"<p>KIP-868 Metadata Transactions</p>"},{"location":"kraft/#learn-more","title":"Learn More","text":"<ul> <li>KIP-500: Replace ZooKeeper with a Self-Managed Metadata Quorum</li> <li>KIP-631: The Quorum-based Kafka Controller</li> <li>KRaft Overview</li> <li>Raft (algorithm) on Wikipedia</li> </ul>"},{"location":"kraft/BrokerServer/","title":"BrokerServer","text":"<p><code>BrokerServer</code> is a KafkaBroker that runs in KRaft mode.</p>"},{"location":"kraft/BrokerServer/#creating-instance","title":"Creating Instance","text":"<p><code>BrokerServer</code> takes the following to be created:</p> <ul> <li> KafkaConfig <li> MetaProperties <li> RaftManager <li> <code>Time</code> <li> Metrics <li> Optional <code>threadNamePrefix</code> <li>Initial Offline Log Directories</li> <li> <code>controllerQuorumVotersFuture</code> (<code>CompletableFuture[util.Map[Integer, AddressSpec]]</code>) <li> Supported Features <p><code>BrokerServer</code> is created when:</p> <ul> <li><code>KafkaRaftServer</code> is created (with <code>BrokerRole</code> among the processRoles)</li> </ul>"},{"location":"kraft/BrokerServer/#initial-offline-log-directories","title":"Initial Offline Log Directories <pre><code>initialOfflineDirs: Seq[String]\n</code></pre> <p><code>BrokerServer</code> is given <code>initialOfflineDirs</code> when created (that is offlineDirs).</p> <p><code>initialOfflineDirs</code> is used to create a LogManager when <code>BrokerServer</code> is requested to startup.</p>","text":""},{"location":"kraft/BrokerServer/#startup","title":"startup <pre><code>startup(): Unit\n</code></pre> <p><code>startup</code>...FIXME</p> <p><code>startup</code>\u00a0is used when:</p> <ul> <li><code>KafkaRaftServer</code> is requested to startup</li> </ul>","text":""},{"location":"kraft/ControllerServer/","title":"ControllerServer","text":"<p><code>ControllerServer</code> represents the <code>controller</code> in process.roles.</p>"},{"location":"kraft/ControllerServer/#creating-instance","title":"Creating Instance","text":"<p><code>ControllerServer</code> takes the following to be created:</p> <ul> <li> SharedServer <li> <code>KafkaConfigSchema</code> <li> <code>BootstrapMetadata</code> <p><code>ControllerServer</code> is created alongside a KafkaRaftServer (for <code>controller</code> in process.roles).</p>"},{"location":"kraft/ControllerServer/#startup","title":"Starting Up","text":"<pre><code>startup(): Unit\n</code></pre> <p><code>startup</code> changes status from <code>SHUTDOWN</code> to <code>STARTING</code>.</p> <p><code>startup</code> uses server.max.startup.time.ms for...FIXME</p> <p><code>startup</code> prints out the following INFO message to the logs:</p> <pre><code>Starting controller\n</code></pre> <p><code>startup</code> requests the DynamicBrokerConfig (of the KafkaConfig) to initialize (with no <code>KafkaZkClient</code> as it runs in Zookeeper-less KRaft mode).</p> <p><code>startup</code> changes status from <code>STARTING</code> to <code>STARTED</code>.</p> <p><code>startup</code> registers new metrics (gauges) in the KafkaMetricsGroup.</p> Metric Name Description <code>ClusterId</code> clusterId <code>yammer-metrics-count</code> <code>linux-disk-read-bytes</code> (only on Linux) <code>linux-disk-write-bytes</code> (only on Linux) <p><code>startup</code>...FIXME</p> <p>Note</p> <p>There is a lot services being registered that seem not necessarily as important at this early stage of the KRaft exploration of mine \ud83d\ude09</p> <p><code>startup</code> requests the SharedServer to startForController.</p> <p><code>startup</code>...FIXME</p> <p><code>startup</code> builds the QuorumController.</p> <p><code>startup</code>...FIXME</p> <p>In the end, <code>startup</code> requests the DynamicBrokerConfig </p> <p><code>startup</code> registers this <code>ControllerServer</code> for dynamic config changes (to the KafkaConfig).</p> <p><code>startup</code> is used when:</p> <ul> <li><code>KafkaRaftServer</code> is requested to startup</li> </ul>"},{"location":"kraft/ControllerServer/#logging","title":"Logging","text":"<p>Enable <code>ALL</code> logging level for <code>kafka.server.ControllerServer</code> logger to see what happens inside.</p> <p>Add the following line to <code>config/log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.server.ControllerServer=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"kraft/FileBasedStateStore/","title":"FileBasedStateStore","text":"<p><code>FileBasedStateStore</code> is a QuorumStateStore that writes election state to a file.</p>"},{"location":"kraft/FileBasedStateStore/#creating-instance","title":"Creating Instance","text":"<p><code>FileBasedStateStore</code> takes the following to be created:</p> <ul> <li>State File</li> </ul> <p><code>FileBasedStateStore</code> is created when:</p> <ul> <li><code>KafkaRaftManager</code> is requested to build a RaftClient</li> </ul>"},{"location":"kraft/FileBasedStateStore/#stateFile","title":"State File","text":"<p><code>FileBasedStateStore</code> is given a state file when created.</p> <p>The state file is <code>quorum-state</code> file under the data directory (of KafkaRaftManager).</p> <p>The state file is loaded in readElectionState and updated in writeElectionState.</p> <p><code>stateFile</code> is deleted in clear.</p>"},{"location":"kraft/FileBasedStateStore/#readElectionState","title":"readElectionState","text":"QuorumStateStore <pre><code>ElectionState readElectionState()\n</code></pre> <p><code>readElectionState</code> is part of the QuorumStateStore abstraction.</p> <p><code>readElectionState</code> is <code>null</code> (undefined) when stateFile does not exist.</p>"},{"location":"kraft/KafkaMetadataLog/","title":"KafkaMetadataLog","text":"<p><code>KafkaMetadataLog</code> is a <code>ReplicatedLog</code>.</p>"},{"location":"kraft/KafkaMetadataLog/#creating-instance","title":"Creating Instance","text":"<p><code>KafkaMetadataLog</code> takes the following to be created:</p> <ul> <li> UnifiedLog <li> <code>Time</code> <li> <code>Scheduler</code> <li> Snapshots <li> <code>TopicPartition</code> <li> <code>MetadataLogConfig</code> <p><code>KafkaMetadataLog</code> is created using apply utility.</p>"},{"location":"kraft/KafkaMetadataLog/#apply","title":"Creating KafkaMetadataLog","text":"<pre><code>apply(\n  topicPartition: TopicPartition,\n  topicId: Uuid,\n  dataDir: File,\n  time: Time,\n  scheduler: Scheduler,\n  config: MetadataLogConfig): KafkaMetadataLog\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is used when:</p> <ul> <li><code>KafkaRaftManager</code> is requested to buildMetadataLog</li> </ul>"},{"location":"kraft/KafkaRaftClient/","title":"KafkaRaftClient","text":"<p><code>KafkaRaftClient</code> is a RaftClient.</p>"},{"location":"kraft/KafkaRaftClient/#creating-instance","title":"Creating Instance","text":"<p><code>KafkaRaftClient</code> takes the following to be created:</p> <ul> <li> <code>RecordSerde&lt;T&gt;</code> <li> <code>NetworkChannel</code> <li> <code>RaftMessageQueue</code> <li> <code>ReplicatedLog</code> <li>QuorumStateStore</li> <li> <code>MemoryPool</code> <li> <code>Time</code> <li> Metrics <li> <code>ExpirationService</code> <li> <code>fetchMaxWaitMs</code> (<code>500</code> ms) <li> Cluster ID <li> Node ID <li> <code>LogContext</code> <li> <code>Random</code> <li> RaftConfig <p><code>KafkaRaftClient</code> is created when:</p> <ul> <li><code>KafkaRaftManager</code> is requested to build a KafkaRaftClient</li> </ul>"},{"location":"kraft/KafkaRaftClient/#quorumStateStore","title":"QuorumStateStore","text":"<p><code>KafkaRaftClient</code> is given a QuorumStateStore when created.</p> <p>The <code>QuorumStateStore</code> is a FileBasedStateStore with the <code>quorum-state</code> state file in the dataDir.</p>"},{"location":"kraft/KafkaRaftManager/","title":"KafkaRaftManager","text":"<p><code>KafkaRaftManager</code> is a RaftManager.</p> <p><code>KafkaRaftManager</code> is created and immediately started when <code>SharedServer</code> is started.</p>"},{"location":"kraft/KafkaRaftManager/#creating-instance","title":"Creating Instance","text":"<p><code>KafkaRaftManager</code> takes the following to be created:</p> <ul> <li> MetaProperties <li> KafkaConfig <li> <code>RecordSerde[T]</code> <li>Partition 0 of __cluster_metadata</li> <li> Topic ID (UUID) <li> <code>Time</code> <li> Metrics <li> Thread Name Prefix (optional) <li> Controller Quorum Voters (<code>CompletableFuture[util.Map[Integer, AddressSpec]]</code>) <li> <code>FaultHandler</code> <p><code>KafkaRaftManager</code> is created when:</p> <ul> <li><code>KafkaServer</code> is requested to start up (with zookeeper.metadata.migration.enable enabled)</li> <li><code>SharedServer</code> is requested to start</li> </ul>"},{"location":"kraft/KafkaRaftManager/#topicPartition","title":"__cluster_metadata-0 Partition","text":"<p><code>KafkaRaftManager</code> is given a <code>TopicPartition</code> when created:</p> <ul> <li><code>__cluster_metadata</code> as the name of the cluster metadata topic</li> <li>Partition <code>0</code></li> </ul>"},{"location":"kraft/KafkaRaftManager/#dataDir","title":"Metadata Log Directory","text":"<p><code>KafkaRaftManager</code> creates a data directory when created.</p>"},{"location":"kraft/KafkaRaftManager/#createDataDir","title":"createDataDir","text":"<pre><code>createDataDir(): File\n</code></pre> <p><code>createDataDir</code> creates the name of the log directory of the TopicPartition.</p> <p><code>createDataDir</code> creates the directory in the metadataLogDir.</p>"},{"location":"kraft/KafkaRaftManager/#startup","title":"Starting Up","text":"<pre><code>startup(): Unit\n</code></pre> Procedure <p><code>startup</code> is a procedure (returns <code>Unit</code>) so what happens inside stays inside (paraphrasing the former advertising slogan of Las Vegas, Nevada).</p> <p><code>startup</code>...FIXME</p> <p><code>startup</code> is used when:</p> <ul> <li><code>KafkaServer</code> is requested to startup</li> <li><code>SharedServer</code> is requested to start</li> <li><code>MetadataShell</code> is requested to initializeWithRaftManager (upon run)</li> </ul>"},{"location":"kraft/KafkaRaftServer/","title":"KafkaRaftServer","text":"<p><code>KafkaRaftServer</code> is a Server for KRaft mode.</p>"},{"location":"kraft/KafkaRaftServer/#creating-instance","title":"Creating Instance","text":"<p><code>KafkaRaftServer</code> takes the following to be created:</p> <ul> <li> KafkaConfig <li> <code>Time</code> <li> Thread Name Prefix (optional) <p><code>KafkaRaftServer</code> is created when:</p> <ul> <li><code>Kafka</code> command-line application is launched (and builds a server with no process.roles)</li> </ul>"},{"location":"kraft/KafkaRaftServer/#startup","title":"startup","text":"Server <pre><code>startup(): Unit\n</code></pre> <p><code>startup</code> is part of the Server abstraction.</p> <p><code>startup</code>...FIXME</p>"},{"location":"kraft/KafkaRaftServer/#initializeLogDirs","title":"initializeLogDirs","text":"<pre><code>initializeLogDirs(\n  config: KafkaConfig): (MetaProperties, Seq[String])\n</code></pre> <p><code>initializeLogDirs</code> uses the log.dirs (if defined) or log.dir and metadata.log.dir for the log directories (<code>logDirs</code>).</p> <p><code>initializeLogDirs</code> getBrokerMetadataAndOfflineDirs the log directories (into <code>rawMetaProperties</code> and <code>offlineDirs</code>).</p> <p>Note</p> <p>metadata.log.dir is not allowed to be among the offline directories (<code>offlineDirs</code>) or a <code>KafkaException</code> is thrown:</p> <pre><code>Cannot start server since `meta.properties` could not be loaded from [metadataLogDir]\n</code></pre> <p><code>initializeLogDirs</code>...FIXME</p> <p><code>initializeLogDirs</code> is used when:</p> <ul> <li><code>KafkaRaftServer</code> is created (and initializes metaProps and offlineDirs)</li> </ul>"},{"location":"kraft/KafkaRaftServer/#offlineDirs","title":"offlineDirs","text":"<pre><code>offlineDirs: Seq[String]\n</code></pre> <p><code>KafkaRaftServer</code> initializes an <code>offlineDirs</code> registry when created.</p> <p><code>offlineDirs</code> is used when:</p> <ul> <li><code>KafkaRaftServer</code> is requested for the broker (to create a BrokerServer)</li> </ul>"},{"location":"kraft/KafkaRaftServer/#metaProps","title":"metaProps","text":"<pre><code>metaProps: MetaProperties\n</code></pre> <p><code>KafkaRaftServer</code> creates a MetaProperties when created.</p> <p><code>metaProps</code> is used to initialize the other <code>KafkaRaftServer</code> services:</p> <ul> <li>BrokerServer</li> <li>ControllerServer</li> <li>KafkaRaftManager</li> <li>Metrics</li> </ul>"},{"location":"kraft/KafkaRaftServer/#broker","title":"BrokerServer","text":"<pre><code>broker: Option[BrokerServer]\n</code></pre> <p><code>KafkaRaftServer</code> creates a BrokerServer when created with BrokerRole.</p> <p>The lifecycle of <code>BrokerServer</code> is tied up to <code>KafkaRaftServer</code>:</p> <ul> <li>startup when <code>KafkaRaftServer</code> is requested to startup</li> <li>shutdown when <code>KafkaRaftServer</code> is requested to shutdown</li> <li>awaitShutdown when <code>KafkaRaftServer</code> is requested to awaitShutdown</li> </ul>"},{"location":"kraft/KafkaRaftServer/#controller","title":"ControllerServer","text":"<pre><code>controller: Option[ControllerServer]\n</code></pre> <p><code>KafkaRaftServer</code> creates a ControllerServer when created with <code>controller</code> in process.roles.</p> <p>The lifecycle of <code>ControllerServer</code> is tied up to <code>KafkaRaftServer</code>:</p> <ul> <li>startup when <code>KafkaRaftServer</code> is requested to startup</li> <li>shutdown when <code>KafkaRaftServer</code> is requested to shutdown</li> <li>awaitShutdown when <code>KafkaRaftServer</code> is requested to awaitShutdown</li> </ul>"},{"location":"kraft/KafkaRaftServer/#sharedServer","title":"SharedServer","text":"<p><code>KafkaRaftServer</code> creates a SharedServer when created.</p> <p>The <code>SharedServer</code> is used when running in KRaft mode to create the following:</p> <ul> <li>BrokerServer when configured as a broker</li> <li>ControllerServer when configured as a controller</li> </ul>"},{"location":"kraft/MetaProperties/","title":"MetaProperties","text":"<p><code>MetaProperties</code> is serialized to <code>meta.properties</code> file in metadata.log.dir (if specified) or the head of the logDirs.</p> <p><code>MetaProperties</code> is used to create a KafkaRaftManager.</p>"},{"location":"kraft/MetaProperties/#creating-instance","title":"Creating Instance","text":"<p><code>MetaProperties</code> takes the following to be created:</p> <ul> <li> Cluster ID <li> Node ID <p><code>MetaProperties</code> is created when:</p> <ul> <li><code>KafkaServer</code> is requested to startup (to create a KafkaRaftManager)</li> <li><code>MetaProperties</code> is requested to parse</li> </ul>"},{"location":"kraft/QuorumState/","title":"QuorumState","text":"<p><code>QuorumState</code> is used by KafkaRaftClient to...FIXME</p>"},{"location":"kraft/QuorumState/#creating-instance","title":"Creating Instance","text":"<p><code>QuorumState</code> takes the following to be created:</p> <ul> <li> Local ID <li>Voters</li> <li> controller.quorum.election.timeout.ms <li> controller.quorum.fetch.timeout.ms <li> <code>QuorumStateStore</code> <li> <code>Time</code> <li> <code>LogContext</code> <li> <code>Random</code> <p><code>QuorumState</code> is created when:</p> <ul> <li><code>KafkaRaftClient</code> is created</li> </ul>"},{"location":"kraft/QuorumState/#voters","title":"voters","text":"<pre><code>Set&lt;Integer&gt; voters\n</code></pre> <p><code>QuorumState</code> is given quorum voters (their IDs) when created based on controller.quorum.voters configuration property.</p>"},{"location":"kraft/QuorumStateStore/","title":"QuorumStateStore","text":"<p><code>QuorumStateStore</code> is an abstraction of kraft quorum state stores.</p>"},{"location":"kraft/QuorumStateStore/#contract-subset","title":"Contract (Subset)","text":""},{"location":"kraft/QuorumStateStore/#readElectionState","title":"readElectionState","text":"<pre><code>ElectionState readElectionState()\n</code></pre> <p>Reads (loads) the latest election state</p> <p>See:</p> <ul> <li>FileBasedStateStore</li> </ul> <p>Used when:</p> <ul> <li><code>QuorumState</code> is requested to initialize</li> </ul>"},{"location":"kraft/QuorumStateStore/#implementations","title":"Implementations","text":"<ul> <li>FileBasedStateStore</li> </ul>"},{"location":"kraft/RaftClient/","title":"RaftClient","text":"<p><code>RaftClient</code> is an abstraction of Raft clients.</p> <p><code>RaftClient</code> is <code>AutoCloseable</code>.</p>"},{"location":"kraft/RaftClient/#contract","title":"Contract","text":""},{"location":"kraft/RaftClient/#initialize","title":"initialize","text":"<pre><code>void initialize()\n</code></pre> <p>See:</p> <ul> <li>KafkaRaftClient</li> </ul> <p>Used when:</p> <ul> <li><code>KafkaRaftManager</code> is requested to build a RaftClient</li> </ul>"},{"location":"kraft/RaftClient/#implementations","title":"Implementations","text":"<ul> <li>KafkaRaftClient</li> </ul>"},{"location":"kraft/RaftConfig/","title":"RaftConfig","text":"<p><code>RaftConfig</code> is the configuration of KafkaRaftManager in KRaft mode.</p>"},{"location":"kraft/RaftConfig/#creating-instance","title":"Creating Instance","text":"<p><code>RaftConfig</code> takes the following to be created:</p> <ul> <li> controller.quorum.voters <li> <code>requestTimeoutMs</code> <li> <code>retryBackoffMs</code> <li> <code>electionTimeoutMs</code> <li> <code>electionBackoffMaxMs</code> <li> <code>fetchTimeoutMs</code> <li> <code>appendLingerMs</code> <p><code>RaftConfig</code> is created when:</p> <ul> <li><code>KafkaRaftManager</code> is created</li> </ul>"},{"location":"kraft/RaftConfig/#controller.quorum.voters","title":"controller.quorum.voters <p>A comma-separated list of <code>{id}@{host}:{port}</code> with the node IDs and the endpoints of all the controllers (quorum voters) in a Kafka cluster in a KRaft mode (e.g., <code>1@localhost:9092,2@localhost:9093,3@localhost:9094</code>)</p> <p>Default: (empty)</p> <p>Importance: High</p> <p>For ProcessRolesProp with <code>controller</code> role, the node id must also be included in <code>controller.quorum.voters</code>.</p> <p>For ProcessRolesProp with <code>broker</code> role only, the node id must not be included in <code>controller.quorum.voters</code>.</p> <p>Available as KafkaConfig.quorumVoters</p> <p>Used when:</p> <ul> <li><code>RaftConfig</code> is requested to parseVoterConnections</li> <li><code>KafkaConfig</code> is requested for QuorumVotersProp and quorumVoters</li> </ul>","text":""},{"location":"kraft/RaftConfig/#parseVoterConnections","title":"parseVoterConnections","text":"<pre><code>Map&lt;Integer, AddressSpec&gt; parseVoterConnections(\n  List&lt;String&gt; voterEntries)\n</code></pre> <p><code>parseVoterConnections</code>...FIXME</p> <p><code>parseVoterConnections</code> is used when:</p> <ul> <li><code>RaftConfig</code> is created and requested to quorumVoterStringsToNodes</li> <li><code>KafkaConfig</code> is requested to validateValues</li> <li><code>KafkaRaftServer</code> is created</li> <li><code>KafkaServer</code> is requested to start up</li> </ul>"},{"location":"kraft/RaftManager/","title":"RaftManager","text":"<p><code>RaftManager</code> is an abstraction of Raft managers.</p>"},{"location":"kraft/RaftManager/#contract","title":"Contract","text":""},{"location":"kraft/RaftManager/#client","title":"client","text":"<pre><code>client: RaftClient[T]\n</code></pre> <p>RaftClient</p> <p>See:</p> <ul> <li>KafkaRaftManager</li> </ul>"},{"location":"kraft/RaftManager/#leaderAndEpoch","title":"leaderAndEpoch","text":"<pre><code>leaderAndEpoch: LeaderAndEpoch\n</code></pre> <p>See:</p> <ul> <li>KafkaRaftManager</li> </ul> <p>Used when:</p> <ul> <li><code>RaftControllerNodeProvider</code> is requested to getControllerInfo</li> </ul>"},{"location":"kraft/RaftManager/#implementations","title":"Implementations","text":"<ul> <li>KafkaRaftManager</li> </ul>"},{"location":"kraft/SharedServer/","title":"SharedServer","text":""},{"location":"kraft/SharedServer/#creating-instance","title":"Creating Instance","text":"<p><code>SharedServer</code> takes the following to be created:</p> <ul> <li> KafkaConfig <li> MetaProperties <li> <code>Time</code> <li> Metrics <li> Controller Quorum Voters (<code>CompletableFuture[util.Map[Integer, AddressSpec]]</code>) <li> <code>FaultHandlerFactory</code> <p><code>SharedServer</code> is created alongside KafkaRaftServer.</p>"},{"location":"log/","title":"LogManager","text":"<p>LogManager</p>"},{"location":"log/CleanerConfig/","title":"CleanerConfig","text":"<p><code>CleanerConfig</code> contains the configuration parameters of the LogCleaner.</p> Parameter Configuration Property Default Value numThreads log.cleaner.threads 1 dedupeBufferSize logCleanerDedupeBufferSize 4*1024*1024L dedupeBufferLoadFactor logCleanerDedupeBufferLoadFactor 0.9d ioBufferSize logCleanerIoBufferSize 1024*1024 maxMessageSize messageMaxBytes 32*1024*1024 maxIoBytesPerSecond logCleanerIoMaxBytesPerSecond Double.MaxValue backOffMs log.cleaner.backoff.ms 15*1000 enableCleaner log.cleaner.enable true hashAlgorithm MD5"},{"location":"log/CleanerThread/","title":"CleanerThread","text":"<p><code>CleanerThread</code> is a non-daemon thread of execution for LogCleaner for log cleanup (one at a time until no more is left).</p>"},{"location":"log/CleanerThread/#creating-instance","title":"Creating Instance","text":"<p><code>CleanerThread</code> takes the following to be created:</p> <ul> <li> Thread ID <p><code>CleanerThread</code> is created when:</p> <ul> <li><code>LogCleaner</code> is requested to start up</li> </ul>"},{"location":"log/CleanerThread/#thread-name","title":"Thread Name <p><code>CleanerThread</code> uses the following as the thread name (with the threadId)</p> <pre><code>kafka-log-cleaner-thread-[threadId]\n</code></pre>","text":""},{"location":"log/CleanerThread/#logcleanerthreads","title":"log.cleaner.threads <p>The number of <code>CleanerThreads</code> (that LogCleaner uses) is controlled by log.cleaner.threads dynamic configuration.</p>","text":""},{"location":"log/CleanerThread/#dowork","title":"doWork <pre><code>doWork(): Unit\n</code></pre> <p><code>doWork</code> is part of the <code>ShutdownableThread</code> abstraction.</p>  <p><code>doWork</code> tryCleanFilthiestLog. If no logs was cleaned up, <code>doWork</code> pauses the thread for log.cleaner.backoff.ms millis.</p> <p>In the end, <code>doWork</code> requests the LogCleanerManager to maintainUncleanablePartitions.</p>","text":""},{"location":"log/CleanerThread/#trycleanfilthiestlog","title":"tryCleanFilthiestLog <pre><code>tryCleanFilthiestLog(): Boolean\n</code></pre> <p><code>tryCleanFilthiestLog</code> cleanFilthiestLog.</p>","text":""},{"location":"log/CleanerThread/#logcleaningexception","title":"LogCleaningException <p>In case of <code>LogCleaningException</code>, <code>tryCleanFilthiestLog</code> prints out the following WARN message to the logs:</p> <pre><code>Unexpected exception thrown when cleaning log [log].\nMarking its partition ([topicPartition]) as uncleanable\n</code></pre> <p><code>doWork</code> requests the LogCleanerManager to maintainUncleanablePartitions and returns <code>false</code>.</p>","text":""},{"location":"log/CleanerThread/#cleaning-filthiest-log","title":"Cleaning Filthiest Log <pre><code>cleanFilthiestLog(): Boolean\n</code></pre> <p><code>cleanFilthiestLog</code> returns <code>cleaned</code> flag to indicate whether a log to clean was found or not.</p>  <p><code>cleanFilthiestLog</code> requests the LogCleanerManager to grabFilthiestCompactedLog.</p> <p>If there is no log to clean up, <code>cleanFilthiestLog</code> \"returns\" <code>false</code> (as <code>cleaned</code> flag). Otherwise, <code>cleanFilthiestLog</code> cleanLog and \"returns\" <code>true</code>.</p> <p><code>cleanFilthiestLog</code> requests the LogCleanerManager for deletableLogs and then every UnifiedLog to deleteOldSegments.</p> <p>In the end, <code>cleanFilthiestLog</code> requests the LogCleanerManager to doneDeleting (with the <code>TopicPartition</code>s).</p>","text":""},{"location":"log/CleanerThread/#logging","title":"Logging <p><code>CleanerThread</code> uses kafka.log.LogCleaner logger.</p>","text":""},{"location":"log/LocalLog/","title":"LocalLog","text":""},{"location":"log/LocalLog/#creating-instance","title":"Creating Instance","text":"<p><code>LocalLog</code> takes the following to be created:</p> <ul> <li> Directory <li> LogConfig <li> <code>LogSegments</code> <li> Recovery Point <li> <code>LogOffsetMetadata</code> <li> <code>Scheduler</code> <li> <code>Time</code> <li> <code>TopicPartition</code> <li> <code>LogDirFailureChannel</code> <p><code>LocalLog</code> is created when:</p> <ul> <li><code>UnifiedLog</code> is requested to create a UnifiedLog</li> </ul>"},{"location":"log/LocalLog/#truncateFullyAndStartAt","title":"truncateFullyAndStartAt","text":"<pre><code>truncateFullyAndStartAt(\n  newOffset: Long): Iterable[LogSegment]\n</code></pre> <p><code>truncateFullyAndStartAt</code>...FIXME</p> <p><code>truncateFullyAndStartAt</code> is used when:</p> <ul> <li><code>UnifiedLog</code> is requested to truncateFullyAndStartAt</li> </ul>"},{"location":"log/LocalLog/#roll","title":"roll","text":"<pre><code>roll(\n  expectedNextOffset: Option[Long] = None): LogSegment\n</code></pre> <p><code>roll</code>...FIXME</p> <p><code>roll</code> is used when:</p> <ul> <li><code>UnifiedLog</code> is requested to roll</li> </ul>"},{"location":"log/LocalLog/#splitOverflowedSegment","title":"splitOverflowedSegment","text":"<pre><code>splitOverflowedSegment(\n  segment: LogSegment,\n  existingSegments: LogSegments,\n  dir: File,\n  topicPartition: TopicPartition,\n  config: LogConfig,\n  scheduler: Scheduler,\n  logDirFailureChannel: LogDirFailureChannel,\n  logPrefix: String): SplitSegmentResult\n</code></pre> <p><code>splitOverflowedSegment</code>...FIXME</p> <p><code>splitOverflowedSegment</code> is used when:</p> <ul> <li><code>UnifiedLog</code> is requested to splitOverflowedSegment</li> </ul>"},{"location":"log/LocalLog/#createNewCleanedSegment","title":"createNewCleanedSegment","text":"<pre><code>createNewCleanedSegment(\n  dir: File,\n  logConfig: LogConfig,\n  baseOffset: Long): LogSegment\n</code></pre> <p><code>createNewCleanedSegment</code>...FIXME</p> <p><code>createNewCleanedSegment</code> is used when:</p> <ul> <li><code>LocalLog</code> is requested to splitOverflowedSegment</li> <li><code>UnifiedLog</code> is requested to createNewCleanedSegment</li> </ul>"},{"location":"log/LocalLog/#createAndDeleteSegment","title":"createAndDeleteSegment","text":"<pre><code>createAndDeleteSegment(\n  newOffset: Long,\n  segmentToDelete: LogSegment,\n  asyncDelete: Boolean,\n  reason: SegmentDeletionReason): LogSegment\n</code></pre> <p><code>createAndDeleteSegment</code>...FIXME</p> <p><code>createAndDeleteSegment</code> is used when:</p> <ul> <li><code>LocalLog</code> is requested to roll, truncateFullyAndStartAt</li> </ul>"},{"location":"log/LogCleaner/","title":"LogCleaner","text":"<p><code>LogCleaner</code> is created and started immediately for LogManager when started with log.cleaner.enable configuration property enabled.</p>"},{"location":"log/LogCleaner/#creating-instance","title":"Creating Instance","text":"<p><code>LogCleaner</code> takes the following to be created:</p> <ul> <li> CleanerConfig <li> Log directories <li> UnifiedLogs by <code>TopicPartition</code> (<code>Pool[TopicPartition, UnifiedLog]</code>) <li> <code>LogDirFailureChannel</code> <li> <code>Time</code> <p><code>LogCleaner</code> is created when:</p> <ul> <li><code>LogManager</code> is requested to startupWithConfigOverrides (with log.cleaner.enable enabled)</li> </ul>"},{"location":"log/LogCleaner/#logcleanermanager","title":"LogCleanerManager <p><code>LogCleaner</code> creates a LogCleanerManager (with the log directories and UnifiedLogs) when created.</p>","text":""},{"location":"log/LogCleaner/#kafkametricsgroup","title":"KafkaMetricsGroup <p><code>LogCleaner</code> is a KafkaMetricsGroup.</p>","text":""},{"location":"log/LogCleaner/#brokerreconfigurable","title":"BrokerReconfigurable <p><code>LogCleaner</code> is a BrokerReconfigurable.</p>","text":""},{"location":"log/LogCleaner/#starting-up","title":"Starting Up <pre><code>startup(): Unit\n</code></pre> <p><code>startup</code> prints out the following INFO message to the logs:</p> <pre><code>Starting the log cleaner\n</code></pre> <p><code>startup</code> creates log.cleaner.threads of CleanerThreads that are started immediately.</p> <p><code>startup</code> adds the <code>CleanerThread</code>s to the cleaners registry.</p>  <p><code>startup</code> is used when:</p> <ul> <li><code>LogCleaner</code> is requested to reconfigure</li> <li><code>LogManager</code> is requested to startupWithConfigOverrides (with log.cleaner.enable enabled)</li> </ul>","text":""},{"location":"log/LogCleaner/#creating-cleanerconfig","title":"Creating CleanerConfig <pre><code>cleanerConfig(\n  config: KafkaConfig): CleanerConfig\n</code></pre> <p><code>cleanerConfig</code> creates a CleanerConfig with the configuration properties from the given KafkaConfig.</p>  <p><code>cleanerConfig</code> is used when:</p> <ul> <li><code>LogCleaner</code> is requested to validateReconfiguration and reconfigure</li> <li><code>LogManager</code> utility is used to create a LogManager</li> </ul>","text":""},{"location":"log/LogCleaner/#cleanerthreads","title":"CleanerThreads <p><code>LogCleaner</code> uses CleanerThreads for log cleaning.</p> <p><code>LogCleaner</code> creates log.cleaner.threads of CleanerThreads when started up. They are shut down when shutdown.</p> <p>Use the following metrics to monitor the threads:</p> <ul> <li>cleaner-recopy-percent</li> <li>DeadThreadCount</li> </ul>","text":""},{"location":"log/LogCleaner/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>kafka.log.LogCleaner</code> logger to see what happens inside.</p> <p>Add the following line to <code>config/log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.log.LogCleaner=ALL\n</code></pre> <p>Refer to Logging.</p>  <p>Note</p> <p>Kafka comes with a preconfigured <code>kafka.log.LogCleaner</code> logger in <code>config/log4j.properties</code>:</p> <pre><code>log4j.appender.cleanerAppender=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.cleanerAppender.DatePattern='.'yyyy-MM-dd-HH\nlog4j.appender.cleanerAppender.File=${kafka.logs.dir}/log-cleaner.log\nlog4j.appender.cleanerAppender.layout=org.apache.log4j.PatternLayout\nlog4j.appender.cleanerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n\n\nlog4j.logger.kafka.log.LogCleaner=INFO, cleanerAppender\nlog4j.additivity.kafka.log.LogCleaner=false\n</code></pre> <p>That means that the logs go to <code>logs/log-cleaner.log</code> file at <code>INFO</code> logging level and are not added to the main logs (per <code>log4j.additivity</code>).</p>","text":""},{"location":"log/LogCleanerManager/","title":"LogCleanerManager","text":"<p><code>LogCleanerManager</code> is used exclusively by LogCleaner to manage the state of partitions for Log Cleanup.</p>"},{"location":"log/LogCleanerManager/#creating-instance","title":"Creating Instance","text":"<p><code>LogCleanerManager</code> takes the following to be created:</p> <ul> <li> Log directories <li> <code>TopicPartition</code>s and their UnifiedLogs (<code>Pool[TopicPartition, UnifiedLog]</code>) <li> <code>LogDirFailureChannel</code> <p><code>LogCleanerManager</code> is created along with a LogCleaner.</p>"},{"location":"log/LogCleanerManager/#performance-metrics","title":"Performance Metrics","text":"<p><code>LogCleanerManager</code> is a KafkaMetricsGroup and registers the following performance metrics in kafka.log:type=LogCleanerManager group.</p> Metric Name Description max-dirty-percent time-since-last-run-ms uncleanable-bytes (for every log directory) uncleanable-partitions-count (for every log directory) <p></p>"},{"location":"log/LogCleanerManager/#grabfilthiestcompactedlog","title":"grabFilthiestCompactedLog <pre><code>grabFilthiestCompactedLog(\n  time: Time,\n  preCleanStats: PreCleanStats = new PreCleanStats()): Option[LogToClean]\n</code></pre> <p><code>grabFilthiestCompactedLog</code>...FIXME</p>  <p><code>grabFilthiestCompactedLog</code> is used when:</p> <ul> <li><code>CleanerThread</code> is requested to clean the filthiest log</li> </ul>","text":""},{"location":"log/LogCleanerManager/#iscompactanddelete","title":"isCompactAndDelete <pre><code>isCompactAndDelete(\n  log: UnifiedLog): Boolean\n</code></pre> <p><code>isCompactAndDelete</code> holds true when the given UnifiedLog is compact and delete.</p>  <p><code>isCompactAndDelete</code> holds true only when both compact and delete cleanup policies are included in the cleanup.policy configuration property. Otherwise, <code>isCompactAndDelete</code> flag is disabled (<code>false</code>).</p>","text":""},{"location":"log/LogCleanerManager/#logging","title":"Logging <p><code>LogCleanerManager</code> uses kafka.log.LogCleaner logger.</p>","text":""},{"location":"log/LogConfig/","title":"LogConfig","text":""},{"location":"log/LogConfig/#cleanuppolicy","title":"cleanup.policy <p>TopicConfig</p>","text":""},{"location":"log/LogConfig/#compact","title":"compact <p><code>compact</code> is <code>true</code> when cleanup.policy contains <code>compact</code> policy.</p>  <p><code>compact</code> is used when:</p> <ul> <li><code>LogCleanerManager</code> is requested to grabFilthiestCompactedLog, pauseCleaningForNonCompactedPartitions, deletableLogs, maybeTruncateCheckpoint, isCompactAndDelete</li> <li><code>LogConfig</code> is requested for maxSegmentMs</li> <li><code>LogManager</code> is requested to updateTopicConfig and cleanupLogs</li> <li><code>UnifiedLog</code> is requested to append</li> </ul>","text":""},{"location":"log/LogConfig/#leaderreplicationthrottledreplicas","title":"leader.replication.throttled.replicas <p>A list of replicas for which log replication should be throttled on the leader side.</p> <p>The list should describe a set of replicas in the form <code>[PartitionId]:[BrokerId],[PartitionId]:[BrokerId]:...</code> or alternatively the wildcard <code>*</code> can be used to throttle all replicas for this topic.</p> <p>Default: (empty)</p> <p>Use <code>LogConfig.LeaderReplicationThrottledReplicas</code> to access the current value</p> <p>Used when:</p> <ul> <li><code>ReassignPartitionsCommand</code> is created</li> <li><code>TopicConfigHandler</code> is requested to processConfigChanges</li> </ul>","text":""},{"location":"log/LogConfig/#followerreplicationthrottledreplicas","title":"follower.replication.throttled.replicas <p>A list of replicas for which log replication should be throttled on the follower side.</p> <p>The list should describe a set of replicas in the form <code>[PartitionId]:[BrokerId],[PartitionId]:[BrokerId]:...</code> or alternatively the wildcard <code>*</code> can be used to throttle all replicas for this topic.</p> <p>Default: (empty)</p> <p>Use <code>LogConfig.FollowerReplicationThrottledReplicas</code> to access the current value</p> <p>Used when:</p> <ul> <li><code>ReassignPartitionsCommand</code> is created</li> <li><code>TopicConfigHandler</code> is requested to processConfigChanges</li> </ul>","text":""},{"location":"log/LogConfig/#uncleanleaderelectionenable","title":"unclean.leader.election.enable <p>TopicConfig</p> <p>Default: <code>false</code> (disabled)</p> <p>Use <code>LogConfig.uncleanLeaderElectionEnable</code> to access the current value.</p> <p>KafkaConfig</p> <p><code>TransactionStateManager</code> disables the property explicitly (<code>false</code>) for __transaction_state topic.</p> <p>Used in extractLogConfigMap</p>","text":""},{"location":"log/LogConfig/#utilities","title":"Utilities","text":""},{"location":"log/LogConfig/#extractlogconfigmap","title":"extractLogConfigMap <pre><code>extractLogConfigMap(\n  kafkaConfig: KafkaConfig): Map[String, Object]\n</code></pre> <p><code>extractLogConfigMap</code>...FIXME</p> <p><code>extractLogConfigMap</code> is used when:</p> <ul> <li><code>LogManager</code> is created</li> <li><code>ConfigHelper</code> is requested to <code>describeConfigs</code></li> <li><code>ZkAdminManager</code> is requested to maybePopulateMetadataAndConfigs</li> </ul>","text":""},{"location":"log/LogLoader/","title":"LogLoader","text":""},{"location":"log/LogLoader/#creating-instance","title":"Creating Instance","text":"<p><code>LogLoader</code> takes the following to be created:</p> <ul> <li> Directory <li> <code>TopicPartition</code> <li> LogConfig <li> <code>Scheduler</code> <li> <code>Time</code> <li> <code>LogDirFailureChannel</code> <li> <code>hadCleanShutdown</code> flag <li> <code>LogSegments</code> <li> <code>logStartOffsetCheckpoint</code> <li> <code>recoveryPointCheckpoint</code> <li> <code>LeaderEpochFileCache</code> <li> <code>ProducerStateManager</code> <li> <code>numRemainingSegments</code> <li> <code>isRemoteLogEnabled</code> flag (default: <code>false</code>) <p><code>LogLoader</code> is created when:</p> <ul> <li><code>UnifiedLog</code> is requested to create a UnifiedLog</li> </ul>"},{"location":"log/LogLoader/#load","title":"Loading","text":"<pre><code>load(): LoadedLogOffsets\n</code></pre> <p><code>load</code>...FIXME</p> <p><code>load</code> is used when:</p> <ul> <li><code>UnifiedLog</code> is requested to create a UnifiedLog</li> </ul>"},{"location":"log/LogManager/","title":"LogManager","text":""},{"location":"log/LogManager/#creating-instance","title":"Creating Instance","text":"<p><code>LogManager</code> takes the following to be created:</p> <ul> <li> Log directories <li> Initial offline directories <li> <code>ConfigRepository</code> <li> <code>LogConfig</code> <li> <code>CleanerConfig</code> <li> num.recovery.threads.per.data.dir <li> log.flush.scheduler.interval.ms <li> log.flush.offset.checkpoint.interval.ms <li> log.flush.start.offset.checkpoint.interval.ms <li> log.retention.check.interval.ms <li> transaction.max.timeout.ms <li> transactional.id.expiration.ms <li> inter.broker.protocol.version <li> <code>Scheduler</code> <li> <code>BrokerTopicStats</code> <li> <code>LogDirFailureChannel</code> <li> <code>Time</code> <li> <code>keepPartitionMetadataFile</code> <p><code>LogManager</code> is created using apply factory method.</p>"},{"location":"log/LogManager/#creating-logmanager","title":"Creating LogManager <pre><code>apply(\n  config: KafkaConfig,\n  initialOfflineDirs: Seq[String],\n  configRepository: ConfigRepository,\n  kafkaScheduler: KafkaScheduler,\n  time: Time,\n  brokerTopicStats: BrokerTopicStats,\n  logDirFailureChannel: LogDirFailureChannel,\n  keepPartitionMetadataFile: Boolean): LogManager\n</code></pre> <p><code>apply</code> extracts log-related configuration properties (from the given KafkaConfig) and creates a LogConfig.</p> <p><code>apply</code> creates a LogCleaner.</p> <p>In the end, <code>apply</code> creates a LogManager based on some configuration properties.</p>  <p><code>apply</code> is used when:</p> <ul> <li><code>BrokerServer</code> is requested to start up</li> <li><code>KafkaServer</code> is requested to start up</li> </ul>","text":""},{"location":"log/LogManager/#kafkametricsgroup","title":"KafkaMetricsGroup <p><code>LogManager</code> is a KafkaMetricsGroup.</p>","text":""},{"location":"log/LogManager/#current-logs","title":"Current Logs <pre><code>currentLogs: Pool[TopicPartition, UnifiedLog]\n</code></pre> <p><code>LogManager</code> defines <code>currentLogs</code> internal registry of UnifiedLogs per <code>TopicPartition</code>.</p> <p><code>LogManager</code> uses the <code>currentLogs</code> registry when:</p> <ul> <li>startupWithConfigOverrides (to create a LogCleaner when enabled)</li> <li>handleLogDirFailure</li> <li>loadLog</li> <li>truncateTo</li> <li>truncateFullyAndStartAt</li> <li>getLog</li> <li>getOrCreateLog</li> <li>and many more</li> </ul>","text":""},{"location":"log/LogManager/#looking-up-log","title":"Looking Up Log <pre><code>getLog(\n  topicPartition: TopicPartition,\n  isFuture: Boolean = false): Option[UnifiedLog]\n</code></pre> <p>With the input <code>isFuture</code> enabled, <code>getLog</code> uses the futureLogs registry to look up the UnifiedLog for the input <code>TopicPartition</code> (if available). Otherwise, <code>getLog</code> uses the currentLogs registry.</p>  <p><code>getLog</code> is used when:</p> <ul> <li><code>Partition</code> is requested to topicId and getOffsetByTimestamp</li> <li><code>LogManager</code> is requested to maybeUpdatePreferredLogDir, getOrCreateLog, asyncDelete</li> <li><code>ReplicaManager</code> is requested to getLog, maybeAddLogDirFetchers</li> </ul>","text":""},{"location":"log/LogManager/#getOrCreateLog","title":"getOrCreateLog <pre><code>getOrCreateLog(\n  topicPartition: TopicPartition,\n  isNew: Boolean = false,\n  isFuture: Boolean = false,\n  topicId: Option[Uuid]): UnifiedLog\n</code></pre> <p><code>getOrCreateLog</code>...FIXME</p>  <p><code>getOrCreateLog</code> is used when:</p> <ul> <li><code>Partition</code> is requested to createLog</li> </ul>","text":""},{"location":"log/LogManager/#startup","title":"startup <pre><code>startup(\n  topicNames: Set[String]): Unit\n</code></pre> <p><code>startup</code> startupWithConfigOverrides with the currentDefaultConfig and fetchTopicConfigOverrides.</p>  <p><code>startup</code> is used when:</p> <ul> <li><code>KafkaServer</code> is requested to startup</li> <li><code>BrokerMetadataPublisher</code> is requested to initializeManagers</li> </ul>","text":""},{"location":"log/LogManager/#startupWithConfigOverrides","title":"startupWithConfigOverrides <pre><code>startupWithConfigOverrides(\n  defaultConfig: LogConfig,\n  topicConfigOverrides: Map[String, LogConfig]): Unit\n</code></pre> <p><code>startupWithConfigOverrides</code>...FIXME</p>","text":""},{"location":"log/LogManager/#loadLogs","title":"loadLogs <pre><code>loadLogs(\n  defaultConfig: LogConfig,\n  topicConfigOverrides: Map[String, LogConfig]): Unit\n</code></pre> <p><code>loadLogs</code>...FIXME</p>","text":""},{"location":"log/LogManager/#loadLog","title":"loadLog <pre><code>loadLog(\n  logDir: File,\n  hadCleanShutdown: Boolean,\n  recoveryPoints: Map[TopicPartition, Long],\n  logStartOffsets: Map[TopicPartition, Long],\n  defaultConfig: LogConfig,\n  topicConfigOverrides: Map[String, LogConfig],\n  numRemainingSegments: ConcurrentMap[String, Int]): UnifiedLog\n</code></pre> <p><code>loadLog</code>...FIXME</p>","text":""},{"location":"log/LogManager/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>kafka.log.LogManager</code> logger to see what happens inside.</p> <p>Add the following line to <code>config/log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.log.LogManager=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"log/LogSegment/","title":"LogSegment","text":""},{"location":"log/LogSegment/#creating-instance","title":"Creating Instance","text":"<p><code>LogSegment</code> takes the following to be created:</p> <ul> <li> <code>FileRecords</code> <li> <code>OffsetIndex</code> <li> <code>TimeIndex</code> <li> <code>TransactionIndex</code> <li> baseOffset <li> indexIntervalBytes <li> rollJitterMs <li> <code>Time</code> <p><code>LogSegment</code> is created\u00a0using open utility.</p>"},{"location":"log/LogSegment/#open","title":"Opening LogSegment","text":"<pre><code>open(\n  dir: File,\n  baseOffset: Long,\n  config: LogConfig,\n  time: Time,\n  fileAlreadyExists: Boolean = false,\n  initFileSize: Int = 0,\n  preallocate: Boolean = false,\n  fileSuffix: String = \"\"): LogSegment\n</code></pre> <p><code>open</code>...FIXME</p> <p><code>open</code>\u00a0is used when:</p> <ul> <li><code>LocalLog</code> is requested to createAndDeleteSegment, roll, createNewCleanedSegment</li> <li><code>LogLoader</code> is requested to load, loadSegmentFiles, recoverLog</li> </ul>"},{"location":"log/LogSegment/#read","title":"Reading Messages","text":"<pre><code>read(\n  startOffset: Long,\n  maxSize: Int,\n  maxPosition: Long = size,\n  minOneMessage: Boolean = false): FetchDataInfo\n</code></pre> <p><code>read</code>...FIXME</p> <p><code>read</code>\u00a0is used when:</p> <ul> <li><code>Log</code> is requested to read</li> <li><code>LogSegment</code> is requested to readNextOffset</li> </ul>"},{"location":"log/UnifiedLog/","title":"UnifiedLog","text":""},{"location":"log/UnifiedLog/#creating-instance","title":"Creating Instance","text":"<p><code>UnifiedLog</code> takes the following to be created:</p> <ul> <li> <code>logStartOffset</code> <li> LocalLog <li> <code>BrokerTopicStats</code> <li> <code>producerIdExpirationCheckIntervalMs</code> <li> <code>LeaderEpochFileCache</code> <li> <code>ProducerStateManager</code> <li> Topic ID <li> <code>keepPartitionMetadataFile</code> <li> <code>remoteStorageSystemEnable</code> (default: <code>false</code>) <li> <code>LogOffsetsListener</code> <p><code>UnifiedLog</code> is created using apply utility.</p>"},{"location":"log/UnifiedLog/#apply","title":"Creating UnifiedLog","text":"<pre><code>apply(\n  dir: File,\n  config: LogConfig,\n  logStartOffset: Long,\n  recoveryPoint: Long,\n  scheduler: Scheduler,\n  brokerTopicStats: BrokerTopicStats,\n  time: Time,\n  maxTransactionTimeoutMs: Int,\n  producerStateManagerConfig: ProducerStateManagerConfig,\n  producerIdExpirationCheckIntervalMs: Int,\n  logDirFailureChannel: LogDirFailureChannel,\n  lastShutdownClean: Boolean = true,\n  topicId: Option[Uuid],\n  keepPartitionMetadataFile: Boolean,\n  numRemainingSegments: ConcurrentMap[String, Int] = new ConcurrentHashMap[String, Int],\n  remoteStorageSystemEnable: Boolean = false,\n  logOffsetsListener: LogOffsetsListener = LogOffsetsListener.NO_OP_OFFSETS_LISTENER): UnifiedLog\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is used when:</p> <ul> <li><code>LogManager</code> is requested to startup (and loadLog), getOrCreateLog</li> <li><code>KafkaMetadataLog</code> is requested to create a KafkaMetadataLog</li> </ul>"},{"location":"log-cleanup/","title":"Log Cleanup","text":"<p>Kafka uses cleanup.policy configuration property to apply cleanup strategies (policy) to logs:</p> <ul> <li>Log Compaction</li> <li>Log Retention</li> </ul>"},{"location":"log-cleanup/#configuration-properties","title":"Configuration Properties <p>The cluster-wide log.cleanup.policy and the per-topic cleanup.policy configuration properties are comma-separated lists of cleanup strategies:</p> <ul> <li> <code>compact</code> - enables log compaction <li> <code>delete</code> - enables log retention  <p>Unless defined, cleanup.policy is exactly log.cleanup.policy.</p>","text":""},{"location":"log-cleanup/#log-compaction","title":"Log Compaction <p>Log Compaction is a cleanup strategy in which...FIXME</p> <p>Kafka brokers use LogCleaner for compact retention strategy.</p> <p>Log compaction can be reconfigured dynamically at runtime.</p>","text":""},{"location":"log-cleanup/#log-retention","title":"Log Retention <p>Log Retention (Garbage Collection) is a cleanup strategy to discard (delete) old log segments when their retention time or size limit has been reached.</p> <p>By default there is only a time limit and no size limit.</p> <p>Kafka brokers schedule <code>kafka-log-retention</code> periodic task for delete retention strategy.</p> <p>Kafka uses log.retention.check.interval.ms configuration property as the interval between regular log checks.</p>","text":""},{"location":"log-cleanup/#time-based-retention","title":"Time-Based Retention","text":"<p>Retention Time is controlled by the cluster-wide log.retention.ms, log.retention.minutes or log.retention.hours configuration properties (from the highest to the lowest priority) or their per-topic retention.ms configuration property.</p>"},{"location":"log-cleanup/#size-based-retention","title":"Size-Based Retention","text":"<p>Retention Size is controlled by the cluster-wide log.retention.bytes or per-topic retention.bytes configuration property.</p>"},{"location":"log-cleanup/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for kafka.log.Log logger to see messages related to log retention.</p>","text":""},{"location":"metadata/","title":"Kafka Metadata","text":""},{"location":"metadata/BrokerMetadataPublisher/","title":"BrokerMetadataPublisher","text":"<p><code>BrokerMetadataPublisher</code> is...FIXME</p>"},{"location":"metadata/MetadataLoader/","title":"MetadataLoader","text":"<p><code>MetadataLoader</code> is built for a SharedServer.</p>"},{"location":"metadata/MetadataLoader/#creating-instance","title":"Creating Instance","text":"<p><code>MetadataLoader</code> takes the following to be created:</p> <ul> <li> <code>Time</code> <li> <code>LogContext</code> <li> Node ID (not used) <li> Thread Name Prefix <li> <code>FaultHandler</code> <li> <code>MetadataLoaderMetrics</code> <li>HighWaterMarkAccessor</li> <p><code>MetadataLoader</code> is created when:</p> <ul> <li><code>MetadataLoader.Builder</code> is requested to build a MetadataLoader</li> </ul>"},{"location":"metadata/MetadataLoader/#highWaterMarkAccessor","title":"HighWaterMarkAccessor","text":"<p><code>MetadataLoader</code> is given a high watermark accessor (a <code>Supplier&lt;OptionalLong&gt;</code>) when created (when <code>SharedServer</code> is requested to start).</p> <p>The high watermark accessor is the highWatermark from the KafkaRaftClient of a KafkaRaftManager.</p>"},{"location":"metadata/MetadataPublisher/","title":"MetadataPublisher","text":"<p><code>MetadataPublisher</code> is an abstraction of metadata publishers.</p>"},{"location":"metadata/MetadataPublisher/#contract-subset","title":"Contract (Subset)","text":""},{"location":"metadata/MetadataPublisher/#onMetadataUpdate","title":"onMetadataUpdate","text":"<pre><code>void onMetadataUpdate(\n  MetadataDelta delta,\n  MetadataImage newImage,\n  LoaderManifest manifest)\n</code></pre> <p>See:</p> <ul> <li>AclPublisher</li> <li>BrokerMetadataPublisher</li> </ul> <p>Used when:</p> <ul> <li><code>MetadataLoader</code> is requested to initializeNewPublishers and maybePublishMetadata</li> <li><code>BrokerMetadataPublisher</code> is requested to onMetadataUpdate</li> </ul>"},{"location":"metadata/MetadataPublisher/#implementations","title":"Implementations","text":"<ul> <li>AclPublisher</li> <li>BrokerMetadataPublisher</li> <li><code>ControllerMetadataMetricsPublisher</code></li> <li><code>DelegationTokenPublisher</code></li> <li><code>DynamicClientQuotaPublisher</code></li> <li><code>DynamicConfigPublisher</code></li> <li><code>FeaturesPublisher</code></li> <li><code>KRaftMigrationDriver</code></li> <li>MetadataShellPublisher</li> <li><code>ScramPublisher</code></li> <li><code>SnapshotGenerator</code></li> </ul>"},{"location":"metrics/","title":"Metrics","text":""},{"location":"metrics/KafkaMetricsGroup/","title":"KafkaMetricsGroup","text":"<p><code>KafkaMetricsGroup</code> is...FIXME</p>"},{"location":"metrics/MetricConfig/","title":"MetricConfig","text":""},{"location":"metrics/MetricConfig/#creating-instance","title":"Creating Instance","text":"<p><code>MetricConfig</code> takes no arguments to be created.</p> <p><code>MetricConfig</code> is created when:</p> <ul> <li>many places (FIXME)</li> </ul>"},{"location":"metrics/MetricConfig/#recordinglevel","title":"RecordingLevel <p><code>MetricConfig</code> uses <code>INFO</code> recording level by default (when created) that can be changed using recordLevel.</p>","text":""},{"location":"metrics/MetricConfig/#recordlevel","title":"recordLevel <pre><code>MetricConfig recordLevel(\n  Sensor.RecordingLevel recordingLevel)\n</code></pre> <p><code>recordLevel</code> sets the recordingLevel to the given <code>RecordingLevel</code>.</p> <p><code>recordLevel</code> is used when:</p> <ul> <li><code>KafkaAdminClient</code> is requested to <code>createInternal</code></li> <li><code>KafkaConsumer</code> is requested to buildMetrics</li> <li><code>KafkaProducer</code> is created</li> <li><code>KafkaStreams</code> (Kafka Streams) is requested to <code>getMetrics</code></li> <li><code>StreamsMetricsImpl</code> (Kafka Streams) is requested to <code>addClientLevelImmutableMetric</code>, <code>addClientLevelMutableMetric</code>, <code>addStoreLevelMutableMetric</code></li> <li><code>Server</code> utility is used to buildMetricsConfig</li> </ul>","text":""},{"location":"metrics/Metrics/","title":"Metrics","text":"<p><code>Metrics</code> is a registry of sensors and performance metrics (of Kafka brokers and clients).</p>"},{"location":"metrics/Metrics/#creating-instance","title":"Creating Instance","text":"<p><code>Metrics</code> takes the following to be created:</p> <ul> <li> MetricConfig <li> <code>MetricsReporter</code>s <li> <code>Time</code> <li> <code>enableExpiration</code> flag <li> <code>MetricsContext</code> <p><code>Metrics</code> is created when:</p> <ul> <li><code>Server</code> utility is used to buildMetrics</li> <li><code>KafkaAdminClient</code> utility is used to <code>createInternal</code></li> <li><code>KafkaConsumer</code> utility is used to buildMetrics</li> <li><code>KafkaProducer</code> is created</li> <li><code>KafkaStreams</code> (Kafka Streams) utility is used to <code>getMetrics</code></li> <li>Kafka Connect clients</li> </ul>"},{"location":"metrics/Metrics/#addreporter","title":"addReporter <pre><code>void addReporter(\n  MetricsReporter reporter)\n</code></pre> <p><code>addReporter</code>...FIXME</p> <p><code>addReporter</code> is used when:</p> <ul> <li><code>DynamicMetricsReporters</code> is requested to <code>createReporters</code></li> </ul>","text":""},{"location":"metrics/Metrics/#sensors","title":"sensors <p><code>Metrics</code> defines <code>sensors</code> collection of metric <code>Sensor</code>s by name (<code>ConcurrentMap&lt;String, Sensor&gt;</code>).</p> <p><code>sensors</code> is empty when <code>Metrics</code> is created.</p> <p>A new <code>Sensor</code> is added in sensor.</p>","text":""},{"location":"metrics/Metrics/#sensor","title":"sensor <pre><code>Sensor sensor(\n  String name,\n  MetricConfig config,\n  long inactiveSensorExpirationTimeSeconds,\n  Sensor.RecordingLevel recordingLevel,\n  Sensor... parents)\nSensor sensor(...) // (1)\n</code></pre> <ol> <li>There are others</li> </ol> <p><code>sensor</code> looks up the sensor (by name) and returns it immediately if available.</p> <p>Otherwise, <code>sensor</code> creates a new <code>Sensor</code> and adds it to the sensors registry.</p> <p>In the end, <code>sensor</code> prints out the following TRACE message to the logs:</p> <pre><code>Added sensor with name [name]\n</code></pre>","text":""},{"location":"metrics/Metrics/#getsensor","title":"getSensor <pre><code>Sensor getSensor(\n  String name)\n</code></pre> <p><code>getSensor</code> looks up the given <code>name</code> in the sensors registry.</p>","text":""},{"location":"metrics/Metrics/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.common.metrics.Metrics</code> logger to see what happens inside.</p> <p>Add the following line to <code>config/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.common.metrics.Metrics=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"metrics/MetricsReporter/","title":"MetricsReporter","text":"<p><code>MetricsReporter</code> is an extension of the <code>Reconfigurable</code> abstraction for metrics reporters.</p>"},{"location":"metrics/MetricsReporter/#contract-subset","title":"Contract (Subset)","text":""},{"location":"metrics/MetricsReporter/#init","title":"init <pre><code>void init(\n  List&lt;KafkaMetric&gt; metrics)\n</code></pre> <p>Used when:</p> <ul> <li><code>Metrics</code> is created and requested to addReporter</li> </ul>","text":""},{"location":"metrics/MetricsReporter/#implementations","title":"Implementations","text":"<ul> <li><code>JmxReporter</code></li> <li><code>PushHttpMetricsReporter</code></li> </ul>"},{"location":"metrics/Sensor/","title":"Sensor","text":""},{"location":"metrics/Sensor/#creating-instance","title":"Creating Instance","text":"<p><code>Sensor</code> takes the following to be created:</p> <ul> <li> Metrics <li> Name <li> Parent <code>Sensor</code>s <li> MetricConfig <li> <code>Time</code> <li> <code>inactiveSensorExpirationTimeSeconds</code> <li> <code>RecordingLevel</code> <p><code>Sensor</code> is created when:</p> <ul> <li><code>Metrics</code> is requested for a sensor</li> </ul>"},{"location":"metrics/Sensor/#shouldrecord","title":"shouldRecord <pre><code>boolean shouldRecord()\n</code></pre> <p><code>shouldRecord</code> requests the RecordingLevel to <code>shouldRecord</code> based on the configured RecordingLevel (in the MetricConfig).</p>","text":""},{"location":"partition-leader-election/","title":"Partition Leader Election","text":"<p>Partition Leader Election is a process of electing a broker as the leader of a partition.</p> <p>Use kafka-leader-election utility for preferred or unclean leader election.</p> <p>Note</p> <p><code>kafka-preferred-replica-election.sh</code> tool has been deprecated since Kafka 2.4.0 (cf. KIP-460: Admin Leader Election RPC).</p> <p>Observe <code>state.change.logger</code> (default: <code>state-change.log</code>) to trace the process in the logs.</p> <p>Internally, Kafka controller uses Election utility (and PartitionLeaderElectionAlgorithms) for the algorithms for partition leader election.</p>"},{"location":"partition-leader-election/#preferred-partition-leader-election","title":"Preferred Partition Leader Election","text":"<p>Preferred Partition Leader Election is...FIXME</p>"},{"location":"partition-leader-election/#unclean-partition-leader-election","title":"Unclean Partition Leader Election","text":"<p>Unclean Partition Leader Election allows a non-ISR replica broker to be elected as a partition leader (as the last resort since doing so may result in data loss).</p> <p>unclean.leader.election.enable configuration property is used to enable it cluster-wide (for any topic) or per topic.</p> <p>Enable INFO logging level for kafka.controller.KafkaController logger to observe the process in the logs.</p>"},{"location":"partition-leader-election/#demo","title":"Demo","text":"<p>Demo: Using kafka-leader-election.</p>"},{"location":"tiered-storage/","title":"Kafka Tiered Storage","text":"<p>In tiered storage approach, a Kafka cluster is configured with two tiers of storage: local and remote.</p> <p>The local tier is the same as the current Kafka that uses the local disks on the Kafka brokers to store the log segments. The new remote tier uses systems (e.g., HDFS, S3) to store the completed log segments.</p> <p>Tiered storage is enabled using remote.log.storage.system.enable property.</p> <p>Tiered storage is not supported with multiple log directories.</p>"},{"location":"tiered-storage/#learn-more","title":"Learn More","text":"<ol> <li>KIP-405: Kafka Tiered Storage</li> </ol>"},{"location":"tiered-storage/RemoteLogManager/","title":"RemoteLogManager","text":""},{"location":"tiered-storage/RemoteLogManager/#creating-instance","title":"Creating Instance","text":"<p><code>RemoteLogManager</code> takes the following to be created:</p> <ul> <li> RemoteLogManagerConfig <li> Broker ID <li> Log Directory <li> Cluster ID <li> <code>Time</code> <li> Fetch Log Function (<code>Function&lt;TopicPartition, Optional&lt;UnifiedLog&gt;&gt;</code>) <li> <code>updateRemoteLogStartOffset</code> (<code>BiConsumer&lt;TopicPartition, Long&gt;</code>) <li> <code>BrokerTopicStats</code> <p><code>RemoteLogManager</code> is created when:</p> <ul> <li><code>BrokerServer</code> is requested to create a RemoteLogManager</li> <li><code>KafkaServer</code> is requested to create a RemoteLogManager</li> </ul>"},{"location":"tiered-storage/RemoteLogManagerConfig/","title":"RemoteLogManagerConfig","text":""},{"location":"tiered-storage/RemoteLogManagerConfig/#creating-instance","title":"Creating Instance","text":"<p><code>RemoteLogManagerConfig</code> takes the following to be created:</p> <ul> <li> AbstractConfig <p><code>RemoteLogManagerConfig</code> is created when:</p> <ul> <li><code>KafkaConfig</code> is created</li> </ul>"},{"location":"tiered-storage/RemoteLogManagerConfig/#remote.log.storage.system.enable","title":"remote.log.storage.system.enable <p>remote.log.storage.system.enable</p> <p>Enables Tiered Storage</p> <p>Default: <code>false</code></p> <p>Available as KafkaConfig.isRemoteLogStorageSystemEnabled</p> <p>Used when:</p> <ul> <li><code>LogManager</code> is created</li> <li><code>TopicConfigHandler</code> is requested to updateLogConfig</li> <li><code>ControllerConfigurationValidator</code> is requested to <code>validate</code></li> <li><code>AdminZkClient</code> is requested to validateTopicCreate and validateTopicConfig</li> </ul>","text":""},{"location":"tools/","title":"Tools","text":""},{"location":"tools/ConsoleConsumer/","title":"ConsoleConsumer","text":"<p><code>ConsoleConsumer</code> is...FIXME</p>"},{"location":"tools/kafka-acls/","title":"kafka-acls","text":""},{"location":"tools/kafka-acls/AclCommand/","title":"AclCommand","text":"<p><code>AclCommand</code> is an administration command-line utility to manage ACLs in a Kafka cluster.</p> <p><code>AclCommand</code> can be executed as <code>kafka-acls</code> shell script.</p>"},{"location":"tools/kafka-acls/AclCommand/#securitydisabledexception","title":"SecurityDisabledException <p><code>kafka-acls.sh</code> requires Authorizer to be configured on a broker (when executed with <code>--bootstrap-server</code> option) or throws a <code>SecurityDisabledException</code>.</p> <pre><code>$ ./bin/kafka-acls.sh --list --bootstrap-server :9092\nSecurityDisabledException: No Authorizer is configured on the broker\n</code></pre>","text":""},{"location":"tools/kafka-acls/AclCommand/#executing-command","title":"Executing Command <p><code>main</code> selects the AclCommandService:</p> <ul> <li><code>AdminClientService</code> when <code>--bootstrap-server</code> option is used</li> <li>AuthorizerService with AclAuthorizer otherwise</li> </ul> <p>In the end, <code>main</code> executes the operation:</p> <ul> <li><code>add</code> to add ACLs</li> <li><code>remove</code> to remove ACLs</li> <li><code>list</code> to list ACLs for <code>--topic</code>, <code>--group</code> or <code>--cluster</code> resource types</li> </ul>","text":""},{"location":"tools/kafka-acls/AclCommandOptions/","title":"AclCommandOptions","text":"<p><code>AclCommandOptions</code> is...FIXME</p>"},{"location":"tools/kafka-acls/AclCommandService/","title":"AclCommandService","text":"<p><code>AclCommandService</code> is an abstraction of AclCommand services for AclCommand.</p>"},{"location":"tools/kafka-acls/AclCommandService/#contract","title":"Contract","text":""},{"location":"tools/kafka-acls/AclCommandService/#addacls","title":"addAcls <pre><code>addAcls(): Unit\n</code></pre> <p>Used when:</p> <ul> <li>AclCommand is executed (with <code>--add</code> option)</li> </ul>","text":""},{"location":"tools/kafka-acls/AclCommandService/#listacls","title":"listAcls <pre><code>listAcls(): Unit\n</code></pre> <p>Used when:</p> <ul> <li>AclCommand is executed (with <code>--list</code> option)</li> </ul>","text":""},{"location":"tools/kafka-acls/AclCommandService/#removeacls","title":"removeAcls <pre><code>removeAcls(): Unit\n</code></pre> <p>Used when:</p> <ul> <li>AclCommand is executed (with <code>--remove</code> option)</li> </ul>","text":""},{"location":"tools/kafka-acls/AclCommandService/#implementations","title":"Implementations","text":"<ul> <li>AdminClientService</li> <li>AuthorizerService</li> </ul>"},{"location":"tools/kafka-acls/AdminClientService/","title":"AdminClientService","text":"<p><code>AdminClientService</code> is an AclCommandService that is used by AclCommand when executed with <code>--bootstrap-server</code> option.</p>"},{"location":"tools/kafka-acls/AdminClientService/#creating-instance","title":"Creating Instance","text":"<p><code>AdminClientService</code> takes the following to be created:</p> <ul> <li> AclCommandOptions"},{"location":"tools/kafka-acls/AuthorizerService/","title":"AuthorizerService","text":"<p><code>AuthorizerService</code> is an AclCommandService that is used by AclCommand when executed with no <code>--bootstrap-server</code> option.</p>"},{"location":"tools/kafka-acls/AuthorizerService/#creating-instance","title":"Creating Instance","text":"<p><code>AuthorizerService</code> takes the following to be created:</p> <ul> <li> Class name of the authorizer <li> AclCommandOptions"},{"location":"tools/kafka-cluster/","title":"kafka-cluster","text":"<p><code>kafka-cluster.sh</code> utility can print out the cluster id or unregister a broker.</p>"},{"location":"tools/kafka-cluster/#cluster-id","title":"cluster-id","text":"<p>The <code>cluster-id</code> command prints out the cluster id.</p>"},{"location":"tools/kafka-cluster/#unregister","title":"unregister","text":"<p>The <code>unregister</code> command removes the registration of a specific broker ID.</p>"},{"location":"tools/kafka-configs/","title":"kafka-configs","text":"<p><code>kafka-configs</code> utility uses ConfigCommand to...FIXME</p>"},{"location":"tools/kafka-configs/#alterbrokerconfig","title":"alterBrokerConfig","text":"<pre><code>./bin/kafka-configs.sh \\\n  --bootstrap-server :9092 \\\n  --alter \\\n  --entity-type brokers \\\n  --entity-name 0 \\\n  --add-config advertised.listeners=plaintext://:9092\n</code></pre>"},{"location":"tools/kafka-configs/#processbrokerconfig","title":"processBrokerConfig","text":"<pre><code>./bin/kafka-configs.sh \\\n  --bootstrap-server :9092 \\\n  --describe \\\n  --entity-type brokers \\\n  --entity-name 0\n</code></pre>"},{"location":"tools/kafka-configs/ConfigCommand/","title":"ConfigCommand","text":"<p><code>kafka.admin.ConfigCommand</code> is a command-line utility.</p> <p><code>ConfigCommand</code> can be executed using kafka-configs shell script.</p>"},{"location":"tools/kafka-configs/ConfigCommand/#processcommandwithzk","title":"processCommandWithZk <pre><code>processCommandWithZk(\n  zkConnectString: String,\n  opts: ConfigCommandOptions): Unit\n</code></pre> <p><code>processCommandWithZk</code>...FIXME</p>  <p><code>processCommandWithZk</code> is used when:</p> <ul> <li><code>ConfigCommand</code> is executed with <code>--zookeeper</code> option (deprecated)</li> </ul>","text":""},{"location":"tools/kafka-configs/ConfigCommand/#alterconfigwithzk","title":"alterConfigWithZk <pre><code>alterConfigWithZk(\n  zkClient: KafkaZkClient,\n  opts: ConfigCommandOptions,\n  adminZkClient: AdminZkClient): Unit\n</code></pre> <p><code>alterConfigWithZk</code>...FIXME</p>  <p><code>alterConfigWithZk</code> is used when:</p> <ul> <li><code>ConfigCommand</code> is executed with <code>--alter</code> option (to alter the configuration for an entity)</li> </ul>","text":""},{"location":"tools/kafka-dump-log/","title":"kafka-dump-log Tool","text":"<p><code>kafka-dump-log.sh</code> utility can dump log segments of a topic (incl. cluster metadata) to the console (e.g., for debugging a seemingly corrupt log segment).</p> <p><code>kafka-dump-log.sh</code> has got two new flags: --cluster-metadata-decoder, and --skip-record-metadata.</p> <pre><code>./bin/kafka-dump-log.sh \\\n  --cluster-metadata-decoder \\\n  --skip-record-metadata \\\n  --files /tmp/kraft-controller-logs/__cluster_metadata-0/00000000000000000000.log,/tmp/kraft-controller-logs/__cluster_metadata-0/00000000000000000000.index\n</code></pre>"},{"location":"tools/kafka-dump-log/#cluster-metadata-decoder","title":"cluster-metadata-decoder","text":"<p><code>--cluster-metadata-decoder</code> flag tells the <code>DumpLogSegments</code> tool to decode the records as cluster metadata records.</p>"},{"location":"tools/kafka-dump-log/#skip-record-metadata","title":"skip-record-metadata","text":"<p><code>--skip-record-metadata</code> flag will skip printing metadata for each record.  However, metadata for each record batch will still be printed when this flag is present.</p>"},{"location":"tools/kafka-get-offsets/","title":"kafka-get-offsets","text":"<p><code>kafka-get-offsets</code> utility is an interactive shell for getting topic-partition offsets.</p> <pre><code>$ ./bin/kafka-get-offsets.sh\nAn interactive shell for getting topic-partition offsets.\n</code></pre> <p><code>kafka-get-offsets</code> uses GetOffsetShell for execution.</p> <pre><code>$ ./bin/kafka-topics.sh \\\n    --bootstrap-server :9092 \\\n    --create \\\n    --if-not-exists \\\n    --topic demo-get-offsets \\\n    --partitions 3\n</code></pre> <pre><code>$ ./bin/kafka-get-offsets.sh \\\n    --bootstrap-server :9092\ndemo-get-offsets:0:0\ndemo-get-offsets:1:0\ndemo-get-offsets:2:0\n</code></pre> <pre><code>$ echo zero | kcat -P -b :9092 -t demo-get-offsets -p 0\n</code></pre> <pre><code>$ ./bin/kafka-get-offsets.sh \\\n    --bootstrap-server :9092 \\\n    --topic demo-get-offsets\ndemo-get-offsets:0:1\ndemo-get-offsets:1:0\ndemo-get-offsets:2:0\n</code></pre> <pre><code>$ ./bin/kafka-get-offsets.sh \\\n    --bootstrap-server :9092 \\\n    --topic-partitions demo-get-offsets:0,demo-get-offsets:2\ndemo-get-offsets:0:1\ndemo-get-offsets:2:0\n</code></pre>"},{"location":"tools/kafka-get-offsets/#options","title":"Options","text":""},{"location":"tools/kafka-get-offsets/#exclude-internal-topics","title":"exclude-internal-topics","text":""},{"location":"tools/kafka-get-offsets/#partitions","title":"partitions","text":""},{"location":"tools/kafka-get-offsets/#time","title":"time","text":""},{"location":"tools/kafka-get-offsets/#topic","title":"topic","text":""},{"location":"tools/kafka-get-offsets/#topic-partitions","title":"topic-partitions","text":""},{"location":"tools/kafka-get-offsets/GetOffsetShell/","title":"GetOffsetShell","text":""},{"location":"tools/kafka-get-offsets/GetOffsetShell/#launching-application","title":"Launching Application <pre><code>main(\n  args: Array[String]): Unit\n</code></pre> <p><code>main</code> fetchOffsets.</p>","text":""},{"location":"tools/kafka-get-offsets/GetOffsetShell/#fetchoffsets","title":"fetchOffsets <pre><code>fetchOffsets(\n  args: Array[String]): Unit\n</code></pre> <p><code>fetchOffsets</code>...FIXME</p>","text":""},{"location":"tools/kafka-leader-election/","title":"kafka-leader-election","text":""},{"location":"tools/kafka-metadata-shell/","title":"kafka-metadata-shell Tool","text":"<p><code>kafka-metadata-shell.sh</code> utility is a Kafka metadata tool to interactively examine the metadata stored in a KRaft cluster.</p> <p><code>kafka-metadata-shell.sh</code> can read the metadata from a metadata snapshot on disk.</p> <p>Every now and then, Kafka Controller writes snapshots to cluster metadata topic's log directory (using <code>SnapshotEmitter</code>).</p> <pre><code>INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000061020-0000000002\n</code></pre> <p>Use --snapshot option to load the snapshot file.</p> <pre><code>$ ./bin/kafka-metadata-shell.sh \\\n  --snapshot /tmp/kraft-controller-logs/__cluster_metadata-0/00000000000000061020-0000000002.checkpoint\nLoading...\nStarting...\n[ Kafka Metadata Shell ]\n&gt;&gt;\n</code></pre> <pre><code>&gt;&gt; help\nWelcome to the Apache Kafka metadata shell.\n\nusage:  {cat,cd,exit,find,help,history,ls,man,pwd,tree} ...\n\npositional arguments:\n  {cat,cd,exit,find,help,history,ls,man,pwd,tree}\n    cat                  Show the contents of metadata files.\n    cd                   Set the current working directory.\n    exit                 Exit the metadata shell.\n    find                 Search for nodes in the directory hierarchy.\n    help                 Display this help message.\n    history              Print command history.\n    ls                   List metadata nodes.\n    man                  Show the help text for a specific command.\n    pwd                  Print the current working directory.\n    tree                 Show the contents of metadata nodes in a tree structure.\n</code></pre> <pre><code>&gt;&gt; ls\nimage  local\n</code></pre> <p>The metadata tool works by replaying the log and storing the state into in-memory nodes. These nodes are presented in a fashion similar to filesystem directories.</p> <pre><code>&gt;&gt; man tree\ntree: Show the contents of metadata nodes in a tree structure.\n\nusage: tree targets [targets ...]\n\npositional arguments:\n  targets                The metadata nodes to display.\n</code></pre> <pre><code>&gt;&gt; tree local\ncommitId:\n  60e845626d8a465a\nversion:\n  3.6.0\n</code></pre> <p><code>kafka-metadata-shell.sh</code> uses MetadataShell as an entry point.</p>"},{"location":"tools/kafka-metadata-shell/#arguments","title":"Arguments","text":""},{"location":"tools/kafka-metadata-shell/#snapshot","title":"snapshot","text":"<p><code>--snapshot</code> (<code>-s</code>) is a metadata snapshot file to read.</p>"},{"location":"tools/kafka-metadata-shell/MetadataShell/","title":"MetadataShell","text":"<p><code>MetadataShell</code> is launched as a command-line application using kafka-metadata-shell.sh shell script.</p>"},{"location":"tools/kafka-metadata-shell/MetadataShell/#creating-instance","title":"Creating Instance","text":"<p><code>MetadataShell</code> takes the following to be created:</p> <ul> <li>KafkaRaftManager</li> <li> Snapshot Path <li> <code>FaultHandler</code> <p><code>MetadataShell</code> is created when:</p> <ul> <li><code>MetadataShell.Builder</code> is requested to <code>build</code> (a <code>MetadataShell</code>)</li> </ul>"},{"location":"tools/kafka-metadata-shell/MetadataShell/#raftManager","title":"KafkaRaftManager","text":"<p><code>MetadataShell</code> is given a KafkaRaftManager when created.</p> <p>Note</p> <p>A <code>KafkaRaftManager</code> does not seem to be defined ever (and seems always <code>null</code>).</p>"},{"location":"tools/kafka-metadata-shell/MetadataShell/#main","title":"Entry Point","text":"<pre><code>void main(\n  String[] args)\n</code></pre> <p><code>main</code>...FIXME</p> <p><code>main</code> is used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"tools/kafka-metadata-shell/MetadataShellPublisher/","title":"MetadataShellPublisher","text":"<p><code>MetadataShellPublisher</code> is a MetadataPublisher.</p>"},{"location":"tools/kafka-reassign-partitions/","title":"kafka-reassign-partitions","text":"<p><code>kafka-reassign-partitions</code> utility is used to move topic partitions between replicas (based on a partition reassignment JSON file).</p> <p><code>kafka-reassign-partitions</code> uses ReassignPartitionsCommand for its execution.</p>"},{"location":"tools/kafka-reassign-partitions/#options","title":"Options","text":"<pre><code>$ ./bin/kafka-reassign-partitions.sh --help\nThis tool helps to move topic partitions between replicas.\nOption                                  Description\n------                                  -----------\n--additional                            Execute this reassignment in addition\n                                          to any other ongoing ones. This\n                                          option can also be used to change\n                                          the throttle of an ongoing\n                                          reassignment.\n...\n</code></pre>"},{"location":"tools/kafka-reassign-partitions/#demo","title":"Demo","text":"<p>Demo: kafka-reassign-partitions</p>"},{"location":"tools/kafka-reassign-partitions/ReassignPartitionsCommand/","title":"ReassignPartitionsCommand","text":"<p><code>ReassignPartitionsCommand</code> is a command-line application for kafka-reassign-partitions to generate, execute and verify a custom partition (re)assignment configuration (as specified using a reassignment JSON file).</p>"},{"location":"tools/kafka-reassign-partitions/ReassignPartitionsCommand/#actions","title":"Actions","text":""},{"location":"tools/kafka-reassign-partitions/ReassignPartitionsCommand/#execute","title":"execute <p>Executes the reassignment as specified by the reassignment-json-file option</p>","text":""},{"location":"tools/kafka-reassign-partitions/ReassignPartitionsCommand/#generate","title":"generate <p>Generates a candidate partition reassignment configuration</p>  <p>Note</p> <p>This only generates a candidate assignment and does not execute it.</p>","text":""},{"location":"tools/kafka-reassign-partitions/ReassignPartitionsCommand/#verify","title":"verify <p>Verifies if the reassignment completed as specified by the reassignment-json-file. If there is a throttle engaged for the replicas specified, and the rebalance has completed, the throttle will be removed</p>","text":""},{"location":"tools/kafka-reassign-partitions/ReassignPartitionsCommand/#options","title":"Options","text":""},{"location":"tools/kafka-reassign-partitions/ReassignPartitionsCommand/#reassignment-json-file","title":"reassignment-json-file <p>A JSON file with a custom partition (re)assignment configuration</p> <p>The format to use is as follows:</p> <pre><code>{\n  \"partitions\": [\n    {\n      \"topic\": \"foo\",\n      \"partition\": 1,\n      \"replicas\": [\n        1,\n        2,\n        3\n      ],\n      \"log_dirs\": [\n        \"dir1\",\n        \"dir2\",\n        \"dir3\"\n      ]\n    }\n  ],\n  \"version\": 1\n}\n</code></pre> <p>Note that <code>log_dirs</code> is optional. When specified, its length must equal the length of the replicas list. The value in this list can be either <code>\"any\"</code> or the absolute path of the log directory on the broker.</p> <p>If absolute log directory path is specified, it is currently required that the replica has not already been created on that broker. The replica will then be created in the specified log directory on the broker later.</p>","text":""},{"location":"tools/kafka-reassign-partitions/ReassignPartitionsCommand/#replica-alter-log-dirs-throttle","title":"replica-alter-log-dirs-throttle <p>The movement of replicas between log directories on the same broker will be throttled to this value (bytes/sec).</p> <p>Default: <code>-1</code></p> <p>Rerunning with this option, whilst a rebalance is in progress, will alter the throttle value. The throttle rate should be at least 1 KB/s.</p>","text":""},{"location":"tools/kafka-reassign-partitions/ReassignPartitionsCommand/#throttle","title":"throttle <p>The movement of partitions between brokers will be throttled to this value (bytes/sec).</p> <p>Default: <code>-1</code></p> <p>Rerunning with this option, whilst a rebalance is in progress, will alter the throttle value. The throttle rate should be at least 1 KB/s.</p>","text":""},{"location":"tools/kafka-reassign-partitions/ReassignPartitionsCommand/#timeout","title":"timeout <p>The maximum time (in ms) allowed to wait for partition reassignment execution to be successfully initiated</p> <p>Default: <code>10000</code></p>","text":""},{"location":"tools/kafka-reassign-partitions/ReassignPartitionsCommand/#executing-application","title":"Executing Application","text":""},{"location":"tools/kafka-reassign-partitions/demo/","title":"Demo: kafka-reassign-partitions","text":""},{"location":"tools/kafka-reassign-partitions/demo/#set-up-kafka-cluster","title":"Set Up Kafka Cluster","text":"<p>Start a 3-broker Kafka cluster.</p> <pre><code>./bin/kafka-server-start.sh config/server.properties \\\n  --override broker.id=10 \\\n  --override log.dirs=/tmp/kafka-logs-10 \\\n  --override listeners=PLAINTEXT://:9192\n</code></pre> <pre><code>./bin/kafka-server-start.sh config/server.properties \\\n  --override broker.id=20 \\\n  --override log.dirs=/tmp/kafka-logs-20 \\\n  --override listeners=PLAINTEXT://:9292\n</code></pre> <pre><code>./bin/kafka-server-start.sh config/server.properties \\\n  --override broker.id=30 \\\n  --override log.dirs=/tmp/kafka-logs-30 \\\n  --override listeners=PLAINTEXT://:9392\n</code></pre>"},{"location":"tools/kafka-reassign-partitions/demo/#create-topic","title":"Create Topic","text":"<pre><code>./bin/kafka-topics.sh \\\n  --bootstrap-server :9192 \\\n  --create \\\n  --topic demo-reassign-partitions \\\n  --replication-factor 2 \\\n  --partitions 1\n</code></pre> <pre><code>./bin/kafka-topics.sh \\\n  --bootstrap-server :9192 \\\n  --describe \\\n  --topic demo-reassign-partitions\n</code></pre> <pre><code>Topic: demo-reassign-partitions TopicId: J3ginqnTTg-LjVoI20dqqw PartitionCount: 1 ReplicationFactor: 2 Configs: segment.bytes=1073741824\n Topic: demo-reassign-partitions Partition: 0 Leader: 10 Replicas: 10,20 Isr: 10,20\n</code></pre>"},{"location":"tools/kafka-reassign-partitions/demo/#reassign-partitionsjson","title":"reassign-partitions.json","text":"reassign-partitions.json<pre><code>{\n  \"partitions\": [\n    {\n      \"topic\": \"demo-reassign-partitions\",\n      \"partition\": 1,\n      \"replicas\": [\n        20,\n        30\n      ]\n    }\n  ],\n  \"version\": 1\n}\n</code></pre>"},{"location":"tools/kafka-reassign-partitions/demo/#generate-reassignment-configuration","title":"Generate Reassignment Configuration","text":"<p>Does not seem to work</p> <pre><code>$ ./bin/kafka-reassign-partitions.sh \\\n  --bootstrap-server :9192 \\\n  --generate \\\n  --topics-to-move-json-file reassign-partitions.json \\\n  --broker-list 10,20,30\n</code></pre>"},{"location":"tools/kafka-reassign-partitions/demo/#verify-reassignment-configuration","title":"Verify Reassignment Configuration","text":"<pre><code>$ ./bin/kafka-reassign-partitions.sh \\\n  --bootstrap-server :9192 \\\n  --verify \\\n  --reassignment-json-file reassign-partitions.json\n</code></pre>"},{"location":"tools/kafka-replica-verification/","title":"kafka-replica-verification","text":"<p><code>kafka-replica-verification</code> utility is used to verify replica consistency (i.e., validate that all replicas for a set of topics have the same data).</p> <p><code>kafka-replica-verification</code> uses ReplicaVerificationTool with ReplicaFetchers for its execution.</p>"},{"location":"tools/kafka-replica-verification/#options","title":"Options","text":"<pre><code>$ ./bin/kafka-replica-verification.sh --help\nValidate that all replicas for a set of topics have the same data.\nOption                                  Description\n------                                  -----------\n--broker-list &lt;String: hostname:        REQUIRED: The list of hostname and\n  port,...,hostname:port&gt;                 port of the server to connect to.\n--fetch-size &lt;Integer: bytes&gt;           The fetch size of each request.\n                                          (default: 1048576)\n--help                                  Print usage information.\n--max-wait-ms &lt;Integer: ms&gt;             The max amount of time each fetch\n                                          request waits. (default: 1000)\n--report-interval-ms &lt;Long: ms&gt;         The reporting interval. (default:\n                                          30000)\n--time &lt;Long: timestamp/-1(latest)/-2   Timestamp for getting the initial\n  (earliest)&gt;                             offsets. (default: -1)\n--topic-white-list &lt;String: Java regex  DEPRECATED use --topics-include\n  (String)&gt;                               instead; ignored if --topics-include\n                                          specified. List of topics to verify\n                                          replica consistency. Defaults to '.\n                                          *' (all topics) (default: .*)\n--topics-include &lt;String: Java regex    List of topics to verify replica\n  (String)&gt;                               consistency. Defaults to '.*' (all\n                                          topics) (default: .*)\n--version                               Print version information and exit.\n</code></pre>"},{"location":"tools/kafka-replica-verification/#demo","title":"Demo","text":"<pre><code>$ ./bin/kafka-replica-verification.sh \\\n    --broker-list :9092 \\\n    --report-interval-ms 5000\nverification process is started.\nmax lag is 0 for partition t100-0 at offset 0 among 1 partitions\n</code></pre>"},{"location":"tools/kafka-replica-verification/ReplicaBuffer/","title":"ReplicaBuffer","text":""},{"location":"tools/kafka-replica-verification/ReplicaBuffer/#creating-instance","title":"Creating Instance","text":"<p><code>ReplicaBuffer</code> takes the following to be created:</p> <ul> <li> Expected Replicas per TopicPartition (<code>Map[TopicPartition, Int]</code>) <li> Initial offsets (<code>Map[TopicPartition, Long]</code>) <li> Expected number of fetchers <li>Report interval</li> <p><code>ReplicaBuffer</code> is created when:</p> <ul> <li><code>ReplicaVerificationTool</code> is executed</li> </ul>"},{"location":"tools/kafka-replica-verification/ReplicaBuffer/#report-interval","title":"Report Interval <p><code>ReplicaBuffer</code> is given a report interval when created.</p> <p>The value is <code>--report-interval-ms</code> and defaults to <code>30s</code>.</p> <p>The interval is used when verifyCheckSum to print out the following to the standard output:</p> <pre><code>[currentTimeMs]: max lag is [lag] for partition [partition] at offset [offset] among [n] partitions\n</code></pre>","text":""},{"location":"tools/kafka-replica-verification/ReplicaFetcher/","title":"ReplicaFetcher","text":"<p><code>ReplicaFetcher</code> is a thread that ReplicaVerificationTool uses for replica verification.</p>"},{"location":"tools/kafka-replica-verification/ReplicaFetcher/#creating-instance","title":"Creating Instance","text":"<p><code>ReplicaFetcher</code> takes the following to be created:</p> <ul> <li> <code>ReplicaFetcher-[brokerId]</code> <li> Source broker <li> <code>TopicPartition</code>s <li> Topics IDs (<code>Map[String, Uuid]</code>) <li> ReplicaBuffer <li> Socket timeout (<code>30000</code>) <li> Socket buffer size (<code>256000</code>) <li> Fetch size <li> Max wait <li> Min bytes <li>doVerification flag</li> <li> Consumer properties <li> Fetcher ID <p><code>ReplicaFetcher</code> is created when:</p> <ul> <li><code>ReplicaVerificationTool</code> command-line utility is executed</li> </ul>"},{"location":"tools/kafka-replica-verification/ReplicaFetcher/#doverification","title":"doVerification <p><code>ReplicaFetcher</code> is given <code>doVerification</code> flag when created.</p> <p><code>doVerification</code> flag is enabled for a single <code>ReplicaFetcher</code> among the replica fetcher threads.</p> <p>The flag is used to determine which <code>ReplicaFetcher</code> should perform verification.</p>","text":""},{"location":"tools/kafka-replica-verification/ReplicaFetcher/#dowork","title":"doWork <pre><code>doWork(): Unit\n</code></pre> <p><code>doWork</code> is part of the <code>ShutdownableThread</code> abstraction.</p>  <p><code>doWork</code> creates a <code>requestMap</code> with <code>TopicPartition</code>s and <code>PartitionData</code>s.</p> <p><code>doWork</code> prints out the following DEBUG message to the logs:</p> <pre><code>Issuing fetch request\n</code></pre> <p><code>doWork</code> sends a <code>Fetch</code> request (with the <code>requestMap</code>).</p> <p>With a <code>FetchResponse</code>, <code>doWork</code> addFetchedData (to the ReplicaBuffer that all <code>ReplicaFetcher</code>s append fetched data to). Otherwise, <code>doWork</code>...FIXME</p> <p><code>doWork</code> decrements the fetcherBarrier latch. If it reaches 0, <code>doWork</code> prints out the following DEBUG message to the logs:</p> <pre><code>Done fetching\n</code></pre> <p><code>doWork</code> waits for the other fetchers to finish and prints out the following DEBUG message to the logs:</p> <pre><code>Ready for verification\n</code></pre> <p>With the doVerification flag enabled, <code>doWork</code> performs verification.</p> <p><code>doWork</code> waits for the verification to be finished and prints out the following DEBUG message to the logs:</p> <pre><code>Done verification\n</code></pre>","text":""},{"location":"tools/kafka-replica-verification/ReplicaFetcher/#verification","title":"Verification <p><code>doWork</code>...FIXME</p>","text":""},{"location":"tools/kafka-replica-verification/ReplicaVerificationTool/","title":"ReplicaVerificationTool","text":"<p><code>ReplicaVerificationTool</code> is a command-line application for kafka-replica-verification to perform replica verification.</p>"},{"location":"tools/kafka-replica-verification/ReplicaVerificationTool/#executing-application","title":"Executing Application <p><code>main</code> prints out the following INFO message to the logs:</p> <pre><code>Getting topic metadata...\n</code></pre> <p><code>main</code> createAdminClient (with the brokers in <code>--broker-list</code> option) for topics metadata and broker info.</p> <p><code>main</code> creates <code>TopicPartitionReplica</code>s for the topics.</p> <p><code>main</code> prints out the following DEBUG message to the logs:</p> <pre><code>Selected topic partitions: [topicPartitionReplicas]\n</code></pre> <p><code>main</code> groups partitions per broker and prints out the following DEBUG message to the logs:</p> <pre><code>Topic partitions per broker: [brokerToTopicPartitions]\n</code></pre> <p><code>main</code> groups partitions per replica to count the number of replicas and prints out the following DEBUG message to the logs:</p> <pre><code>Expected replicas per topic partition: [expectedReplicasPerTopicPartition]\n</code></pre> <p><code>main</code> creates a consumer config.</p> <p><code>main</code> creates ReplicaFetchers for every replica broker and starts them all.</p> <p><code>ReplicaFetcher</code>s run until termination (Ctrl-C). <code>main</code> prints out the following INFO message to the logs:</p> <pre><code>Stopping all fetchers\n</code></pre>","text":""},{"location":"tools/kafka-storage/","title":"kafka-storage Utility","text":"<p><code>kafka-storage</code> script is used to...FIXME...when setting up a Kafka cluster in KRaft mode.</p> <pre><code>$ ./bin/kafka-storage.sh -h\nusage: kafka-storage [-h] {info,format,random-uuid} ...\n\nThe Kafka storage tool.\n\npositional arguments:\n  {info,format,random-uuid}\n    info                 Get information about the Kafka log directories on this node.\n    format               Format the Kafka log directories on this node.\n    random-uuid          Print a random UUID.\n\noptional arguments:\n  -h, --help             show this help message and exit\n</code></pre> <p><code>kafka-storage</code> runs kafka.tools.StorageTool.</p>"},{"location":"tools/kafka-storage/#format","title":"format","text":"<p>format command is used to format the Kafka storage directories of a node (brokers and controllers).</p> <pre><code>$ ./bin/kafka-storage.sh format -h\nusage: kafka-storage format [-h] --config CONFIG --cluster-id CLUSTER_ID [--add-scram ADD_SCRAM] [--ignore-formatted] [--release-version RELEASE_VERSION]\n\noptional arguments:\n  -h, --help             show this help message and exit\n  --config CONFIG, -c CONFIG\n                         The Kafka configuration file to use.\n  --cluster-id CLUSTER_ID, -t CLUSTER_ID\n                         The cluster ID to use.\n  --add-scram ADD_SCRAM, -S ADD_SCRAM\n                         A SCRAM_CREDENTIAL to add to the __cluster_metadata log e.g.\n                         'SCRAM-SHA-256=[name=alice,password=alice-secret]'\n                         'SCRAM-SHA-512=[name=alice,iterations=8192,salt=\"N3E=\",saltedpassword=\"YCE=\"]'\n  --ignore-formatted, -g\n  --release-version RELEASE_VERSION, -r RELEASE_VERSION\n                         A KRaft release version to use for the initial metadata version. The minimum is 3.0, the default is 3.6-IV2\n</code></pre> <p><code>format</code> is the second command to be executed while setting up a Kafka cluster.</p> <pre><code>$ ./bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties\nFormatting /tmp/kraft-combined-logs with metadata.version 3.6-IV2.\n</code></pre>"},{"location":"tools/kafka-storage/#info","title":"info","text":"<p>The <code>info</code> command gives information about the configured storage directories.</p> <pre><code>$ ./bin/kafka-storage.sh info -h\nusage: kafka-storage info [-h] --config CONFIG\n\noptional arguments:\n  -h, --help             show this help message and exit\n  --config CONFIG, -c CONFIG\n                         The Kafka configuration file to use.\n</code></pre> <pre><code>$ ./bin/kafka-storage.sh info -c config/kraft/server.properties\nFound log directory:\n  /tmp/kraft-combined-logs\n\nFound metadata: {cluster.id=F9_futKUQPKBwpQddvXsDQ, node.id=1, version=1}\n\n$ tree /tmp/kraft-combined-logs\n/tmp/kraft-combined-logs\n\u251c\u2500\u2500 bootstrap.checkpoint\n\u2514\u2500\u2500 meta.properties\n</code></pre>"},{"location":"tools/kafka-storage/#random-uuid","title":"random-uuid","text":"<p>random-uuid command prints out a pseudo randomly-generated UUID of a cluster to stdout.</p> <pre><code>$ ./bin/kafka-storage.sh random-uuid\npnHyFfvWT6i2F2wZzSUx6A\n</code></pre> <p><code>random-uuid</code> is the first command to be executed while setting up a Kafka cluster.</p> <pre><code>KAFKA_CLUSTER_ID=\"$(bin/kafka-storage.sh random-uuid)\"\n</code></pre>"},{"location":"tools/kafka-storage/StorageTool/","title":"StorageTool","text":"<p><code>StorageTool</code> (<code>kafka.tools.StorageTool</code>) is an utility to...FIXME</p> <p><code>StorageTool</code> can be executed using bin/kafka-storage.sh script.</p>"},{"location":"tools/kafka-storage/StorageTool/#random-uuid","title":"random-uuid","text":""},{"location":"tools/kafka-storage/StorageTool/#format","title":"format","text":"<p><code>format</code> command configToLogDirectories.</p>"},{"location":"tools/kafka-storage/StorageTool/#configToLogDirectories","title":"configToLogDirectories","text":"<pre><code>configToLogDirectories(\n  config: KafkaConfig): Seq[String]\n</code></pre> <p><code>configToLogDirectories</code> takes the value of the following configuration properties:</p> <ul> <li>log.dirs or log.dir (depending on availability)</li> <li>metadataLogDir</li> </ul>"},{"location":"transactions/","title":"Transactions","text":"<p>Apache Kafka supports transactional record delivery (and consumption if in consumer-process-produce processing mode).</p> <p>Every Kafka broker runs a TransactionCoordinator to manage (coordinate) transactions.</p>"},{"location":"transactions/#transactional-producer","title":"Transactional Producer","text":"<p>A KafkaProducer is transactional when transactional.id configuration property is specified.</p> <p>Any record sending has to be after KafkaProducer.initTransactions followed by KafkaProducer.beginTransaction. Otherwise, the underlying TransactionManager is going to be in a wrong state (that will inevitably lead to exceptions).</p>"},{"location":"transactions/#transaction-aware-consumer","title":"Transaction-Aware Consumer","text":"<p>A KafkaConsumer supports transactions using isolation.level configuration property.</p>"},{"location":"transactions/#kafka-console-consumer","title":"kafka-console-consumer","text":"<p><code>kafka-console-consumer</code> supports <code>--isolation-level</code> option for isolation.level configuration property.</p>"},{"location":"transactions/#demo","title":"Demo","text":"<p>Demo: Transactional Kafka Producer</p>"},{"location":"transactions/#transactional-configuration-properties","title":"Transactional Configuration Properties <p>TransactionConfig</p>","text":""},{"location":"transactions/#transaction-topic","title":"Transaction Topic <p>Kafka brokers use <code>__transaction_state</code> internal topic for managing transactions (as records).</p> <p><code>__transaction_state</code> is auto-created at the first transaction.</p> <p>The number of partitions is configured using transaction.state.log.num.partitions configuration property.</p> <p>A transaction (record) is assigned a partition (txn topic partition) based on the absolute hash code of the transactional.id.</p>","text":""},{"location":"transactions/#learning-resources","title":"Learning Resources <ul> <li>Transactions in Apache Kafka by Confluent</li> </ul>","text":""},{"location":"transactions/TransactionConfig/","title":"TransactionConfig","text":"<p><code>TransactionConfig</code> holds the values of the transactional configuration properties.</p>"},{"location":"transactions/TransactionConfig/#transactionalidexpirationms","title":"transactional.id.expiration.ms <p>transactional.id.expiration.ms</p> <p>Default: 7 days</p>","text":""},{"location":"transactions/TransactionConfig/#transactionmaxtimeoutms","title":"transaction.max.timeout.ms <p>transaction.max.timeout.ms</p> <p>Default: 15 minutes</p>","text":""},{"location":"transactions/TransactionConfig/#transactionstatelognumpartitions","title":"transaction.state.log.num.partitions <p>transaction.state.log.num.partitions</p> <p>Default: 50</p>","text":""},{"location":"transactions/TransactionConfig/#transactionstatelogreplicationfactor","title":"transaction.state.log.replication.factor <p>transaction.state.log.replication.factor</p> <p>Default: 3</p>","text":""},{"location":"transactions/TransactionConfig/#transactionstatelogsegmentbytes","title":"transaction.state.log.segment.bytes <p>transaction.state.log.segment.bytes</p> <p>Default: 100 * 1024 * 1024</p>","text":""},{"location":"transactions/TransactionConfig/#transactionstatelogloadbuffersize","title":"transaction.state.log.load.buffer.size <p>transaction.state.log.load.buffer.size</p> <p>Default: 5 * 1024 * 1024</p>","text":""},{"location":"transactions/TransactionConfig/#transactionstatelogminisr","title":"transaction.state.log.min.isr <p>transaction.state.log.min.isr</p> <p>Default: 2</p>","text":""},{"location":"transactions/TransactionConfig/#transactionaborttimedouttransactioncleanupintervalms","title":"transaction.abort.timed.out.transaction.cleanup.interval.ms <p>transaction.abort.timed.out.transaction.cleanup.interval.ms</p> <p>Default: 10 seconds</p>","text":""},{"location":"transactions/TransactionConfig/#transactionremoveexpiredtransactioncleanupintervalms","title":"transaction.remove.expired.transaction.cleanup.interval.ms <p>transaction.remove.expired.transaction.cleanup.interval.ms</p> <p>Default: 1 hour</p>","text":""},{"location":"transactions/TransactionConfig/#requesttimeoutms","title":"request.timeout.ms <p>request.timeout.ms</p> <p>Default: 30000</p>","text":""},{"location":"transactions/TransactionCoordinator/","title":"TransactionCoordinator","text":"<p><code>TransactionCoordinator</code> runs on every Kafka broker (BrokerServer or KafkaServer).</p>"},{"location":"transactions/TransactionCoordinator/#creating-instance","title":"Creating Instance","text":"<p><code>TransactionCoordinator</code> takes the following to be created:</p> <ul> <li> Broker Id <li> TransactionConfig <li> <code>Scheduler</code> <li> <code>createProducerIdGenerator</code> function (<code>() =&gt; ProducerIdGenerator</code>) <li> TransactionStateManager <li> <code>TransactionMarkerChannelManager</code> <li> <code>Time</code> <li> <code>LogContext</code> <p><code>TransactionCoordinator</code> is created using apply factory.</p>"},{"location":"transactions/TransactionCoordinator/#creating-transactioncoordinator","title":"Creating TransactionCoordinator <pre><code>apply(\n  config: KafkaConfig,\n  replicaManager: ReplicaManager,\n  scheduler: Scheduler,\n  createProducerIdGenerator: () =&gt; ProducerIdGenerator,\n  metrics: Metrics,\n  metadataCache: MetadataCache,\n  time: Time): TransactionCoordinator\n</code></pre> <p><code>apply</code> creates a TransactionConfig.</p> <p><code>apply</code> creates a TransactionStateManager (with the brokerId and the other Kafka services).</p> <p><code>apply</code> creates a <code>LogContext</code> that uses the following log prefix (with the brokerId):</p> <pre><code>[TransactionCoordinator id=[brokerId]]\n</code></pre> <p><code>apply</code> creates a <code>TransactionMarkerChannelManager</code>.</p> <p>In the end, <code>apply</code> creates a TransactionCoordinator.</p> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>BrokerServer</code> is requested to start up</li> <li><code>KafkaServer</code> is requested to start up</li> </ul>","text":""},{"location":"transactions/TransactionCoordinator/#starting-up","title":"Starting Up <pre><code>startup(\n  retrieveTransactionTopicPartitionCount: () =&gt; Int,\n  enableTransactionalIdExpiration: Boolean = true): Unit\n</code></pre> <p><code>startup</code>...FIXME</p> <p><code>startup</code>\u00a0is used when:</p> <ul> <li><code>BrokerServer</code> is requested to start up</li> <li><code>KafkaServer</code> is requested to start up</li> </ul>","text":""},{"location":"transactions/TransactionCoordinator/#onelection","title":"onElection <pre><code>onElection(\n  txnTopicPartitionId: Int,\n  coordinatorEpoch: Int): Unit\n</code></pre> <p><code>onElection</code> prints out the following INFO message to the logs:</p> <pre><code>Elected as the txn coordinator for partition [txnTopicPartitionId] at epoch [coordinatorEpoch]\n</code></pre> <p><code>onElection</code> requests the TransactionMarkerChannelManager to removeMarkersForTxnTopicPartition for the given <code>txnTopicPartitionId</code> partition.</p> <p>In the end, <code>onElection</code> requests the TransactionStateManager to loadTransactionsForTxnTopicPartition.</p> <p><code>onElection</code>\u00a0is used when:</p> <ul> <li><code>RequestHandlerHelper</code> is requested to onLeadershipChange</li> </ul>","text":""},{"location":"transactions/TransactionCoordinator/#onresignation","title":"onResignation <pre><code>onResignation(\n  txnTopicPartitionId: Int,\n  coordinatorEpoch: Option[Int]): Unit\n</code></pre> <p><code>onResignation</code>...FIXME</p> <p><code>onResignation</code>\u00a0is used when:</p> <ul> <li><code>KafkaApis</code> is requested to handleStopReplicaRequest</li> <li><code>RequestHandlerHelper</code> is requested to onLeadershipChange</li> </ul>","text":""},{"location":"transactions/TransactionCoordinator/#handleinitproducerid","title":"handleInitProducerId <pre><code>handleInitProducerId(\n  transactionalId: String,\n  transactionTimeoutMs: Int,\n  expectedProducerIdAndEpoch: Option[ProducerIdAndEpoch],\n  responseCallback: InitProducerIdCallback): Unit\n</code></pre> <p>For <code>transactionalId</code> undefined (<code>null</code>), <code>handleInitProducerId</code> requests the ProducerIdGenerator to <code>generateProducerId</code> and sends it back (using the given <code>InitProducerIdCallback</code>).</p> <p><code>handleInitProducerId</code> requests the TransactionStateManager to getTransactionState for the given <code>transactionalId</code>.</p> <p><code>handleInitProducerId</code> prints out the following INFO message to the logs:</p> <pre><code>Initialized transactionalId [transactionalId] with producerId [producerId] and producer epoch [producerEpoch]\non partition __transaction_state-[partition]\n</code></pre> <p>In the end, <code>handleInitProducerId</code> requests the TransactionStateManager to appendTransactionToLog.</p> <p><code>handleInitProducerId</code>\u00a0is used when:</p> <ul> <li><code>KafkaApis</code> is requested to handleInitProducerIdRequest</li> </ul>","text":""},{"location":"transactions/TransactionCoordinator/#transactiontopicconfigs","title":"transactionTopicConfigs <pre><code>transactionTopicConfigs: Properties\n</code></pre> <p><code>transactionTopicConfigs</code> requests the TransactionStateManager for the transactionTopicConfigs</p>  <p><code>transactionTopicConfigs</code> is used when:</p> <ul> <li><code>DefaultAutoTopicCreationManager</code> is requested to creatableTopic (for <code>__transaction_state</code> topic)</li> </ul>","text":""},{"location":"transactions/TransactionCoordinator/#onendtransactioncomplete","title":"onEndTransactionComplete <pre><code>onEndTransactionComplete(\n  txnIdAndPidEpoch: TransactionalIdAndProducerIdEpoch)(\n  error: Errors): Unit\n</code></pre> <p><code>onEndTransactionComplete</code> branches off per the <code>error</code> to print out a message to the logs.</p>  <p><code>onEndTransactionComplete</code> is used when:</p> <ul> <li><code>TransactionCoordinator</code> is requested to start up (and schedules the <code>transaction-abort</code> task)</li> </ul>","text":""},{"location":"transactions/TransactionCoordinator/#no-errors","title":"No Errors <p>For no errors, <code>onEndTransactionComplete</code> prints out the following INFO message to the logs:</p> <pre><code>Completed rollback of ongoing transaction for transactionalId [transactionalId] due to timeout\n</code></pre>","text":""},{"location":"transactions/TransactionCoordinator/#rollback-cancelled","title":"Rollback Cancelled <p>For <code>INVALID_PRODUCER_ID_MAPPING</code>, <code>PRODUCER_FENCED</code>, <code>CONCURRENT_TRANSACTIONS</code>, <code>onEndTransactionComplete</code> prints out the following DEBUG message to the logs:</p> <pre><code>Rollback of ongoing transaction for transactionalId [transactionalId] has been cancelled due to error [error]\n</code></pre>","text":""},{"location":"transactions/TransactionCoordinator/#rollback-failed","title":"Rollback Failed <p>For all other errors, <code>onEndTransactionComplete</code> prints out the following WARN message to the logs:</p> <pre><code>Rollback of ongoing transaction for transactionalId [transactionalId] failed due to error [error]\n</code></pre>","text":""},{"location":"transactions/TransactionCoordinator/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>kafka.coordinator.transaction.TransactionCoordinator</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.coordinator.transaction.TransactionCoordinator=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"transactions/TransactionMarkerChannelManager/","title":"TransactionMarkerChannelManager","text":"<p><code>TransactionMarkerChannelManager</code> is...FIXME</p>"},{"location":"transactions/TransactionStateManager/","title":"TransactionStateManager","text":""},{"location":"transactions/TransactionStateManager/#creating-instance","title":"Creating Instance","text":"<p><code>TransactionStateManager</code> takes the following to be created:</p> <ul> <li> Broker ID <li> <code>Scheduler</code> <li> ReplicaManager <li> TransactionConfig <li> <code>Time</code> <li> <code>Metrics</code> <p><code>TransactionStateManager</code> is created\u00a0when:</p> <ul> <li><code>TransactionCoordinator</code> utility is used to create a TransactionCoordinator</li> </ul>"},{"location":"transactions/TransactionStateManager/#starting-up","title":"Starting Up <pre><code>startup(\n  retrieveTransactionTopicPartitionCount: () =&gt; Int,\n  enableTransactionalIdExpiration: Boolean = true): Unit\n</code></pre> <p><code>startup</code>...FIXME</p> <p><code>startup</code>\u00a0is used when:</p> <ul> <li><code>TransactionCoordinator</code> is requested to start up</li> </ul>","text":""},{"location":"transactions/TransactionStateManager/#enabletransactionalidexpiration","title":"enableTransactionalIdExpiration <pre><code>enableTransactionalIdExpiration(): Unit\n</code></pre> <p><code>enableTransactionalIdExpiration</code>...FIXME</p>","text":""},{"location":"transactions/TransactionStateManager/#appendtransactiontolog","title":"appendTransactionToLog <pre><code>appendTransactionToLog(\n  transactionalId: String,\n  coordinatorEpoch: Int,\n  newMetadata: TxnTransitMetadata,\n  responseCallback: Errors =&gt; Unit,\n  retryOnError: Errors =&gt; Boolean = _ =&gt; false): Unit\n</code></pre> <p><code>appendTransactionToLog</code> generates the key and the value (of the record to represent the transaction in the topic) based on the given <code>transactionalId</code> and the <code>TxnTransitMetadata</code>, respectively.</p> <p><code>appendTransactionToLog</code>...FIXME</p> <p><code>appendTransactionToLog</code> requests the ReplicaManager to appendRecords (with <code>-1</code> acks, <code>internalTopicsAllowed</code> enabled annd <code>Coordinator</code> origin) and prints out the following TRACE message to the logs:</p> <pre><code>Appending new metadata [newMetadata] for transaction id [transactionalId] with coordinator epoch [coordinatorEpoch] to the local transaction log\n</code></pre> <p><code>appendTransactionToLog</code>\u00a0is used when:</p> <ul> <li><code>TransactionCoordinator</code> is requested to handleInitProducerId, handleAddPartitionsToTransaction, endTransaction</li> <li><code>TransactionMarkerChannelManager</code> is requested to <code>tryAppendToLog</code></li> </ul>","text":""},{"location":"transactions/TransactionStateManager/#partitionfor","title":"partitionFor <pre><code>partitionFor(\n  transactionalId: String): Int\n</code></pre> <p><code>partitionFor</code> calculates the partition for the given <code>transactionalId</code>.</p> <p><code>partitionFor</code> gets the absolute value of the <code>hashCode</code> of the <code>transactionalId</code> string modulo the number of partitions of the <code>__transaction_state</code> topic.</p> <p><code>partitionFor</code>\u00a0is used when:</p> <ul> <li><code>TransactionStateManager</code> is requested to appendTransactionToLog, enableTransactionalIdExpiration, getAndMaybeAddTransactionState</li> <li><code>TransactionCoordinator</code> is requested to handleInitProducerId</li> <li><code>TransactionMarkerChannelManager</code> is requested to <code>addTxnMarkersToBrokerQueue</code></li> </ul>","text":""},{"location":"transactions/TransactionStateManager/#loadtransactionsfortxntopicpartition","title":"loadTransactionsForTxnTopicPartition <pre><code>loadTransactionsForTxnTopicPartition(\n  partitionId: Int,\n  coordinatorEpoch: Int,\n  sendTxnMarkers: SendTxnMarkersCallback): Unit\n</code></pre> <p><code>loadTransactionsForTxnTopicPartition</code>...FIXME</p> <p><code>loadTransactionsForTxnTopicPartition</code>\u00a0is used when:</p> <ul> <li><code>TransactionCoordinator</code> is requested to onElection</li> </ul>","text":""},{"location":"transactions/TransactionStateManager/#removetransactionsfortxntopicpartition","title":"removeTransactionsForTxnTopicPartition <pre><code>removeTransactionsForTxnTopicPartition(\n  partitionId: Int): Unit\nremoveTransactionsForTxnTopicPartition(\n  partitionId: Int,\n  coordinatorEpoch: Int): Unit\n</code></pre> <p><code>removeTransactionsForTxnTopicPartition</code>...FIXME</p> <p><code>removeTransactionsForTxnTopicPartition</code>\u00a0is used when:</p> <ul> <li><code>TransactionCoordinator</code> is requested to onResignation</li> </ul>","text":""},{"location":"transactions/TransactionStateManager/#transactiontopicconfigs","title":"transactionTopicConfigs <pre><code>transactionTopicConfigs: Properties\n</code></pre>    Property Name Property Value     cleanup.policy compact   compression.type <code>uncompressed</code>   min.insync.replicas transaction.state.log.min.isr   segment.bytes transaction.state.log.segment.bytes   unclean.leader.election.enable <code>false</code>     <p><code>transactionTopicConfigs</code> is used when:</p> <ul> <li><code>TransactionCoordinator</code> is requested to transactionTopicConfigs</li> </ul>","text":""},{"location":"zk/AdminZkClient/","title":"AdminZkClient","text":""},{"location":"zk/AdminZkClient/#creating-instance","title":"Creating Instance","text":"<p><code>AdminZkClient</code> takes the following to be created:</p> <ul> <li> KafkaZkClient <p><code>AdminZkClient</code> is created when:</p> <ul> <li><code>ConfigCommand</code> is requested to <code>processCommandWithZk</code></li> <li><code>DynamicBrokerConfig</code> is requested to initialize</li> <li><code>KafkaServer</code> is requested to start up</li> <li><code>ZkAdminManager</code> is created (<code>adminZkClient</code>)</li> <li><code>ZkConfigManager</code> is created (<code>adminZkClient</code>)</li> <li><code>ZkConfigRepository</code> is created (<code>adminZkClient</code>)</li> </ul>"},{"location":"zk/AdminZkClient/#fetchentityconfig","title":"fetchEntityConfig <pre><code>fetchEntityConfig(\n  rootEntityType: String,\n  sanitizedEntityName: String): Properties\n</code></pre> <p><code>fetchEntityConfig</code> requests the KafkaZkClient to getEntityConfigs.</p>  <p><code>fetchEntityConfig</code> is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"zk/AdminZkClient/#changeentityconfig","title":"changeEntityConfig <pre><code>changeEntityConfig(\n  rootEntityType: String,\n  fullSanitizedEntityName: String,\n  configs: Properties): Unit\n</code></pre> <p><code>changeEntityConfig</code>...FIXME</p>  <p><code>changeEntityConfig</code> is used when:</p> <ul> <li><code>AdminZkClient</code> is requested to changeClientIdConfig, changeUserOrUserClientIdConfig, changeIpConfig, changeTopicConfig, changeBrokerConfig</li> </ul>","text":""},{"location":"zk/AdminZkClient/#changetopicconfig","title":"changeTopicConfig <pre><code>changeTopicConfig(\n  topic: String,\n  configs: Properties): Unit\n</code></pre> <p><code>changeTopicConfig</code>...FIXME</p>  <p><code>changeTopicConfig</code> is used when:</p> <ul> <li><code>ZkAdminManager</code> is requested to alterTopicConfigs</li> <li><code>AdminZkClient</code> is requested to changeConfigs</li> </ul>","text":""},{"location":"zk/AdminZkClient/#changeconfigs","title":"changeConfigs <pre><code>changeConfigs(\n  entityType: String,\n  entityName: String,\n  configs: Properties): Unit\n</code></pre> <p><code>changeConfigs</code>...FIXME</p>  <p><code>changeConfigs</code> is used when:</p> <ul> <li><code>ConfigCommand</code> is requested to alterConfigWithZk</li> <li><code>ZkAdminManager</code> is requested to alterClientQuotas, alterUserScramCredentials</li> </ul>","text":""},{"location":"zk/KafkaZkClient/","title":"KafkaZkClient","text":""},{"location":"zk/KafkaZkClient/#getentityconfigs","title":"getEntityConfigs <pre><code>getEntityConfigs(\n  rootEntityType: String,\n  sanitizedEntityName: String): Properties\n</code></pre> <p><code>getEntityConfigs</code> talks to Zookeeper for <code>config</code> data in the <code>/config/[entityType]/[entityName]</code> znode.</p> <pre><code>$ ./bin/zkCli.sh -server localhost:2181 ls /config\n[brokers, changes, clients, ips, topics, users]\n</code></pre>  <p><code>getEntityConfigs</code> is used when:</p> <ul> <li><code>AdminZkClient</code> is requested to fetchEntityConfig</li> </ul>","text":""},{"location":"zk/KafkaZkClient/#createconfigchangenotification","title":"createConfigChangeNotification <pre><code>createConfigChangeNotification(\n  sanitizedEntityPath: String): Unit\n</code></pre> <p><code>createConfigChangeNotification</code>...FIXME</p>  <p><code>createConfigChangeNotification</code> is used when:</p> <ul> <li><code>AdminZkClient</code> is requested to changeEntityConfig</li> </ul>","text":""},{"location":"zk/TopicPartitionStateZNode/","title":"TopicPartitionStateZNode","text":""}]}