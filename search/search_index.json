{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Apache Kafka 3.0.0 \u00b6 Welcome to The Internals of Apache Kafka online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Kafka as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Apache Kafka \ud83d\udd25 Last update: 2021-09-27","title":"Home"},{"location":"#the-internals-of-apache-kafka-300","text":"Welcome to The Internals of Apache Kafka online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Kafka as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Apache Kafka \ud83d\udd25 Last update: 2021-09-27","title":"The Internals of Apache Kafka 3.0.0"},{"location":"AbstractConfig/","text":"AbstractConfig \u00b6 AbstractConfig is...FIXME","title":"AbstractConfig"},{"location":"AbstractConfig/#abstractconfig","text":"AbstractConfig is...FIXME","title":"AbstractConfig"},{"location":"AutoTopicCreationManager/","text":"AutoTopicCreationManager \u00b6 AutoTopicCreationManager is an abstraction of managers that can create topics . Contract \u00b6 createTopics \u00b6 createTopics ( topicNames : Set [ String ], controllerMutationQuota : ControllerMutationQuota ): Seq [ MetadataResponseTopic ] Used when: KafkaApis is requested to getTopicMetadata and handleFindCoordinatorRequest shutdown \u00b6 shutdown (): Unit Used when: BrokerServer is requested to shutdown KafkaServer is requested to shutdown start \u00b6 start (): Unit Used when: BrokerServer is requested to startup KafkaServer is requested to startup Implementations \u00b6 DefaultAutoTopicCreationManager Creating AutoTopicCreationManager \u00b6 apply ( config : KafkaConfig , metadataCache : MetadataCache , time : Time , metrics : Metrics , threadNamePrefix : Option [ String ], adminManager : Option [ ZkAdminManager ], controller : Option [ KafkaController ], groupCoordinator : GroupCoordinator , txnCoordinator : TransactionCoordinator , enableForwarding : Boolean ): AutoTopicCreationManager apply creates a DefaultAutoTopicCreationManager . apply is used when: KafkaServer is requested to startup","title":"AutoTopicCreationManager"},{"location":"AutoTopicCreationManager/#autotopiccreationmanager","text":"AutoTopicCreationManager is an abstraction of managers that can create topics .","title":"AutoTopicCreationManager"},{"location":"AutoTopicCreationManager/#contract","text":"","title":"Contract"},{"location":"AutoTopicCreationManager/#createtopics","text":"createTopics ( topicNames : Set [ String ], controllerMutationQuota : ControllerMutationQuota ): Seq [ MetadataResponseTopic ] Used when: KafkaApis is requested to getTopicMetadata and handleFindCoordinatorRequest","title":" createTopics"},{"location":"AutoTopicCreationManager/#shutdown","text":"shutdown (): Unit Used when: BrokerServer is requested to shutdown KafkaServer is requested to shutdown","title":" shutdown"},{"location":"AutoTopicCreationManager/#start","text":"start (): Unit Used when: BrokerServer is requested to startup KafkaServer is requested to startup","title":" start"},{"location":"AutoTopicCreationManager/#implementations","text":"DefaultAutoTopicCreationManager","title":"Implementations"},{"location":"AutoTopicCreationManager/#creating-autotopiccreationmanager","text":"apply ( config : KafkaConfig , metadataCache : MetadataCache , time : Time , metrics : Metrics , threadNamePrefix : Option [ String ], adminManager : Option [ ZkAdminManager ], controller : Option [ KafkaController ], groupCoordinator : GroupCoordinator , txnCoordinator : TransactionCoordinator , enableForwarding : Boolean ): AutoTopicCreationManager apply creates a DefaultAutoTopicCreationManager . apply is used when: KafkaServer is requested to startup","title":" Creating AutoTopicCreationManager"},{"location":"BrokerServer/","text":"BrokerServer \u00b6 startup \u00b6 startup (): Unit startup ...FIXME startup is used when: KafkaRaftServer is requested to startup","title":"BrokerServer"},{"location":"BrokerServer/#brokerserver","text":"","title":"BrokerServer"},{"location":"BrokerServer/#startup","text":"startup (): Unit startup ...FIXME startup is used when: KafkaRaftServer is requested to startup","title":" startup"},{"location":"Configurable/","text":"Configurable \u00b6 Configurable is...FIXME","title":"Configurable"},{"location":"Configurable/#configurable","text":"Configurable is...FIXME","title":"Configurable"},{"location":"DefaultAutoTopicCreationManager/","text":"DefaultAutoTopicCreationManager \u00b6 DefaultAutoTopicCreationManager is a AutoTopicCreationManager . Creating Instance \u00b6 DefaultAutoTopicCreationManager takes the following to be created: KafkaConfig Optional BrokerToControllerChannelManager Optional ZkAdminManager Optional KafkaController GroupCoordinator TransactionCoordinator DefaultAutoTopicCreationManager is created when: AutoTopicCreationManager utility is used to create an AutoTopicCreationManager BrokerServer is requested to startup TransactionCoordinator \u00b6 DefaultAutoTopicCreationManager is given a TransactionCoordinator when created . DefaultAutoTopicCreationManager uses the TransactionCoordinator for the transactionTopicConfigs when requested to creatableTopic for __transaction_state topic.","title":"DefaultAutoTopicCreationManager"},{"location":"DefaultAutoTopicCreationManager/#defaultautotopiccreationmanager","text":"DefaultAutoTopicCreationManager is a AutoTopicCreationManager .","title":"DefaultAutoTopicCreationManager"},{"location":"DefaultAutoTopicCreationManager/#creating-instance","text":"DefaultAutoTopicCreationManager takes the following to be created: KafkaConfig Optional BrokerToControllerChannelManager Optional ZkAdminManager Optional KafkaController GroupCoordinator TransactionCoordinator DefaultAutoTopicCreationManager is created when: AutoTopicCreationManager utility is used to create an AutoTopicCreationManager BrokerServer is requested to startup","title":"Creating Instance"},{"location":"DefaultAutoTopicCreationManager/#transactioncoordinator","text":"DefaultAutoTopicCreationManager is given a TransactionCoordinator when created . DefaultAutoTopicCreationManager uses the TransactionCoordinator for the transactionTopicConfigs when requested to creatableTopic for __transaction_state topic.","title":" TransactionCoordinator"},{"location":"FetchResponseData/","text":"FetchResponseData \u00b6 FetchResponseData is a ApiMessage .","title":"FetchResponseData"},{"location":"FetchResponseData/#fetchresponsedata","text":"FetchResponseData is a ApiMessage .","title":"FetchResponseData"},{"location":"InterBrokerSendThread/","text":"InterBrokerSendThread \u00b6","title":"InterBrokerSendThread"},{"location":"InterBrokerSendThread/#interbrokersendthread","text":"","title":"InterBrokerSendThread"},{"location":"Kafka/","text":"Kafka Utility \u00b6 Kafka is a command-line application . main \u00b6 main ( args : Array [ String ]): Unit main ...FIXME buildServer \u00b6 buildServer ( props : Properties ): Server buildServer creates a KafkaServer or KafkaRaftServer based on process.roles configuration property (in the given Properties ).","title":"Kafka"},{"location":"Kafka/#kafka-utility","text":"Kafka is a command-line application .","title":"Kafka Utility"},{"location":"Kafka/#main","text":"main ( args : Array [ String ]): Unit main ...FIXME","title":" main"},{"location":"Kafka/#buildserver","text":"buildServer ( props : Properties ): Server buildServer creates a KafkaServer or KafkaRaftServer based on process.roles configuration property (in the given Properties ).","title":" buildServer"},{"location":"KafkaApis/","text":"KafkaApis \u00b6 Creating Instance \u00b6 KafkaApis takes the following to be created: RequestChannel MetadataSupport ReplicaManager GroupCoordinator TransactionCoordinator AutoTopicCreationManager Broker ID KafkaConfig ConfigRepository MetadataCache Metrics Optional Authorizer QuotaManagers FetchManager BrokerTopicStats Cluster ID Time DelegationTokenManager ApiVersionManager KafkaApis is created when: BrokerServer is requested to startup (for the dataPlaneRequestProcessor and the controlPlaneRequestProcessor ) KafkaServer is requested to startup (for the dataPlaneRequestProcessor and the controlPlaneRequestProcessor ) TransactionCoordinator \u00b6 KafkaApis is given a TransactionCoordinator when created . The TransactionCoordinator is used for the following: handleAddOffsetsToTxnRequest handleAddPartitionToTxnRequest handleEndTxnRequest handleFindCoordinatorRequest handleInitProducerIdRequest handleLeaderAndIsrRequest handleStopReplicaRequest handleInitProducerIdRequest \u00b6 handleInitProducerIdRequest ( request : RequestChannel . Request ): Unit handleInitProducerIdRequest assumes that the given RequestChannel.Request is an InitProducerIdRequest . handleInitProducerIdRequest authorizes the request. With producerId and producerEpoch set either to -1 s ( NO_PRODUCER_ID and NO_PRODUCER_EPOCH ) or some non- -1 values, handleInitProducerIdRequest requests the TransactionCoordinator to handleInitProducerId . Otherwise, handleInitProducerIdRequest sends an error back. handleInitProducerIdRequest is used when: KafkaApis is requested to handle a INIT_PRODUCER_ID request handleFetchRequest \u00b6 handleFetchRequest ( request : RequestChannel . Request ): Unit handleFetchRequest assumes that the given RequestChannel.Request is an FetchRequest . handleFetchRequest authorizes the request. In the end, handleFetchRequest requests the ReplicaManager to fetchMessages . handleFetchRequest is used when: KafkaApis is requested to handle a FETCH request","title":"KafkaApis"},{"location":"KafkaApis/#kafkaapis","text":"","title":"KafkaApis"},{"location":"KafkaApis/#creating-instance","text":"KafkaApis takes the following to be created: RequestChannel MetadataSupport ReplicaManager GroupCoordinator TransactionCoordinator AutoTopicCreationManager Broker ID KafkaConfig ConfigRepository MetadataCache Metrics Optional Authorizer QuotaManagers FetchManager BrokerTopicStats Cluster ID Time DelegationTokenManager ApiVersionManager KafkaApis is created when: BrokerServer is requested to startup (for the dataPlaneRequestProcessor and the controlPlaneRequestProcessor ) KafkaServer is requested to startup (for the dataPlaneRequestProcessor and the controlPlaneRequestProcessor )","title":"Creating Instance"},{"location":"KafkaApis/#transactioncoordinator","text":"KafkaApis is given a TransactionCoordinator when created . The TransactionCoordinator is used for the following: handleAddOffsetsToTxnRequest handleAddPartitionToTxnRequest handleEndTxnRequest handleFindCoordinatorRequest handleInitProducerIdRequest handleLeaderAndIsrRequest handleStopReplicaRequest","title":" TransactionCoordinator"},{"location":"KafkaApis/#handleinitproduceridrequest","text":"handleInitProducerIdRequest ( request : RequestChannel . Request ): Unit handleInitProducerIdRequest assumes that the given RequestChannel.Request is an InitProducerIdRequest . handleInitProducerIdRequest authorizes the request. With producerId and producerEpoch set either to -1 s ( NO_PRODUCER_ID and NO_PRODUCER_EPOCH ) or some non- -1 values, handleInitProducerIdRequest requests the TransactionCoordinator to handleInitProducerId . Otherwise, handleInitProducerIdRequest sends an error back. handleInitProducerIdRequest is used when: KafkaApis is requested to handle a INIT_PRODUCER_ID request","title":" handleInitProducerIdRequest"},{"location":"KafkaApis/#handlefetchrequest","text":"handleFetchRequest ( request : RequestChannel . Request ): Unit handleFetchRequest assumes that the given RequestChannel.Request is an FetchRequest . handleFetchRequest authorizes the request. In the end, handleFetchRequest requests the ReplicaManager to fetchMessages . handleFetchRequest is used when: KafkaApis is requested to handle a FETCH request","title":" handleFetchRequest"},{"location":"KafkaConfig/","text":"KafkaConfig \u00b6 transactional.id.expiration.ms \u00b6 transaction.max.timeout.ms \u00b6 transaction.state.log.num.partitions \u00b6 The number of partitions for the transaction topic Default: 50 Must be at least 1 transaction.state.log.replication.factor \u00b6 transaction.state.log.segment.bytes \u00b6 transaction.state.log.load.buffer.size \u00b6 transaction.state.log.min.isr \u00b6 transaction.abort.timed.out.transaction.cleanup.interval.ms \u00b6 transaction.remove.expired.transaction.cleanup.interval.ms \u00b6 process.roles \u00b6 The roles that this process plays: broker , controller , or broker,controller . When undefined or empty the cluster runs with Zookeeper. Default: (empty) Supported values: broker controller This configuration is only applicable for clusters in KRaft (Kafka Raft) mode (instead of ZooKeeper). request.timeout.ms \u00b6 request.timeout.ms","title":"KafkaConfig"},{"location":"KafkaConfig/#kafkaconfig","text":"","title":"KafkaConfig"},{"location":"KafkaConfig/#transactionalidexpirationms","text":"","title":" transactional.id.expiration.ms"},{"location":"KafkaConfig/#transactionmaxtimeoutms","text":"","title":" transaction.max.timeout.ms"},{"location":"KafkaConfig/#transactionstatelognumpartitions","text":"The number of partitions for the transaction topic Default: 50 Must be at least 1","title":" transaction.state.log.num.partitions"},{"location":"KafkaConfig/#transactionstatelogreplicationfactor","text":"","title":" transaction.state.log.replication.factor"},{"location":"KafkaConfig/#transactionstatelogsegmentbytes","text":"","title":" transaction.state.log.segment.bytes"},{"location":"KafkaConfig/#transactionstatelogloadbuffersize","text":"","title":" transaction.state.log.load.buffer.size"},{"location":"KafkaConfig/#transactionstatelogminisr","text":"","title":" transaction.state.log.min.isr"},{"location":"KafkaConfig/#transactionaborttimedouttransactioncleanupintervalms","text":"","title":" transaction.abort.timed.out.transaction.cleanup.interval.ms"},{"location":"KafkaConfig/#transactionremoveexpiredtransactioncleanupintervalms","text":"","title":" transaction.remove.expired.transaction.cleanup.interval.ms"},{"location":"KafkaConfig/#processroles","text":"The roles that this process plays: broker , controller , or broker,controller . When undefined or empty the cluster runs with Zookeeper. Default: (empty) Supported values: broker controller This configuration is only applicable for clusters in KRaft (Kafka Raft) mode (instead of ZooKeeper).","title":" process.roles"},{"location":"KafkaConfig/#requesttimeoutms","text":"request.timeout.ms","title":" request.timeout.ms"},{"location":"KafkaRaftServer/","text":"KafkaRaftServer \u00b6 KafkaRaftServer is a Server . startup \u00b6 startup (): Unit startup ...FIXME startup is part of the Server abstraction.","title":"KafkaRaftServer"},{"location":"KafkaRaftServer/#kafkaraftserver","text":"KafkaRaftServer is a Server .","title":"KafkaRaftServer"},{"location":"KafkaRaftServer/#startup","text":"startup (): Unit startup ...FIXME startup is part of the Server abstraction.","title":" startup"},{"location":"KafkaServer/","text":"KafkaServer \u00b6 KafkaServer is a Server that Kafka command-line application uses in Zookeeper mode (when executed with the process.roles configuration property undefined). Creating Instance \u00b6 KafkaServer takes the following to be created: KafkaConfig Time (default: SYSTEM ) Optional Thread Name Prefix (default: undefined) enableForwarding flag (default: false ) KafkaServer is created when: Kafka command-line application is launched (and builds a server ) TransactionCoordinator \u00b6 KafkaServer creates and starts a TransactionCoordinator when created . KafkaServer uses the TransactionCoordinator to create the following: data-plane and the control-plane request processors AutoTopicCreationManager The TransactionCoordinator is requested to shutdown along with KafkaServer . Data-Plane Request Processor \u00b6 KafkaServer creates a KafkaApis for data-related communication. KafkaApis is used to create data-plane request handler pool . KafkaRequestHandlerPool \u00b6 Control-Plane Request Processor \u00b6 KafkaServer creates a KafkaApis for control-related communication. startup \u00b6 startup (): Unit startup prints out the following INFO message to the logs: starting startup initZkClient and creates a ZkConfigRepository . startup ...FIXME startup prints out the following INFO message to the logs: Cluster ID = [clusterId] startup ...FIXME startup creates a TransactionCoordinator (with the ReplicaManager ) and requests it to startup . startup ...FIXME startup is part of the Server abstraction. Logging \u00b6 Enable ALL logging level for kafka.server.KafkaServer logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.kafka.server.KafkaServer=ALL Refer to Logging .","title":"KafkaServer"},{"location":"KafkaServer/#kafkaserver","text":"KafkaServer is a Server that Kafka command-line application uses in Zookeeper mode (when executed with the process.roles configuration property undefined).","title":"KafkaServer"},{"location":"KafkaServer/#creating-instance","text":"KafkaServer takes the following to be created: KafkaConfig Time (default: SYSTEM ) Optional Thread Name Prefix (default: undefined) enableForwarding flag (default: false ) KafkaServer is created when: Kafka command-line application is launched (and builds a server )","title":"Creating Instance"},{"location":"KafkaServer/#transactioncoordinator","text":"KafkaServer creates and starts a TransactionCoordinator when created . KafkaServer uses the TransactionCoordinator to create the following: data-plane and the control-plane request processors AutoTopicCreationManager The TransactionCoordinator is requested to shutdown along with KafkaServer .","title":" TransactionCoordinator"},{"location":"KafkaServer/#data-plane-request-processor","text":"KafkaServer creates a KafkaApis for data-related communication. KafkaApis is used to create data-plane request handler pool .","title":" Data-Plane Request Processor"},{"location":"KafkaServer/#kafkarequesthandlerpool","text":"","title":" KafkaRequestHandlerPool"},{"location":"KafkaServer/#control-plane-request-processor","text":"KafkaServer creates a KafkaApis for control-related communication.","title":" Control-Plane Request Processor"},{"location":"KafkaServer/#startup","text":"startup (): Unit startup prints out the following INFO message to the logs: starting startup initZkClient and creates a ZkConfigRepository . startup ...FIXME startup prints out the following INFO message to the logs: Cluster ID = [clusterId] startup ...FIXME startup creates a TransactionCoordinator (with the ReplicaManager ) and requests it to startup . startup ...FIXME startup is part of the Server abstraction.","title":" startup"},{"location":"KafkaServer/#logging","text":"Enable ALL logging level for kafka.server.KafkaServer logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.kafka.server.KafkaServer=ALL Refer to Logging .","title":"Logging"},{"location":"Log/","text":"Log \u00b6 Creating Instance \u00b6 Log takes the following to be created: Directory LogConfig LogSegments logStartOffset recoveryPoint LogOffsetMetadata Scheduler BrokerTopicStats Time producerIdExpirationCheckIntervalMs TopicPartition Optional LeaderEpochFileCache ProducerStateManager LogDirFailureChannel Optional Topic ID keepPartitionMetadataFile Log is created using apply utility. Creating Log \u00b6 apply ( dir : File , config : LogConfig , logStartOffset : Long , recoveryPoint : Long , scheduler : Scheduler , brokerTopicStats : BrokerTopicStats , time : Time = Time . SYSTEM , maxProducerIdExpirationMs : Int , producerIdExpirationCheckIntervalMs : Int , logDirFailureChannel : LogDirFailureChannel , lastShutdownClean : Boolean = true , topicId : Option [ Uuid ], keepPartitionMetadataFile : Boolean ): Log apply ...FIXME apply is used when: LogManager is requested to loadLog and getOrCreateLog KafkaMetadataLog is requested to apply Reading Messages \u00b6 read ( startOffset : Long , maxLength : Int , isolation : FetchIsolation , minOneMessage : Boolean ): FetchDataInfo read prints out the following TRACE message to the logs: Reading maximum [maxLength] bytes at offset [startOffset] from log with total length [size] bytes read ...FIXME read requests the LogSegment to read messages . read ...FIXME read is used when: Partition is requested to readRecords GroupMetadataManager is requested to doLoadGroupsAndOffsets TransactionStateManager is requested to loadTransactionMetadata Log is requested to convertToOffsetMetadataOrThrow KafkaMetadataLog is requested to read Logging \u00b6 Enable ALL logging level for kafka.log.Log logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.kafka.log.Log=ALL Refer to Logging .","title":"Log"},{"location":"Log/#log","text":"","title":"Log"},{"location":"Log/#creating-instance","text":"Log takes the following to be created: Directory LogConfig LogSegments logStartOffset recoveryPoint LogOffsetMetadata Scheduler BrokerTopicStats Time producerIdExpirationCheckIntervalMs TopicPartition Optional LeaderEpochFileCache ProducerStateManager LogDirFailureChannel Optional Topic ID keepPartitionMetadataFile Log is created using apply utility.","title":"Creating Instance"},{"location":"Log/#creating-log","text":"apply ( dir : File , config : LogConfig , logStartOffset : Long , recoveryPoint : Long , scheduler : Scheduler , brokerTopicStats : BrokerTopicStats , time : Time = Time . SYSTEM , maxProducerIdExpirationMs : Int , producerIdExpirationCheckIntervalMs : Int , logDirFailureChannel : LogDirFailureChannel , lastShutdownClean : Boolean = true , topicId : Option [ Uuid ], keepPartitionMetadataFile : Boolean ): Log apply ...FIXME apply is used when: LogManager is requested to loadLog and getOrCreateLog KafkaMetadataLog is requested to apply","title":" Creating Log"},{"location":"Log/#reading-messages","text":"read ( startOffset : Long , maxLength : Int , isolation : FetchIsolation , minOneMessage : Boolean ): FetchDataInfo read prints out the following TRACE message to the logs: Reading maximum [maxLength] bytes at offset [startOffset] from log with total length [size] bytes read ...FIXME read requests the LogSegment to read messages . read ...FIXME read is used when: Partition is requested to readRecords GroupMetadataManager is requested to doLoadGroupsAndOffsets TransactionStateManager is requested to loadTransactionMetadata Log is requested to convertToOffsetMetadataOrThrow KafkaMetadataLog is requested to read","title":" Reading Messages"},{"location":"Log/#logging","text":"Enable ALL logging level for kafka.log.Log logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.kafka.log.Log=ALL Refer to Logging .","title":"Logging"},{"location":"LogSegment/","text":"LogSegment \u00b6 Creating Instance \u00b6 LogSegment takes the following to be created: FileRecords OffsetIndex TimeIndex TransactionIndex baseOffset indexIntervalBytes rollJitterMs Time LogSegment is created using open utility. Opening LogSegment \u00b6 open ( dir : File , baseOffset : Long , config : LogConfig , time : Time , fileAlreadyExists : Boolean = false , initFileSize : Int = 0 , preallocate : Boolean = false , fileSuffix : String = \"\" ): LogSegment open ...FIXME open is used when: Log is requested to roll a log segment and truncateFullyAndStartAt LogCleaner is requested to createNewCleanedSegment LogLoader is requested to load , loadSegmentFiles and recoverLog Reading Messages \u00b6 read ( startOffset : Long , maxSize : Int , maxPosition : Long = size , minOneMessage : Boolean = false ): FetchDataInfo read ...FIXME read is used when: Log is requested to read LogSegment is requested to readNextOffset","title":"LogSegment"},{"location":"LogSegment/#logsegment","text":"","title":"LogSegment"},{"location":"LogSegment/#creating-instance","text":"LogSegment takes the following to be created: FileRecords OffsetIndex TimeIndex TransactionIndex baseOffset indexIntervalBytes rollJitterMs Time LogSegment is created using open utility.","title":"Creating Instance"},{"location":"LogSegment/#opening-logsegment","text":"open ( dir : File , baseOffset : Long , config : LogConfig , time : Time , fileAlreadyExists : Boolean = false , initFileSize : Int = 0 , preallocate : Boolean = false , fileSuffix : String = \"\" ): LogSegment open ...FIXME open is used when: Log is requested to roll a log segment and truncateFullyAndStartAt LogCleaner is requested to createNewCleanedSegment LogLoader is requested to load , loadSegmentFiles and recoverLog","title":" Opening LogSegment"},{"location":"LogSegment/#reading-messages","text":"read ( startOffset : Long , maxSize : Int , maxPosition : Long = size , minOneMessage : Boolean = false ): FetchDataInfo read ...FIXME read is used when: Log is requested to read LogSegment is requested to readNextOffset","title":" Reading Messages"},{"location":"Partition/","text":"Partition \u00b6 readRecords \u00b6 readRecords ( lastFetchedEpoch : Optional [ Integer ], fetchOffset : Long , currentLeaderEpoch : Optional [ Integer ], maxBytes : Int , fetchIsolation : FetchIsolation , fetchOnlyFromLeader : Boolean , minOneMessage : Boolean ): LogReadInfo readRecords ...FIXME In the end, readRecords requests the Log to read messages . readRecords is used when: ReplicaManager is requested to readFromLocalLog","title":"Partition"},{"location":"Partition/#partition","text":"","title":"Partition"},{"location":"Partition/#readrecords","text":"readRecords ( lastFetchedEpoch : Optional [ Integer ], fetchOffset : Long , currentLeaderEpoch : Optional [ Integer ], maxBytes : Int , fetchIsolation : FetchIsolation , fetchOnlyFromLeader : Boolean , minOneMessage : Boolean ): LogReadInfo readRecords ...FIXME In the end, readRecords requests the Log to read messages . readRecords is used when: ReplicaManager is requested to readFromLocalLog","title":" readRecords"},{"location":"PartitionData/","text":"PartitionData \u00b6 PartitionData is a Message of FetchResponseData . abortedTransactions \u00b6 List < AbortedTransaction > abortedTransactions List < AbortedTransaction > abortedTransactions () abortedTransactions ...FIXME abortedTransactions is used when: FIXME Preferred Read Replica \u00b6 Default: -1 preferredReadReplica is used when: Fetcher is requested to initializeCompletedFetch","title":"PartitionData"},{"location":"PartitionData/#partitiondata","text":"PartitionData is a Message of FetchResponseData .","title":"PartitionData"},{"location":"PartitionData/#abortedtransactions","text":"List < AbortedTransaction > abortedTransactions List < AbortedTransaction > abortedTransactions () abortedTransactions ...FIXME abortedTransactions is used when: FIXME","title":" abortedTransactions"},{"location":"PartitionData/#preferred-read-replica","text":"Default: -1 preferredReadReplica is used when: Fetcher is requested to initializeCompletedFetch","title":" Preferred Read Replica"},{"location":"ReplicaAlterLogDirsThread/","text":"ReplicaAlterLogDirsThread \u00b6 ReplicaAlterLogDirsThread is...FIXME","title":"ReplicaAlterLogDirsThread"},{"location":"ReplicaAlterLogDirsThread/#replicaalterlogdirsthread","text":"ReplicaAlterLogDirsThread is...FIXME","title":"ReplicaAlterLogDirsThread"},{"location":"ReplicaManager/","text":"ReplicaManager \u00b6 fetchMessages \u00b6 fetchMessages ( timeout : Long , replicaId : Int , fetchMinBytes : Int , fetchMaxBytes : Int , hardMaxBytesLimit : Boolean , fetchInfos : Seq [( TopicPartition , PartitionData )], quota : ReplicaQuota , responseCallback : Seq [( TopicPartition , FetchPartitionData )] => Unit , isolationLevel : IsolationLevel , clientMetadata : Option [ ClientMetadata ]): Unit fetchMessages determines whether the request comes from a follower or a consumer (based on the given replicaId ). fetchMessages determines FetchIsolation : FetchLogEnd if the request comes from a follower FetchTxnCommitted if the request comes from a consumer with READ_COMMITTED isolation level FetchHighWatermark otherwise fetchMessages readFromLocalLog (passing in the FetchIsolation among the others). fetchMessages ...FIXME fetchMessages is used when: KafkaApis is requested to handle a Fetch request ReplicaAlterLogDirsThread is requested to fetchFromLeader readFromLocalLog \u00b6 readFromLocalLog ( replicaId : Int , fetchOnlyFromLeader : Boolean , fetchIsolation : FetchIsolation , fetchMaxBytes : Int , hardMaxBytesLimit : Boolean , readPartitionInfo : Seq [( TopicPartition , PartitionData )], quota : ReplicaQuota , clientMetadata : Option [ ClientMetadata ]): Seq [( TopicPartition , LogReadResult )] readFromLocalLog ...FIXME readFromLocalLog finds the Partition and requests it to readRecords . readFromLocalLog ...FIXME readFromLocalLog is used when: DelayedFetch is requested to onComplete ReplicaManager is requested to fetchMessages","title":"ReplicaManager"},{"location":"ReplicaManager/#replicamanager","text":"","title":"ReplicaManager"},{"location":"ReplicaManager/#fetchmessages","text":"fetchMessages ( timeout : Long , replicaId : Int , fetchMinBytes : Int , fetchMaxBytes : Int , hardMaxBytesLimit : Boolean , fetchInfos : Seq [( TopicPartition , PartitionData )], quota : ReplicaQuota , responseCallback : Seq [( TopicPartition , FetchPartitionData )] => Unit , isolationLevel : IsolationLevel , clientMetadata : Option [ ClientMetadata ]): Unit fetchMessages determines whether the request comes from a follower or a consumer (based on the given replicaId ). fetchMessages determines FetchIsolation : FetchLogEnd if the request comes from a follower FetchTxnCommitted if the request comes from a consumer with READ_COMMITTED isolation level FetchHighWatermark otherwise fetchMessages readFromLocalLog (passing in the FetchIsolation among the others). fetchMessages ...FIXME fetchMessages is used when: KafkaApis is requested to handle a Fetch request ReplicaAlterLogDirsThread is requested to fetchFromLeader","title":" fetchMessages"},{"location":"ReplicaManager/#readfromlocallog","text":"readFromLocalLog ( replicaId : Int , fetchOnlyFromLeader : Boolean , fetchIsolation : FetchIsolation , fetchMaxBytes : Int , hardMaxBytesLimit : Boolean , readPartitionInfo : Seq [( TopicPartition , PartitionData )], quota : ReplicaQuota , clientMetadata : Option [ ClientMetadata ]): Seq [( TopicPartition , LogReadResult )] readFromLocalLog ...FIXME readFromLocalLog finds the Partition and requests it to readRecords . readFromLocalLog ...FIXME readFromLocalLog is used when: DelayedFetch is requested to onComplete ReplicaManager is requested to fetchMessages","title":" readFromLocalLog"},{"location":"RequestHandlerHelper/","text":"RequestHandlerHelper \u00b6 Creating Instance \u00b6 RequestHandlerHelper takes the following to be created: RequestChannel QuotaManagers Time Log Prefix RequestHandlerHelper is created when: ControllerApis is created ( requestHelper ) KafkaApis is created onLeadershipChange \u00b6 onLeadershipChange ( groupCoordinator : GroupCoordinator , txnCoordinator : TransactionCoordinator , updatedLeaders : Iterable [ Partition ], updatedFollowers : Iterable [ Partition ]): Unit onLeadershipChange ...FIXME onLeadershipChange is used when: BrokerServer is requested to start up KafkaApis is requested to handleLeaderAndIsrRequest BrokerMetadataListener is requested to handleCommits and execCommits","title":"RequestHandlerHelper"},{"location":"RequestHandlerHelper/#requesthandlerhelper","text":"","title":"RequestHandlerHelper"},{"location":"RequestHandlerHelper/#creating-instance","text":"RequestHandlerHelper takes the following to be created: RequestChannel QuotaManagers Time Log Prefix RequestHandlerHelper is created when: ControllerApis is created ( requestHelper ) KafkaApis is created","title":"Creating Instance"},{"location":"RequestHandlerHelper/#onleadershipchange","text":"onLeadershipChange ( groupCoordinator : GroupCoordinator , txnCoordinator : TransactionCoordinator , updatedLeaders : Iterable [ Partition ], updatedFollowers : Iterable [ Partition ]): Unit onLeadershipChange ...FIXME onLeadershipChange is used when: BrokerServer is requested to start up KafkaApis is requested to handleLeaderAndIsrRequest BrokerMetadataListener is requested to handleCommits and execCommits","title":" onLeadershipChange"},{"location":"Server/","text":"Server \u00b6 Server is an abstraction of Kafka servers ( brokers ). Contract \u00b6 awaitShutdown \u00b6 awaitShutdown (): Unit Used when: FIXME startup \u00b6 startup (): Unit Used when: Kafka utility is executed (on command line) shutdown \u00b6 shutdown (): Unit Used when: FIXME Implementations \u00b6 KafkaRaftServer KafkaServer","title":"Server"},{"location":"Server/#server","text":"Server is an abstraction of Kafka servers ( brokers ).","title":"Server"},{"location":"Server/#contract","text":"","title":"Contract"},{"location":"Server/#awaitshutdown","text":"awaitShutdown (): Unit Used when: FIXME","title":" awaitShutdown"},{"location":"Server/#startup","text":"startup (): Unit Used when: Kafka utility is executed (on command line)","title":" startup"},{"location":"Server/#shutdown","text":"shutdown (): Unit Used when: FIXME","title":" shutdown"},{"location":"Server/#implementations","text":"KafkaRaftServer KafkaServer","title":"Implementations"},{"location":"Utils/","text":"Utils \u00b6 murmur2 \u00b6 int murmur2 ( byte [] data ) murmur2 generates a 32-bit murmur2 hash for the given byte array. murmur2 is used when: DefaultPartitioner is requested to compute a partition for a record Demo \u00b6 import org.apache.kafka.common.utils.Utils val keyBytes = \"hello\".getBytes val hash = Utils.murmur2(keyBytes) println(hash) toPositive \u00b6 int toPositive ( int number ) toPositive converts a number to a positive value.","title":"Utils"},{"location":"Utils/#utils","text":"","title":"Utils"},{"location":"Utils/#murmur2","text":"int murmur2 ( byte [] data ) murmur2 generates a 32-bit murmur2 hash for the given byte array. murmur2 is used when: DefaultPartitioner is requested to compute a partition for a record","title":" murmur2"},{"location":"Utils/#demo","text":"import org.apache.kafka.common.utils.Utils val keyBytes = \"hello\".getBytes val hash = Utils.murmur2(keyBytes) println(hash)","title":" Demo"},{"location":"Utils/#topositive","text":"int toPositive ( int number ) toPositive converts a number to a positive value.","title":" toPositive"},{"location":"building-from-sources/","text":"Building from Sources \u00b6 Based on README.md : KAFKA_VERSION=3.0.0 SCALA_VERSION=2.13 $ java -version openjdk version \"11.0.12\" 2021-07-20 OpenJDK Runtime Environment Temurin-11.0.12+7 (build 11.0.12+7) OpenJDK 64-Bit Server VM Temurin-11.0.12+7 (build 11.0.12+7, mixed mode) ./gradlew clean releaseTarGz install -PskipSigning=true && \\ tar -zxvf core/build/distributions/kafka_$SCALA_VERSION-$KAFKA_VERSION.tgz cd kafka_$SCALA_VERSION-$KAFKA_VERSION $ ./bin/kafka-server-start.sh --version | tail -1 3.0.0 (Commit:8cb0a5e9d3441962)","title":"Building from Sources"},{"location":"building-from-sources/#building-from-sources","text":"Based on README.md : KAFKA_VERSION=3.0.0 SCALA_VERSION=2.13 $ java -version openjdk version \"11.0.12\" 2021-07-20 OpenJDK Runtime Environment Temurin-11.0.12+7 (build 11.0.12+7) OpenJDK 64-Bit Server VM Temurin-11.0.12+7 (build 11.0.12+7, mixed mode) ./gradlew clean releaseTarGz install -PskipSigning=true && \\ tar -zxvf core/build/distributions/kafka_$SCALA_VERSION-$KAFKA_VERSION.tgz cd kafka_$SCALA_VERSION-$KAFKA_VERSION $ ./bin/kafka-server-start.sh --version | tail -1 3.0.0 (Commit:8cb0a5e9d3441962)","title":"Building from Sources"},{"location":"logging/","text":"Logging \u00b6 Brokers \u00b6 Kafka brokers use Apache Log4j 2 for logging and use config/log4j.properties by default. The default logging level is INFO with stdout appender. log4j.rootLogger=INFO, stdout, kafkaAppender log4j.logger.kafka=INFO log4j.logger.org.apache.kafka=INFO Tools \u00b6 Kafka tools (e.g. kafka-console-producer ) use config/tools-log4j.properties as the logging configuration file. Clients \u00b6 build.sbt \u00b6 libraryDependencies += \"org.apache.kafka\" % \"kafka-clients\" % \"2.8.0\" val slf4j = \"1.7.32\" libraryDependencies += \"org.slf4j\" % \"slf4j-api\" % slf4j libraryDependencies += \"org.slf4j\" % \"slf4j-log4j12\" % slf4j log4j.properties \u00b6 In src/main/resources/log4j.properties use the following: log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL","title":"Logging"},{"location":"logging/#logging","text":"","title":"Logging"},{"location":"logging/#brokers","text":"Kafka brokers use Apache Log4j 2 for logging and use config/log4j.properties by default. The default logging level is INFO with stdout appender. log4j.rootLogger=INFO, stdout, kafkaAppender log4j.logger.kafka=INFO log4j.logger.org.apache.kafka=INFO","title":"Brokers"},{"location":"logging/#tools","text":"Kafka tools (e.g. kafka-console-producer ) use config/tools-log4j.properties as the logging configuration file.","title":"Tools"},{"location":"logging/#clients","text":"","title":"Clients"},{"location":"logging/#buildsbt","text":"libraryDependencies += \"org.apache.kafka\" % \"kafka-clients\" % \"2.8.0\" val slf4j = \"1.7.32\" libraryDependencies += \"org.slf4j\" % \"slf4j-api\" % slf4j libraryDependencies += \"org.slf4j\" % \"slf4j-log4j12\" % slf4j","title":"build.sbt"},{"location":"logging/#log4jproperties","text":"In src/main/resources/log4j.properties use the following: log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL","title":"log4j.properties"},{"location":"overview/","text":"Apache Kafka \u00b6 Apache Kafka is an open source project for a distributed publish-subscribe messaging system rethought as a distributed commit log . Messages \u00b6 Messages ( records , events ) are byte arrays (String, JSON, and Avro are among the most common formats). If a message has a key, Kafka (uses Partitioner ) to make sure that all messages of the same key are in the same partition. Topics \u00b6 Kafka stores messages in topics that are partitioned and replicated across multiple brokers in a cluster. Kafka Clients \u00b6 Producers send messages to topics from which consumers read. Language Agnostic \u00b6 Kafka clients use binary protocol to talk to a Kafka cluster. Consumer Groups \u00b6 Consumers may be grouped in a consumer group with multiple consumers. Each consumer in a consumer group will read messages from a unique subset of partitions in each topic they subscribe to. Each message is delivered to one consumer in the group, and all messages with the same key arrive to the same consumer. Durability \u00b6 Kafka does not track which messages were read by consumers. Kafka keeps all messages for a finite amount of time, and it is consumers' responsibility to track their location per topic (offsets).","title":"Overview"},{"location":"overview/#apache-kafka","text":"Apache Kafka is an open source project for a distributed publish-subscribe messaging system rethought as a distributed commit log .","title":"Apache Kafka"},{"location":"overview/#messages","text":"Messages ( records , events ) are byte arrays (String, JSON, and Avro are among the most common formats). If a message has a key, Kafka (uses Partitioner ) to make sure that all messages of the same key are in the same partition.","title":"Messages"},{"location":"overview/#topics","text":"Kafka stores messages in topics that are partitioned and replicated across multiple brokers in a cluster.","title":"Topics"},{"location":"overview/#kafka-clients","text":"Producers send messages to topics from which consumers read.","title":"Kafka Clients"},{"location":"overview/#language-agnostic","text":"Kafka clients use binary protocol to talk to a Kafka cluster.","title":"Language Agnostic"},{"location":"overview/#consumer-groups","text":"Consumers may be grouped in a consumer group with multiple consumers. Each consumer in a consumer group will read messages from a unique subset of partitions in each topic they subscribe to. Each message is delivered to one consumer in the group, and all messages with the same key arrive to the same consumer.","title":"Consumer Groups"},{"location":"overview/#durability","text":"Kafka does not track which messages were read by consumers. Kafka keeps all messages for a finite amount of time, and it is consumers' responsibility to track their location per topic (offsets).","title":"Durability"},{"location":"clients/","text":"Kafka Clients \u00b6","title":"Kafka Clients"},{"location":"clients/#kafka-clients","text":"","title":"Kafka Clients"},{"location":"clients/CommonClientConfigs/","text":"CommonClientConfigs \u00b6 client.id \u00b6 retries \u00b6 retry.backoff.ms \u00b6 request.timeout.ms \u00b6","title":"CommonClientConfigs"},{"location":"clients/CommonClientConfigs/#commonclientconfigs","text":"","title":"CommonClientConfigs"},{"location":"clients/CommonClientConfigs/#clientid","text":"","title":" client.id"},{"location":"clients/CommonClientConfigs/#retries","text":"","title":" retries"},{"location":"clients/CommonClientConfigs/#retrybackoffms","text":"","title":" retry.backoff.ms"},{"location":"clients/CommonClientConfigs/#requesttimeoutms","text":"","title":" request.timeout.ms"},{"location":"clients/KafkaClient/","text":"KafkaClient \u00b6 KafkaClient is an interface to NetworkClient . Contract \u00b6 inFlightRequestCount \u00b6 int inFlightRequestCount () int inFlightRequestCount ( String nodeId ) Used when: ConsumerNetworkClient is requested to pendingRequestCount and poll Sender is requested to run SenderMetrics is requested for requests-in-flight performance metric leastLoadedNode \u00b6 Node leastLoadedNode ( long now ) Used when: ConsumerNetworkClient is requested for the leastLoadedNode DefaultMetadataUpdater is requested to maybeUpdate KafkaAdminClient is used Sender is requested for the maybeSendAndPollTransactionalRequest newClientRequest \u00b6 ClientRequest newClientRequest ( String nodeId , AbstractRequest . Builder <?> requestBuilder , long createdTimeMs , boolean expectResponse ) ClientRequest newClientRequest ( String nodeId , AbstractRequest . Builder <?> requestBuilder , long createdTimeMs , boolean expectResponse , int requestTimeoutMs , RequestCompletionHandler callback ) Used when: AdminClientRunnable is requested to sendEligibleCalls ConsumerNetworkClient is requested to send NetworkClient is requested to newClientRequest , sendInternalMetadataRequest and handleInitiateApiVersionRequests RequestSendThread is requested to doWork Sender is requested to run KafkaServer is requested to controlledShutdown ReplicaFetcherBlockingSend is requested to sendRequest ReplicaVerificationTool is used poll \u00b6 List < ClientResponse > poll ( long timeout , long now ) Used when: FIXME pollDelayMs \u00b6 long pollDelayMs ( Node node , long now ) Used when: FIXME Is Node Ready and Connected \u00b6 boolean ready ( Node node , long now ); Used when: AdminClientRunnable is requested to sendEligibleCalls ConsumerNetworkClient is requested to tryConnect and trySend InterBrokerSendThread is requested to sendRequests NetworkClientUtils is requested to awaitReady Sender is requested to sendProducerData send \u00b6 void send ( ClientRequest request , long now ) Used when: FIXME wakeup \u00b6 void wakeup () Used when: FIXME Implementations \u00b6 NetworkClient Closeable \u00b6 KafkaClient is a Closeable ( Java ).","title":"KafkaClient"},{"location":"clients/KafkaClient/#kafkaclient","text":"KafkaClient is an interface to NetworkClient .","title":"KafkaClient"},{"location":"clients/KafkaClient/#contract","text":"","title":"Contract"},{"location":"clients/KafkaClient/#inflightrequestcount","text":"int inFlightRequestCount () int inFlightRequestCount ( String nodeId ) Used when: ConsumerNetworkClient is requested to pendingRequestCount and poll Sender is requested to run SenderMetrics is requested for requests-in-flight performance metric","title":" inFlightRequestCount"},{"location":"clients/KafkaClient/#leastloadednode","text":"Node leastLoadedNode ( long now ) Used when: ConsumerNetworkClient is requested for the leastLoadedNode DefaultMetadataUpdater is requested to maybeUpdate KafkaAdminClient is used Sender is requested for the maybeSendAndPollTransactionalRequest","title":" leastLoadedNode"},{"location":"clients/KafkaClient/#newclientrequest","text":"ClientRequest newClientRequest ( String nodeId , AbstractRequest . Builder <?> requestBuilder , long createdTimeMs , boolean expectResponse ) ClientRequest newClientRequest ( String nodeId , AbstractRequest . Builder <?> requestBuilder , long createdTimeMs , boolean expectResponse , int requestTimeoutMs , RequestCompletionHandler callback ) Used when: AdminClientRunnable is requested to sendEligibleCalls ConsumerNetworkClient is requested to send NetworkClient is requested to newClientRequest , sendInternalMetadataRequest and handleInitiateApiVersionRequests RequestSendThread is requested to doWork Sender is requested to run KafkaServer is requested to controlledShutdown ReplicaFetcherBlockingSend is requested to sendRequest ReplicaVerificationTool is used","title":" newClientRequest"},{"location":"clients/KafkaClient/#poll","text":"List < ClientResponse > poll ( long timeout , long now ) Used when: FIXME","title":" poll"},{"location":"clients/KafkaClient/#polldelayms","text":"long pollDelayMs ( Node node , long now ) Used when: FIXME","title":" pollDelayMs"},{"location":"clients/KafkaClient/#is-node-ready-and-connected","text":"boolean ready ( Node node , long now ); Used when: AdminClientRunnable is requested to sendEligibleCalls ConsumerNetworkClient is requested to tryConnect and trySend InterBrokerSendThread is requested to sendRequests NetworkClientUtils is requested to awaitReady Sender is requested to sendProducerData","title":" Is Node Ready and Connected"},{"location":"clients/KafkaClient/#send","text":"void send ( ClientRequest request , long now ) Used when: FIXME","title":" send"},{"location":"clients/KafkaClient/#wakeup","text":"void wakeup () Used when: FIXME","title":" wakeup"},{"location":"clients/KafkaClient/#implementations","text":"NetworkClient","title":"Implementations"},{"location":"clients/KafkaClient/#closeable","text":"KafkaClient is a Closeable ( Java ).","title":" Closeable"},{"location":"clients/Metadata/","text":"Metadata \u00b6 update \u00b6 void update ( int requestVersion , MetadataResponse response , boolean isPartialUpdate , long nowMs ) update ...FIXME update is used when: ProducerMetadata is requested to update Metadata is requested to updateWithCurrentRequestVersion DefaultMetadataUpdater is requested to handleSuccessfulResponse","title":"Metadata"},{"location":"clients/Metadata/#metadata","text":"","title":"Metadata"},{"location":"clients/Metadata/#update","text":"void update ( int requestVersion , MetadataResponse response , boolean isPartialUpdate , long nowMs ) update ...FIXME update is used when: ProducerMetadata is requested to update Metadata is requested to updateWithCurrentRequestVersion DefaultMetadataUpdater is requested to handleSuccessfulResponse","title":" update"},{"location":"clients/MetadataUpdater/","text":"MetadataUpdater \u00b6 MetadataUpdater is...FIXME","title":"MetadataUpdater"},{"location":"clients/MetadataUpdater/#metadataupdater","text":"MetadataUpdater is...FIXME","title":"MetadataUpdater"},{"location":"clients/NetworkClient/","text":"NetworkClient \u00b6 NetworkClient is a KafkaClient . leastLoadedNode \u00b6 Node leastLoadedNode ( long now ) leastLoadedNode requests the MetadataUpdater for the nodes (in a non-blocking fashion). leastLoadedNode generates a random number to offset the first node to start checking node candidates from. leastLoadedNode finds three nodes: foundReady that is ready (perhaps with some in-flight requests ) foundConnecting with a connection already being established (using the ClusterConnectionStates registry) foundCanConnect that can be connected When a node is found that is ready and has no in-flight requests , leastLoadedNode prints out the following TRACE message to the logs and returns the node immediately: Found least loaded node [node] connected with no in-flight requests When a node candidate does not meet any of the above requirements, leastLoadedNode prints out the following TRACE message to the logs: Removing node [node] from least loaded node selection since it is neither ready for sending nor connecting leastLoadedNode prefers the foundReady node over the foundConnecting with the foundCanConnect as the last resort. When no node could be found, leastLoadedNode prints out the following TRACE message to the logs (and returns null ): Least loaded node selection failed to find an available node leastLoadedNode is part of the KafkaClient abstraction.","title":"NetworkClient"},{"location":"clients/NetworkClient/#networkclient","text":"NetworkClient is a KafkaClient .","title":"NetworkClient"},{"location":"clients/NetworkClient/#leastloadednode","text":"Node leastLoadedNode ( long now ) leastLoadedNode requests the MetadataUpdater for the nodes (in a non-blocking fashion). leastLoadedNode generates a random number to offset the first node to start checking node candidates from. leastLoadedNode finds three nodes: foundReady that is ready (perhaps with some in-flight requests ) foundConnecting with a connection already being established (using the ClusterConnectionStates registry) foundCanConnect that can be connected When a node is found that is ready and has no in-flight requests , leastLoadedNode prints out the following TRACE message to the logs and returns the node immediately: Found least loaded node [node] connected with no in-flight requests When a node candidate does not meet any of the above requirements, leastLoadedNode prints out the following TRACE message to the logs: Removing node [node] from least loaded node selection since it is neither ready for sending nor connecting leastLoadedNode prefers the foundReady node over the foundConnecting with the foundCanConnect as the last resort. When no node could be found, leastLoadedNode prints out the following TRACE message to the logs (and returns null ): Least loaded node selection failed to find an available node leastLoadedNode is part of the KafkaClient abstraction.","title":" leastLoadedNode"},{"location":"clients/NetworkClientUtils/","text":"NetworkClientUtils \u00b6","title":"NetworkClientUtils"},{"location":"clients/NetworkClientUtils/#networkclientutils","text":"","title":"NetworkClientUtils"},{"location":"clients/admin/","text":"Kafka Admin \u00b6","title":"Kafka Admin"},{"location":"clients/admin/#kafka-admin","text":"","title":"Kafka Admin"},{"location":"clients/admin/AdminClientRunnable/","text":"AdminClientRunnable \u00b6","title":"AdminClientRunnable"},{"location":"clients/admin/AdminClientRunnable/#adminclientrunnable","text":"","title":"AdminClientRunnable"},{"location":"clients/consumer/","text":"Kafka Consumers \u00b6 KafkaConsumer uses Fetcher to fetch records from a Kafka cluster. One could say that KafkaConsumer is a developer-oriented interface to Fetcher . KafkaConsumer is assigned a IsolationLevel based on isolation.level configuration property.","title":"Kafka Consumers"},{"location":"clients/consumer/#kafka-consumers","text":"KafkaConsumer uses Fetcher to fetch records from a Kafka cluster. One could say that KafkaConsumer is a developer-oriented interface to Fetcher . KafkaConsumer is assigned a IsolationLevel based on isolation.level configuration property.","title":"Kafka Consumers"},{"location":"clients/consumer/Consumer/","text":"Consumer \u00b6 Consumer<K, V> is an interface to KafkaConsumer for Kafka developers to use to consume records (with K keys and V values) from a Kafka cluster. Contract (Subset) \u00b6 enforceRebalance \u00b6 void enforceRebalance () groupMetadata \u00b6 ConsumerGroupMetadata groupMetadata () Subscribing to Topics \u00b6 void subscribe ( Collection < String > topics ) void subscribe ( Collection < String > topics , ConsumerRebalanceListener callback ) void subscribe ( Pattern pattern ) void subscribe ( Pattern pattern , ConsumerRebalanceListener callback ) Waking Up \u00b6 void wakeup ()","title":"Consumer"},{"location":"clients/consumer/Consumer/#consumer","text":"Consumer<K, V> is an interface to KafkaConsumer for Kafka developers to use to consume records (with K keys and V values) from a Kafka cluster.","title":"Consumer"},{"location":"clients/consumer/Consumer/#contract-subset","text":"","title":"Contract (Subset)"},{"location":"clients/consumer/Consumer/#enforcerebalance","text":"void enforceRebalance ()","title":" enforceRebalance"},{"location":"clients/consumer/Consumer/#groupmetadata","text":"ConsumerGroupMetadata groupMetadata ()","title":" groupMetadata"},{"location":"clients/consumer/Consumer/#subscribing-to-topics","text":"void subscribe ( Collection < String > topics ) void subscribe ( Collection < String > topics , ConsumerRebalanceListener callback ) void subscribe ( Pattern pattern ) void subscribe ( Pattern pattern , ConsumerRebalanceListener callback )","title":" Subscribing to Topics"},{"location":"clients/consumer/Consumer/#waking-up","text":"void wakeup ()","title":" Waking Up"},{"location":"clients/consumer/ConsumerConfig/","text":"ConsumerConfig \u00b6 check.crcs \u00b6 client.rack \u00b6 fetch.max.bytes \u00b6 fetch.max.wait.ms \u00b6 fetch.min.bytes \u00b6 group.id \u00b6 A unique identifier of the consumer group a consumer belongs to. Required if the consumer uses either the group management functionality by using Consumer.subscribe or the Kafka-based offset management strategy. Default: (undefined) isolation.level \u00b6 Controls how KafkaConsumer should read messages written transactionally Default: read_uncommitted Supported values: read_uncommitted - Consumer.poll() will only return transactional messages which have been committed (filtering out transactional messages which are not committed). read_committed - Consumer.poll() will return all messages, even transactional messages which have been not committed yet or even aborted. Non-transactional messages will be returned unconditionally in either mode. Messages will always be returned in offset order. Hence, in read_committed mode, consumer.poll() will only return messages up to the last stable offset (LSO), which is the one less than the offset of the first open transaction. In particular any messages appearing after messages belonging to ongoing transactions will be withheld until the relevant transaction has been completed. As a result, read_committed consumers will not be able to read up to the high watermark when there are in-flight transactions. Further, when in read_committed the seekToEnd method will return the last stable offset. kafka-console-consumer supports setting the property using --isolation-level option. max.partition.fetch.bytes \u00b6 max.poll.records \u00b6 request.timeout.ms \u00b6 retry.backoff.ms \u00b6","title":"ConsumerConfig"},{"location":"clients/consumer/ConsumerConfig/#consumerconfig","text":"","title":"ConsumerConfig"},{"location":"clients/consumer/ConsumerConfig/#checkcrcs","text":"","title":" check.crcs"},{"location":"clients/consumer/ConsumerConfig/#clientrack","text":"","title":" client.rack"},{"location":"clients/consumer/ConsumerConfig/#fetchmaxbytes","text":"","title":" fetch.max.bytes"},{"location":"clients/consumer/ConsumerConfig/#fetchmaxwaitms","text":"","title":" fetch.max.wait.ms"},{"location":"clients/consumer/ConsumerConfig/#fetchminbytes","text":"","title":" fetch.min.bytes"},{"location":"clients/consumer/ConsumerConfig/#groupid","text":"A unique identifier of the consumer group a consumer belongs to. Required if the consumer uses either the group management functionality by using Consumer.subscribe or the Kafka-based offset management strategy. Default: (undefined)","title":" group.id"},{"location":"clients/consumer/ConsumerConfig/#isolationlevel","text":"Controls how KafkaConsumer should read messages written transactionally Default: read_uncommitted Supported values: read_uncommitted - Consumer.poll() will only return transactional messages which have been committed (filtering out transactional messages which are not committed). read_committed - Consumer.poll() will return all messages, even transactional messages which have been not committed yet or even aborted. Non-transactional messages will be returned unconditionally in either mode. Messages will always be returned in offset order. Hence, in read_committed mode, consumer.poll() will only return messages up to the last stable offset (LSO), which is the one less than the offset of the first open transaction. In particular any messages appearing after messages belonging to ongoing transactions will be withheld until the relevant transaction has been completed. As a result, read_committed consumers will not be able to read up to the high watermark when there are in-flight transactions. Further, when in read_committed the seekToEnd method will return the last stable offset. kafka-console-consumer supports setting the property using --isolation-level option.","title":" isolation.level"},{"location":"clients/consumer/ConsumerConfig/#maxpartitionfetchbytes","text":"","title":" max.partition.fetch.bytes"},{"location":"clients/consumer/ConsumerConfig/#maxpollrecords","text":"","title":" max.poll.records"},{"location":"clients/consumer/ConsumerConfig/#requesttimeoutms","text":"","title":" request.timeout.ms"},{"location":"clients/consumer/ConsumerConfig/#retrybackoffms","text":"","title":" retry.backoff.ms"},{"location":"clients/consumer/ConsumerCoordinator/","text":"ConsumerCoordinator \u00b6 ConsumerCoordinator is...FIXME","title":"ConsumerCoordinator"},{"location":"clients/consumer/ConsumerCoordinator/#consumercoordinator","text":"ConsumerCoordinator is...FIXME","title":"ConsumerCoordinator"},{"location":"clients/consumer/ConsumerNetworkClient/","text":"ConsumerNetworkClient \u00b6","title":"ConsumerNetworkClient"},{"location":"clients/consumer/ConsumerNetworkClient/#consumernetworkclient","text":"","title":"ConsumerNetworkClient"},{"location":"clients/consumer/Fetcher/","text":"Fetcher \u00b6 Creating Instance \u00b6 Fetcher takes the following to be created: LogContext ConsumerNetworkClient fetch.min.bytes fetch.max.bytes fetch.max.wait.ms max.partition.fetch.bytes max.poll.records check.crcs client.rack Key Deserializer Value Deserializer ConsumerMetadata SubscriptionState Metrics FetcherMetricsRegistry Time retry.backoff.ms request.timeout.ms IsolationLevel ApiVersions Fetcher is created along with KafkaConsumer . IsolationLevel \u00b6 Fetcher is given an IsolationLevel when created (based on isolation.level configuration property) Fetcher uses the IsolationLevel for the following: sendFetches (and prepareFetchRequests ) fetchOffsetsByTimes fetchRecords sendListOffsetRequest sendFetches \u00b6 int sendFetches () sendFetches ...FIXME sendFetches is used when: KafkaConsumer is requested to poll (and pollForFetches ) prepareFetchRequests \u00b6 Map < Node , FetchSessionHandler . FetchRequestData > prepareFetchRequests () prepareFetchRequests ...FIXME Preferred Read Replica \u00b6 Node selectReadReplica ( TopicPartition partition , Node leaderReplica , long currentTimeMs ) selectReadReplica requests the SubscriptionState for the preferredReadReplica of the given TopicPartition . offsetsForTimes \u00b6 Map < TopicPartition , OffsetAndTimestamp > offsetsForTimes ( Map < TopicPartition , Long > timestampsToSearch , Timer timer ) offsetsForTimes ...FIXME offsetsForTimes is used when: KafkaConsumer is requested to offsetsForTimes beginningOffsets \u00b6 Map < TopicPartition , Long > beginningOffsets ( Collection < TopicPartition > partitions , Timer timer ) beginningOffsets ...FIXME beginningOffsets is used when: KafkaConsumer is requested to beginningOffsets endOffsets \u00b6 Map < TopicPartition , Long > endOffsets ( Collection < TopicPartition > partitions , Timer timer ) endOffsets ...FIXME endOffsets is used when: KafkaConsumer is requested to endOffsets and currentLag beginningOrEndOffset \u00b6 Map < TopicPartition , Long > beginningOrEndOffset ( Collection < TopicPartition > partitions , long timestamp , Timer timer ) beginningOrEndOffset ...FIXME beginningOrEndOffset is used when: Fetcher is requested to beginningOffsets and endOffsets fetchOffsetsByTimes \u00b6 ListOffsetResult fetchOffsetsByTimes ( Map < TopicPartition , Long > timestampsToSearch , Timer timer , boolean requireTimestamps ) fetchOffsetsByTimes ...FIXME fetchOffsetsByTimes is used when: Fetcher is requested to offsetsForTimes and beginningOrEndOffset sendListOffsetsRequests \u00b6 RequestFuture < ListOffsetResult > sendListOffsetsRequests ( Map < TopicPartition , Long > timestampsToSearch , boolean requireTimestamps ) sendListOffsetsRequests ...FIXME Fetched Records \u00b6 Map < TopicPartition , List < ConsumerRecord < K , V >>> fetchedRecords () fetchedRecords returns up to max.poll.records number of records from the CompletedFetch Queue . For nextInLineFetch unintialized or consumed already, fetchedRecords takes a peek at a CompletedFetch collection of records (in the CompletedFetch Queue ). If uninitialized, fetchedRecords initializeCompletedFetch with the records. fetchedRecords saves the CompletedFetch records to the nextInLineFetch internal registry. fetchedRecords takes the CompletedFetch collection of records out (off the CompletedFetch Queue ). For the partition of the nextInLineFetch collection of records paused , fetchedRecords prints out the following DEBUG message to the logs and null s the nextInLineFetch registry. Skipping fetching records for assigned partition [p] because it is paused For all the other cases, fetchedRecords fetches the records out of the nextInLineFetch collection of records (up to the number of records left to fetch). In the end, fetchedRecords returns the ConsumerRecord s per TopicPartition (out of the CompletedFetch Queue ). fetchedRecords is used when: KafkaConsumer is requested to pollForFetches fetchRecords \u00b6 List < ConsumerRecord < K , V >> fetchRecords ( CompletedFetch completedFetch , int maxRecords ) fetchRecords ...FIXME initializeCompletedFetch \u00b6 CompletedFetch initializeCompletedFetch ( CompletedFetch nextCompletedFetch ) initializeCompletedFetch ...FIXME resetOffsetsIfNeeded \u00b6 void resetOffsetsIfNeeded () resetOffsetsIfNeeded ...FIXME resetOffsetsIfNeeded is used when: KafkaConsumer is requested to updateFetchPositions resetOffsetsAsync \u00b6 void resetOffsetsAsync ( Map < TopicPartition , Long > partitionResetTimestamps ) resetOffsetsAsync ...FIXME sendListOffsetRequest \u00b6 RequestFuture < ListOffsetResult > sendListOffsetRequest ( Node node , Map < TopicPartition , ListOffsetsPartition > timestampsToSearch , boolean requireTimestamp ) sendListOffsetRequest ...FIXME sendListOffsetRequest is used when: Fetcher is requested to resetOffsetsIfNeeded (via resetOffsetsAsync ) and fetchOffsetsByTimes (via sendListOffsetsRequests ) clearBufferedDataForUnassignedTopics \u00b6 void clearBufferedDataForUnassignedTopics ( Collection < String > assignedTopics ) clearBufferedDataForUnassignedTopics ...FIXME clearBufferedDataForUnassignedTopics is used when: KafkaConsumer is requested to subscribe CompletedFetch Queue \u00b6 Fetcher creates an empty ConcurrentLinkedQueue ( Java ) of CompletedFetch es when created . New CompletedFetch es (one per partition) are added to the queue in sendFetches (on a successful receipt of response from a Kafka cluster).","title":"Fetcher"},{"location":"clients/consumer/Fetcher/#fetcher","text":"","title":"Fetcher"},{"location":"clients/consumer/Fetcher/#creating-instance","text":"Fetcher takes the following to be created: LogContext ConsumerNetworkClient fetch.min.bytes fetch.max.bytes fetch.max.wait.ms max.partition.fetch.bytes max.poll.records check.crcs client.rack Key Deserializer Value Deserializer ConsumerMetadata SubscriptionState Metrics FetcherMetricsRegistry Time retry.backoff.ms request.timeout.ms IsolationLevel ApiVersions Fetcher is created along with KafkaConsumer .","title":"Creating Instance"},{"location":"clients/consumer/Fetcher/#isolationlevel","text":"Fetcher is given an IsolationLevel when created (based on isolation.level configuration property) Fetcher uses the IsolationLevel for the following: sendFetches (and prepareFetchRequests ) fetchOffsetsByTimes fetchRecords sendListOffsetRequest","title":" IsolationLevel"},{"location":"clients/consumer/Fetcher/#sendfetches","text":"int sendFetches () sendFetches ...FIXME sendFetches is used when: KafkaConsumer is requested to poll (and pollForFetches )","title":" sendFetches"},{"location":"clients/consumer/Fetcher/#preparefetchrequests","text":"Map < Node , FetchSessionHandler . FetchRequestData > prepareFetchRequests () prepareFetchRequests ...FIXME","title":" prepareFetchRequests"},{"location":"clients/consumer/Fetcher/#preferred-read-replica","text":"Node selectReadReplica ( TopicPartition partition , Node leaderReplica , long currentTimeMs ) selectReadReplica requests the SubscriptionState for the preferredReadReplica of the given TopicPartition .","title":" Preferred Read Replica"},{"location":"clients/consumer/Fetcher/#offsetsfortimes","text":"Map < TopicPartition , OffsetAndTimestamp > offsetsForTimes ( Map < TopicPartition , Long > timestampsToSearch , Timer timer ) offsetsForTimes ...FIXME offsetsForTimes is used when: KafkaConsumer is requested to offsetsForTimes","title":" offsetsForTimes"},{"location":"clients/consumer/Fetcher/#beginningoffsets","text":"Map < TopicPartition , Long > beginningOffsets ( Collection < TopicPartition > partitions , Timer timer ) beginningOffsets ...FIXME beginningOffsets is used when: KafkaConsumer is requested to beginningOffsets","title":" beginningOffsets"},{"location":"clients/consumer/Fetcher/#endoffsets","text":"Map < TopicPartition , Long > endOffsets ( Collection < TopicPartition > partitions , Timer timer ) endOffsets ...FIXME endOffsets is used when: KafkaConsumer is requested to endOffsets and currentLag","title":" endOffsets"},{"location":"clients/consumer/Fetcher/#beginningorendoffset","text":"Map < TopicPartition , Long > beginningOrEndOffset ( Collection < TopicPartition > partitions , long timestamp , Timer timer ) beginningOrEndOffset ...FIXME beginningOrEndOffset is used when: Fetcher is requested to beginningOffsets and endOffsets","title":" beginningOrEndOffset"},{"location":"clients/consumer/Fetcher/#fetchoffsetsbytimes","text":"ListOffsetResult fetchOffsetsByTimes ( Map < TopicPartition , Long > timestampsToSearch , Timer timer , boolean requireTimestamps ) fetchOffsetsByTimes ...FIXME fetchOffsetsByTimes is used when: Fetcher is requested to offsetsForTimes and beginningOrEndOffset","title":" fetchOffsetsByTimes"},{"location":"clients/consumer/Fetcher/#sendlistoffsetsrequests","text":"RequestFuture < ListOffsetResult > sendListOffsetsRequests ( Map < TopicPartition , Long > timestampsToSearch , boolean requireTimestamps ) sendListOffsetsRequests ...FIXME","title":" sendListOffsetsRequests"},{"location":"clients/consumer/Fetcher/#fetched-records","text":"Map < TopicPartition , List < ConsumerRecord < K , V >>> fetchedRecords () fetchedRecords returns up to max.poll.records number of records from the CompletedFetch Queue . For nextInLineFetch unintialized or consumed already, fetchedRecords takes a peek at a CompletedFetch collection of records (in the CompletedFetch Queue ). If uninitialized, fetchedRecords initializeCompletedFetch with the records. fetchedRecords saves the CompletedFetch records to the nextInLineFetch internal registry. fetchedRecords takes the CompletedFetch collection of records out (off the CompletedFetch Queue ). For the partition of the nextInLineFetch collection of records paused , fetchedRecords prints out the following DEBUG message to the logs and null s the nextInLineFetch registry. Skipping fetching records for assigned partition [p] because it is paused For all the other cases, fetchedRecords fetches the records out of the nextInLineFetch collection of records (up to the number of records left to fetch). In the end, fetchedRecords returns the ConsumerRecord s per TopicPartition (out of the CompletedFetch Queue ). fetchedRecords is used when: KafkaConsumer is requested to pollForFetches","title":" Fetched Records"},{"location":"clients/consumer/Fetcher/#fetchrecords","text":"List < ConsumerRecord < K , V >> fetchRecords ( CompletedFetch completedFetch , int maxRecords ) fetchRecords ...FIXME","title":" fetchRecords"},{"location":"clients/consumer/Fetcher/#initializecompletedfetch","text":"CompletedFetch initializeCompletedFetch ( CompletedFetch nextCompletedFetch ) initializeCompletedFetch ...FIXME","title":" initializeCompletedFetch"},{"location":"clients/consumer/Fetcher/#resetoffsetsifneeded","text":"void resetOffsetsIfNeeded () resetOffsetsIfNeeded ...FIXME resetOffsetsIfNeeded is used when: KafkaConsumer is requested to updateFetchPositions","title":" resetOffsetsIfNeeded"},{"location":"clients/consumer/Fetcher/#resetoffsetsasync","text":"void resetOffsetsAsync ( Map < TopicPartition , Long > partitionResetTimestamps ) resetOffsetsAsync ...FIXME","title":" resetOffsetsAsync"},{"location":"clients/consumer/Fetcher/#sendlistoffsetrequest","text":"RequestFuture < ListOffsetResult > sendListOffsetRequest ( Node node , Map < TopicPartition , ListOffsetsPartition > timestampsToSearch , boolean requireTimestamp ) sendListOffsetRequest ...FIXME sendListOffsetRequest is used when: Fetcher is requested to resetOffsetsIfNeeded (via resetOffsetsAsync ) and fetchOffsetsByTimes (via sendListOffsetsRequests )","title":" sendListOffsetRequest"},{"location":"clients/consumer/Fetcher/#clearbuffereddataforunassignedtopics","text":"void clearBufferedDataForUnassignedTopics ( Collection < String > assignedTopics ) clearBufferedDataForUnassignedTopics ...FIXME clearBufferedDataForUnassignedTopics is used when: KafkaConsumer is requested to subscribe","title":" clearBufferedDataForUnassignedTopics"},{"location":"clients/consumer/Fetcher/#completedfetch-queue","text":"Fetcher creates an empty ConcurrentLinkedQueue ( Java ) of CompletedFetch es when created . New CompletedFetch es (one per partition) are added to the queue in sendFetches (on a successful receipt of response from a Kafka cluster).","title":" CompletedFetch Queue"},{"location":"clients/consumer/KafkaConsumer/","text":"KafkaConsumer \u00b6 KafkaConsumer is a Consumer . Creating Instance \u00b6 KafkaConsumer takes the following to be created: Configuration ( ConsumerConfig or Map<String, Object> or Properties ) Deserializer<K> Deserializer<V> Group ID \u00b6 KafkaConsumer can be given a group ID using group.id (indirectly in the config ) configuration property when created . IsolationLevel \u00b6 KafkaConsumer can be given an IsolationLevel using isolation.level configuration property (indirectly in the config ) when created . KafkaConsumer uses the IsolationLevel for the following: Creating a Fetcher (when created ) currentLag Fetcher \u00b6 KafkaConsumer creates a Fetcher when created . ConsumerCoordinator \u00b6 KafkaConsumer creates a ConsumerCoordinator when created with the group.id specified. enforceRebalance \u00b6 void enforceRebalance () enforceRebalance requests the ConsumerCoordinator to requestRejoin with the following reason: rebalance enforced by user enforceRebalance is part of the Consumer abstraction. groupMetadata \u00b6 ConsumerGroupMetadata groupMetadata () groupMetadata ...FIXME groupMetadata is part of the Consumer abstraction. Polling for Records \u00b6 ConsumerRecords < K , V > poll ( Duration timeout ) // (1) ConsumerRecords < K , V > poll ( Timer timer , boolean includeMetadataInTimeout ) // (2) Uses includeMetadataInTimeout enabled ( true ) A private method poll ...FIXME poll is part of the Consumer abstraction. Polling for Fetches \u00b6 Map < TopicPartition , List < ConsumerRecord < K , V >>> pollForFetches ( Timer timer ) pollForFetches requests the Fetcher for fetched records and returns them immediately if available. Otherwise, pollForFetches requests the Fetcher to sendFetches . pollForFetches prints out the following TRACE message to the logs: Polling for fetches with timeout [pollTimeout] pollForFetches requests the ConsumerNetworkClient to poll (with the pollTimeout until it expires or the Fetcher has some available fetches ready). In the end, pollForFetches requests the Fetcher for the fetched records again. Subscribing to Topics \u00b6 void subscribe ( Collection < String > topics ) // (1) void subscribe ( Collection < String > topics , ConsumerRebalanceListener listener ) void subscribe ( Pattern pattern ) // (2) void subscribe ( Pattern pattern , ConsumerRebalanceListener callback ) Uses NoOpConsumerRebalanceListener Uses NoOpConsumerRebalanceListener subscribe ...FIXME subscribe is part of the Consumer abstraction. Waking Up \u00b6 void wakeup () wakeup ...FIXME wakeup is part of the Consumer abstraction.","title":"KafkaConsumer"},{"location":"clients/consumer/KafkaConsumer/#kafkaconsumer","text":"KafkaConsumer is a Consumer .","title":"KafkaConsumer"},{"location":"clients/consumer/KafkaConsumer/#creating-instance","text":"KafkaConsumer takes the following to be created: Configuration ( ConsumerConfig or Map<String, Object> or Properties ) Deserializer<K> Deserializer<V>","title":"Creating Instance"},{"location":"clients/consumer/KafkaConsumer/#group-id","text":"KafkaConsumer can be given a group ID using group.id (indirectly in the config ) configuration property when created .","title":" Group ID"},{"location":"clients/consumer/KafkaConsumer/#isolationlevel","text":"KafkaConsumer can be given an IsolationLevel using isolation.level configuration property (indirectly in the config ) when created . KafkaConsumer uses the IsolationLevel for the following: Creating a Fetcher (when created ) currentLag","title":" IsolationLevel"},{"location":"clients/consumer/KafkaConsumer/#fetcher","text":"KafkaConsumer creates a Fetcher when created .","title":" Fetcher"},{"location":"clients/consumer/KafkaConsumer/#consumercoordinator","text":"KafkaConsumer creates a ConsumerCoordinator when created with the group.id specified.","title":" ConsumerCoordinator"},{"location":"clients/consumer/KafkaConsumer/#enforcerebalance","text":"void enforceRebalance () enforceRebalance requests the ConsumerCoordinator to requestRejoin with the following reason: rebalance enforced by user enforceRebalance is part of the Consumer abstraction.","title":" enforceRebalance"},{"location":"clients/consumer/KafkaConsumer/#groupmetadata","text":"ConsumerGroupMetadata groupMetadata () groupMetadata ...FIXME groupMetadata is part of the Consumer abstraction.","title":" groupMetadata"},{"location":"clients/consumer/KafkaConsumer/#polling-for-records","text":"ConsumerRecords < K , V > poll ( Duration timeout ) // (1) ConsumerRecords < K , V > poll ( Timer timer , boolean includeMetadataInTimeout ) // (2) Uses includeMetadataInTimeout enabled ( true ) A private method poll ...FIXME poll is part of the Consumer abstraction.","title":" Polling for Records"},{"location":"clients/consumer/KafkaConsumer/#polling-for-fetches","text":"Map < TopicPartition , List < ConsumerRecord < K , V >>> pollForFetches ( Timer timer ) pollForFetches requests the Fetcher for fetched records and returns them immediately if available. Otherwise, pollForFetches requests the Fetcher to sendFetches . pollForFetches prints out the following TRACE message to the logs: Polling for fetches with timeout [pollTimeout] pollForFetches requests the ConsumerNetworkClient to poll (with the pollTimeout until it expires or the Fetcher has some available fetches ready). In the end, pollForFetches requests the Fetcher for the fetched records again.","title":" Polling for Fetches"},{"location":"clients/consumer/KafkaConsumer/#subscribing-to-topics","text":"void subscribe ( Collection < String > topics ) // (1) void subscribe ( Collection < String > topics , ConsumerRebalanceListener listener ) void subscribe ( Pattern pattern ) // (2) void subscribe ( Pattern pattern , ConsumerRebalanceListener callback ) Uses NoOpConsumerRebalanceListener Uses NoOpConsumerRebalanceListener subscribe ...FIXME subscribe is part of the Consumer abstraction.","title":" Subscribing to Topics"},{"location":"clients/consumer/KafkaConsumer/#waking-up","text":"void wakeup () wakeup ...FIXME wakeup is part of the Consumer abstraction.","title":" Waking Up"},{"location":"clients/consumer/SubscriptionState/","text":"SubscriptionState \u00b6 Preferred Read Replica \u00b6 preferredReadReplica \u00b6 Optional < Integer > preferredReadReplica ( TopicPartition tp , long timeMs ) preferredReadReplica looks up the state of the given TopicPartition and, if found, requests it for the preferredReadReplica . Otherwise, preferredReadReplica returns an undefined preferred read replica. preferredReadReplica is used when: Fetcher is requested to selectReadReplica updatePreferredReadReplica \u00b6 void updatePreferredReadReplica ( TopicPartition tp , int preferredReadReplicaId , LongSupplier timeMs ) updatePreferredReadReplica looks up the state of the given TopicPartition and requests it to updatePreferredReadReplica . updatePreferredReadReplica is used when: Fetcher is requested to initializeCompletedFetch clearPreferredReadReplica \u00b6 Optional < Integer > clearPreferredReadReplica ( TopicPartition tp ) clearPreferredReadReplica looks up the state of the given TopicPartition and requests it to clearPreferredReadReplica . clearPreferredReadReplica is used when: Fetcher is requested to selectReadReplica and initializeCompletedFetch","title":"SubscriptionState"},{"location":"clients/consumer/SubscriptionState/#subscriptionstate","text":"","title":"SubscriptionState"},{"location":"clients/consumer/SubscriptionState/#preferred-read-replica","text":"","title":"Preferred Read Replica"},{"location":"clients/consumer/SubscriptionState/#preferredreadreplica","text":"Optional < Integer > preferredReadReplica ( TopicPartition tp , long timeMs ) preferredReadReplica looks up the state of the given TopicPartition and, if found, requests it for the preferredReadReplica . Otherwise, preferredReadReplica returns an undefined preferred read replica. preferredReadReplica is used when: Fetcher is requested to selectReadReplica","title":" preferredReadReplica"},{"location":"clients/consumer/SubscriptionState/#updatepreferredreadreplica","text":"void updatePreferredReadReplica ( TopicPartition tp , int preferredReadReplicaId , LongSupplier timeMs ) updatePreferredReadReplica looks up the state of the given TopicPartition and requests it to updatePreferredReadReplica . updatePreferredReadReplica is used when: Fetcher is requested to initializeCompletedFetch","title":" updatePreferredReadReplica"},{"location":"clients/consumer/SubscriptionState/#clearpreferredreadreplica","text":"Optional < Integer > clearPreferredReadReplica ( TopicPartition tp ) clearPreferredReadReplica looks up the state of the given TopicPartition and requests it to clearPreferredReadReplica . clearPreferredReadReplica is used when: Fetcher is requested to selectReadReplica and initializeCompletedFetch","title":" clearPreferredReadReplica"},{"location":"clients/consumer/TopicPartitionState/","text":"TopicPartitionState \u00b6 preferredReadReplica \u00b6 Integer preferredReadReplica TopicPartitionState manages the preferred read replica (of a TopicPartition ) for a specified amount of time (until expires or is cleared out). preferredReadReplica is used when: SubscriptionState is requested for the preferredReadReplica , updatePreferredReadReplica and clearPreferredReadReplica","title":"TopicPartitionState"},{"location":"clients/consumer/TopicPartitionState/#topicpartitionstate","text":"","title":"TopicPartitionState"},{"location":"clients/consumer/TopicPartitionState/#preferredreadreplica","text":"Integer preferredReadReplica TopicPartitionState manages the preferred read replica (of a TopicPartition ) for a specified amount of time (until expires or is cleared out). preferredReadReplica is used when: SubscriptionState is requested for the preferredReadReplica , updatePreferredReadReplica and clearPreferredReadReplica","title":" preferredReadReplica"},{"location":"clients/consumer/preferred-read-replica/","text":"Preferred Read Replica \u00b6 Preferred Read Replica is the broker ID of one of the in-sync replicas of a partition for Kafka Consumer to read records from.","title":"Preferred Read Replica"},{"location":"clients/consumer/preferred-read-replica/#preferred-read-replica","text":"Preferred Read Replica is the broker ID of one of the in-sync replicas of a partition for Kafka Consumer to read records from.","title":"Preferred Read Replica"},{"location":"clients/producer/","text":"Kafka Producers \u00b6 KafkaProducer uses Sender to send records to a Kafka cluster. KafkaProducer can be transactional or idempotent (and associated with a TransactionManager ).","title":"Kafka Producers"},{"location":"clients/producer/#kafka-producers","text":"KafkaProducer uses Sender to send records to a Kafka cluster. KafkaProducer can be transactional or idempotent (and associated with a TransactionManager ).","title":"Kafka Producers"},{"location":"clients/producer/BufferPool/","text":"BufferPool \u00b6 BufferPool is...FIXME","title":"BufferPool"},{"location":"clients/producer/BufferPool/#bufferpool","text":"BufferPool is...FIXME","title":"BufferPool"},{"location":"clients/producer/Callback/","text":"Callback \u00b6 Callback is...FIXME","title":"Callback"},{"location":"clients/producer/Callback/#callback","text":"Callback is...FIXME","title":"Callback"},{"location":"clients/producer/DefaultPartitioner/","text":"DefaultPartitioner \u00b6 DefaultPartitioner is a Partitioner . Demo \u00b6 import org.apache.kafka.clients.producer.internals.DefaultPartitioner val partitioner = new DefaultPartitioner val keyBytes = \"hello\".getBytes val numPartitions = 3 val p = partitioner.partition(null, null, keyBytes, null, null, null, numPartitions) println(p) The following snippet should generate the same partition value (since it is exactly how DefaultPartitioner does it). import org.apache.kafka.common.utils.Utils val keyBytes = \"hello\".getBytes val numPartitions = 3 val p = Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions println(p)","title":"DefaultPartitioner"},{"location":"clients/producer/DefaultPartitioner/#defaultpartitioner","text":"DefaultPartitioner is a Partitioner .","title":"DefaultPartitioner"},{"location":"clients/producer/DefaultPartitioner/#demo","text":"import org.apache.kafka.clients.producer.internals.DefaultPartitioner val partitioner = new DefaultPartitioner val keyBytes = \"hello\".getBytes val numPartitions = 3 val p = partitioner.partition(null, null, keyBytes, null, null, null, numPartitions) println(p) The following snippet should generate the same partition value (since it is exactly how DefaultPartitioner does it). import org.apache.kafka.common.utils.Utils val keyBytes = \"hello\".getBytes val numPartitions = 3 val p = Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions println(p)","title":"Demo"},{"location":"clients/producer/KafkaProducer/","text":"KafkaProducer \u00b6 KafkaProducer<K, V> is a concrete Producer . Creating Instance \u00b6 KafkaProducer takes the following to be created: ProducerConfig Key Serializer<K> Value Serializer<V> ProducerMetadata KafkaClient ProducerInterceptor<K, V> s Time configureTransactionState \u00b6 TransactionManager configureTransactionState ( ProducerConfig config , LogContext logContext ) configureTransactionState creates a new TransactionManager or returns null . configureTransactionState checks whether the following configuration properties are specified in the given ProducerConfig : enable.idempotence transactional.id With transactional.id specified, configureTransactionState turns the enable.idempotence on and prints out the following INFO message to the logs: Overriding the default [enable.idempotence] to true since transactional.id is specified. With idempotence enabled , configureTransactionState creates a TransactionManager with the values of the following configuration properties: transactional.id transaction.timeout.ms retry.backoff.ms When the TransactionManager is transactional , configureTransactionState prints out the following INFO message to the logs: Instantiated a transactional producer. Otherwise, configureTransactionState prints out the following INFO message to the logs: Instantiated an idempotent producer. In the end, configureTransactionState returns the TransactionManager or null . newSender \u00b6 Sender newSender ( LogContext logContext , KafkaClient kafkaClient , ProducerMetadata metadata ) newSender ...FIXME configureInflightRequests \u00b6 int configureInflightRequests ( ProducerConfig config ) configureInflightRequests gives the value of the max.in.flight.requests.per.connection (in the given ProducerConfig ). configureInflightRequests throws a ConfigException when the idempotence is enabled and the value of the max.in.flight.requests.per.connection is above 5: Must set max.in.flight.requests.per.connection to at most 5 to use the idempotent producer. configureAcks \u00b6 short configureAcks ( ProducerConfig config , Logger log ) configureAcks returns the value of acks configuration property (in the given ProducerConfig ). With idempotenceEnabled , configureAcks prints out the following INFO message to the logs when there is no acks configuration property defined: Overriding the default [acks] to all since idempotence is enabled. With idempotenceEnabled and the acks not -1 , configureAcks throws a ConfigException : Must set acks to all in order to use the idempotent producer. Otherwise we cannot guarantee idempotence. configureDeliveryTimeout \u00b6 int configureDeliveryTimeout ( ProducerConfig config , Logger log ) configureDeliveryTimeout ...FIXME TransactionManager \u00b6 KafkaProducer may create a TransactionManager when created (with idempotenceEnabled ). TransactionManager is used to create the following: RecordAccumulator Sender KafkaProducer uses the TransactionManager for the following transactional methods: abortTransaction beginTransaction commitTransaction initTransactions sendOffsetsToTransaction doSend throwIfNoTransactionManager \u00b6 KafkaProducer throws an IllegalStateException for the transactional methods but TransactionManager is not configured. Cannot use transactional methods without enabling transactions by setting the transactional.id configuration property Sender Thread \u00b6 KafkaProducer creates a Sender when created . Sender is immediately started as a daemon thread with the following name (using the clientId ): kafka-producer-network-thread | [clientId] KafkaProducer is actually considered open (and usable) as long as the Sender is running . KafkaProducer simply requests the Sender to wake up for the following: initTransactions sendOffsetsToTransaction commitTransaction abortTransaction doSend waitOnMetadata flush RecordAccumulator \u00b6 KafkaProducer creates a RecordAccumulator when created . This RecordAccumulator is used for the following: Create a Sender append when doSend beginFlush when flush max.block.ms \u00b6 KafkaProducer uses max.block.ms configuration property. Transactional Methods \u00b6 abortTransaction \u00b6 void abortTransaction () abortTransaction prints out the following INFO message to the logs: Aborting incomplete transaction abortTransaction ...FIXME abortTransaction is part of the Producer abstraction. beginTransaction \u00b6 void beginTransaction () beginTransaction requests the TransactionManager to beginTransaction . beginTransaction is part of the Producer abstraction. initTransactions \u00b6 void initTransactions () initTransactions requests the TransactionManager to initializeTransactions and requests the Sender to wakeup . In the end, initTransactions waits max.block.ms until transaction initialization is completed (successfully or not). initTransactions is part of the Producer abstraction. sendOffsetsToTransaction \u00b6 void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , ConsumerGroupMetadata groupMetadata ) sendOffsetsToTransaction requests the TransactionManager to sendOffsetsToTransaction and requests the Sender to wakeup . In the end, sendOffsetsToTransaction waits max.block.ms for the send to be completed (successfully or not). sendOffsetsToTransaction is part of the Producer abstraction. Sending Record \u00b6 Future < RecordMetadata > send ( ProducerRecord < K , V > record ) // (1) Future < RecordMetadata > send ( ProducerRecord < K , V > record , Callback callback ) Uses uninitialized Callback ( null ) send requests the interceptors to onSend with the given record (possibly modifying it) followed by doSend . send is part of the Producer abstraction. doSend \u00b6 Future < RecordMetadata > doSend ( ProducerRecord < K , V > record , Callback callback ) doSend waitOnMetadata for the topic and partition of the given record. doSend requests the key Serializer to serialize the record (passing in the topic, the headers and the key of the record). doSend requests the value Serializer to serialize the record (passing in the topic, the headers and the value of the record). doSend determines the partition for the record. doSend ensureValidRecordSize for the record (upper bound estimate). doSend prints out the following TRACE message to the logs: Attempting to append record [r] with callback [c] to topic [t] partition [p] doSend requests the RecordAccumulator to append the record (with the abortOnNewBatch flag enabled). When aborted for a new batch, doSend ...FIXME (repeats the steps)...and prints out the following TRACE message to the logs: Retrying append due to new batch creation for topic [t] partition [p]. The old partition was [prev] When transactional , doSend requests the TransactionManager to maybeAddPartitionToTransaction . For batchIsFull or a new batch created, doSend prints out the following TRACE message to the logs and requests the Sender to wakeup . Waking up the sender since topic [t] partition [p] is either full or getting a new batch partition \u00b6 int partition ( ProducerRecord < K , V > record , byte [] serializedKey , byte [] serializedValue , Cluster cluster ) partition is the partition (of the given ProducerRecord ) if defined or requests the Partitioner for the partition . Flushing \u00b6 void flush () flush requests the RecordAccumulator to beginFlush . flush requests the Sender to wakeup . flush requests the RecordAccumulator to awaitFlushCompletion . flush is part of the Producer abstraction. waitOnMetadata \u00b6 ClusterAndWaitTime waitOnMetadata ( String topic , Integer partition , long nowMs , long maxWaitMs ) waitOnMetadata requests the ProducerMetadata for the current cluster info . waitOnMetadata ...FIXME waitOnMetadata is used when: KafkaProducer is requested to doSend and partitionsFor Demo \u00b6 // Necessary imports import org.apache.kafka.clients.producer.KafkaProducer import org.apache.kafka.clients.producer.ProducerConfig import org.apache.kafka.common.serialization.StringSerializer // Creating a KafkaProducer import java.util.Properties val props = new Properties() props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \":9092\") val producer = new KafkaProducer[String, String](props) // Creating a record to be sent import org.apache.kafka.clients.producer.ProducerRecord val r = new ProducerRecord[String, String](\"0\", \"this is a message\") // Sending the record (with no Callback) import java.util.concurrent.Future import org.apache.kafka.clients.producer.RecordMetadata val metadataF: Future[RecordMetadata] = producer.send(r) Logging \u00b6 Enable ALL logging level for org.apache.kafka.clients.producer.KafkaProducer logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL Refer to Logging .","title":"KafkaProducer"},{"location":"clients/producer/KafkaProducer/#kafkaproducer","text":"KafkaProducer<K, V> is a concrete Producer .","title":"KafkaProducer"},{"location":"clients/producer/KafkaProducer/#creating-instance","text":"KafkaProducer takes the following to be created: ProducerConfig Key Serializer<K> Value Serializer<V> ProducerMetadata KafkaClient ProducerInterceptor<K, V> s Time","title":"Creating Instance"},{"location":"clients/producer/KafkaProducer/#configuretransactionstate","text":"TransactionManager configureTransactionState ( ProducerConfig config , LogContext logContext ) configureTransactionState creates a new TransactionManager or returns null . configureTransactionState checks whether the following configuration properties are specified in the given ProducerConfig : enable.idempotence transactional.id With transactional.id specified, configureTransactionState turns the enable.idempotence on and prints out the following INFO message to the logs: Overriding the default [enable.idempotence] to true since transactional.id is specified. With idempotence enabled , configureTransactionState creates a TransactionManager with the values of the following configuration properties: transactional.id transaction.timeout.ms retry.backoff.ms When the TransactionManager is transactional , configureTransactionState prints out the following INFO message to the logs: Instantiated a transactional producer. Otherwise, configureTransactionState prints out the following INFO message to the logs: Instantiated an idempotent producer. In the end, configureTransactionState returns the TransactionManager or null .","title":" configureTransactionState"},{"location":"clients/producer/KafkaProducer/#newsender","text":"Sender newSender ( LogContext logContext , KafkaClient kafkaClient , ProducerMetadata metadata ) newSender ...FIXME","title":" newSender"},{"location":"clients/producer/KafkaProducer/#configureinflightrequests","text":"int configureInflightRequests ( ProducerConfig config ) configureInflightRequests gives the value of the max.in.flight.requests.per.connection (in the given ProducerConfig ). configureInflightRequests throws a ConfigException when the idempotence is enabled and the value of the max.in.flight.requests.per.connection is above 5: Must set max.in.flight.requests.per.connection to at most 5 to use the idempotent producer.","title":" configureInflightRequests"},{"location":"clients/producer/KafkaProducer/#configureacks","text":"short configureAcks ( ProducerConfig config , Logger log ) configureAcks returns the value of acks configuration property (in the given ProducerConfig ). With idempotenceEnabled , configureAcks prints out the following INFO message to the logs when there is no acks configuration property defined: Overriding the default [acks] to all since idempotence is enabled. With idempotenceEnabled and the acks not -1 , configureAcks throws a ConfigException : Must set acks to all in order to use the idempotent producer. Otherwise we cannot guarantee idempotence.","title":" configureAcks"},{"location":"clients/producer/KafkaProducer/#configuredeliverytimeout","text":"int configureDeliveryTimeout ( ProducerConfig config , Logger log ) configureDeliveryTimeout ...FIXME","title":" configureDeliveryTimeout"},{"location":"clients/producer/KafkaProducer/#transactionmanager","text":"KafkaProducer may create a TransactionManager when created (with idempotenceEnabled ). TransactionManager is used to create the following: RecordAccumulator Sender KafkaProducer uses the TransactionManager for the following transactional methods: abortTransaction beginTransaction commitTransaction initTransactions sendOffsetsToTransaction doSend","title":" TransactionManager"},{"location":"clients/producer/KafkaProducer/#throwifnotransactionmanager","text":"KafkaProducer throws an IllegalStateException for the transactional methods but TransactionManager is not configured. Cannot use transactional methods without enabling transactions by setting the transactional.id configuration property","title":" throwIfNoTransactionManager"},{"location":"clients/producer/KafkaProducer/#sender-thread","text":"KafkaProducer creates a Sender when created . Sender is immediately started as a daemon thread with the following name (using the clientId ): kafka-producer-network-thread | [clientId] KafkaProducer is actually considered open (and usable) as long as the Sender is running . KafkaProducer simply requests the Sender to wake up for the following: initTransactions sendOffsetsToTransaction commitTransaction abortTransaction doSend waitOnMetadata flush","title":" Sender Thread"},{"location":"clients/producer/KafkaProducer/#recordaccumulator","text":"KafkaProducer creates a RecordAccumulator when created . This RecordAccumulator is used for the following: Create a Sender append when doSend beginFlush when flush","title":" RecordAccumulator"},{"location":"clients/producer/KafkaProducer/#maxblockms","text":"KafkaProducer uses max.block.ms configuration property.","title":" max.block.ms"},{"location":"clients/producer/KafkaProducer/#transactional-methods","text":"","title":"Transactional Methods"},{"location":"clients/producer/KafkaProducer/#aborttransaction","text":"void abortTransaction () abortTransaction prints out the following INFO message to the logs: Aborting incomplete transaction abortTransaction ...FIXME abortTransaction is part of the Producer abstraction.","title":" abortTransaction"},{"location":"clients/producer/KafkaProducer/#begintransaction","text":"void beginTransaction () beginTransaction requests the TransactionManager to beginTransaction . beginTransaction is part of the Producer abstraction.","title":" beginTransaction"},{"location":"clients/producer/KafkaProducer/#inittransactions","text":"void initTransactions () initTransactions requests the TransactionManager to initializeTransactions and requests the Sender to wakeup . In the end, initTransactions waits max.block.ms until transaction initialization is completed (successfully or not). initTransactions is part of the Producer abstraction.","title":" initTransactions"},{"location":"clients/producer/KafkaProducer/#sendoffsetstotransaction","text":"void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , ConsumerGroupMetadata groupMetadata ) sendOffsetsToTransaction requests the TransactionManager to sendOffsetsToTransaction and requests the Sender to wakeup . In the end, sendOffsetsToTransaction waits max.block.ms for the send to be completed (successfully or not). sendOffsetsToTransaction is part of the Producer abstraction.","title":" sendOffsetsToTransaction"},{"location":"clients/producer/KafkaProducer/#sending-record","text":"Future < RecordMetadata > send ( ProducerRecord < K , V > record ) // (1) Future < RecordMetadata > send ( ProducerRecord < K , V > record , Callback callback ) Uses uninitialized Callback ( null ) send requests the interceptors to onSend with the given record (possibly modifying it) followed by doSend . send is part of the Producer abstraction.","title":" Sending Record"},{"location":"clients/producer/KafkaProducer/#dosend","text":"Future < RecordMetadata > doSend ( ProducerRecord < K , V > record , Callback callback ) doSend waitOnMetadata for the topic and partition of the given record. doSend requests the key Serializer to serialize the record (passing in the topic, the headers and the key of the record). doSend requests the value Serializer to serialize the record (passing in the topic, the headers and the value of the record). doSend determines the partition for the record. doSend ensureValidRecordSize for the record (upper bound estimate). doSend prints out the following TRACE message to the logs: Attempting to append record [r] with callback [c] to topic [t] partition [p] doSend requests the RecordAccumulator to append the record (with the abortOnNewBatch flag enabled). When aborted for a new batch, doSend ...FIXME (repeats the steps)...and prints out the following TRACE message to the logs: Retrying append due to new batch creation for topic [t] partition [p]. The old partition was [prev] When transactional , doSend requests the TransactionManager to maybeAddPartitionToTransaction . For batchIsFull or a new batch created, doSend prints out the following TRACE message to the logs and requests the Sender to wakeup . Waking up the sender since topic [t] partition [p] is either full or getting a new batch","title":" doSend"},{"location":"clients/producer/KafkaProducer/#partition","text":"int partition ( ProducerRecord < K , V > record , byte [] serializedKey , byte [] serializedValue , Cluster cluster ) partition is the partition (of the given ProducerRecord ) if defined or requests the Partitioner for the partition .","title":" partition"},{"location":"clients/producer/KafkaProducer/#flushing","text":"void flush () flush requests the RecordAccumulator to beginFlush . flush requests the Sender to wakeup . flush requests the RecordAccumulator to awaitFlushCompletion . flush is part of the Producer abstraction.","title":" Flushing"},{"location":"clients/producer/KafkaProducer/#waitonmetadata","text":"ClusterAndWaitTime waitOnMetadata ( String topic , Integer partition , long nowMs , long maxWaitMs ) waitOnMetadata requests the ProducerMetadata for the current cluster info . waitOnMetadata ...FIXME waitOnMetadata is used when: KafkaProducer is requested to doSend and partitionsFor","title":" waitOnMetadata"},{"location":"clients/producer/KafkaProducer/#demo","text":"// Necessary imports import org.apache.kafka.clients.producer.KafkaProducer import org.apache.kafka.clients.producer.ProducerConfig import org.apache.kafka.common.serialization.StringSerializer // Creating a KafkaProducer import java.util.Properties val props = new Properties() props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \":9092\") val producer = new KafkaProducer[String, String](props) // Creating a record to be sent import org.apache.kafka.clients.producer.ProducerRecord val r = new ProducerRecord[String, String](\"0\", \"this is a message\") // Sending the record (with no Callback) import java.util.concurrent.Future import org.apache.kafka.clients.producer.RecordMetadata val metadataF: Future[RecordMetadata] = producer.send(r)","title":"Demo"},{"location":"clients/producer/KafkaProducer/#logging","text":"Enable ALL logging level for org.apache.kafka.clients.producer.KafkaProducer logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL Refer to Logging .","title":"Logging"},{"location":"clients/producer/Partitioner/","text":"Partitioner \u00b6 Partitioner is an abstraction of partitioners for a KafkaProducer to determine the partition of records (to be sent out ). Configurable \u00b6 Partitioner is a Configurable . Closeable \u00b6 Partitioner is a Closeable ( Java ). Contract \u00b6 onNewBatch \u00b6 void onNewBatch ( String topic , Cluster cluster , int prevPartition ) Used when: KafkaProducer is requested to send a record (and doSend ) Computing Partition \u00b6 int partition ( String topic , Object key , byte [] keyBytes , Object value , byte [] valueBytes , Cluster cluster ) Used when: KafkaProducer is requested to send a record (and determines the partition ) Implementations \u00b6 DefaultPartitioner UniformStickyPartitioner RoundRobinPartitioner","title":"Partitioner"},{"location":"clients/producer/Partitioner/#partitioner","text":"Partitioner is an abstraction of partitioners for a KafkaProducer to determine the partition of records (to be sent out ).","title":"Partitioner"},{"location":"clients/producer/Partitioner/#configurable","text":"Partitioner is a Configurable .","title":" Configurable"},{"location":"clients/producer/Partitioner/#closeable","text":"Partitioner is a Closeable ( Java ).","title":" Closeable"},{"location":"clients/producer/Partitioner/#contract","text":"","title":"Contract"},{"location":"clients/producer/Partitioner/#onnewbatch","text":"void onNewBatch ( String topic , Cluster cluster , int prevPartition ) Used when: KafkaProducer is requested to send a record (and doSend )","title":" onNewBatch"},{"location":"clients/producer/Partitioner/#computing-partition","text":"int partition ( String topic , Object key , byte [] keyBytes , Object value , byte [] valueBytes , Cluster cluster ) Used when: KafkaProducer is requested to send a record (and determines the partition )","title":" Computing Partition"},{"location":"clients/producer/Partitioner/#implementations","text":"DefaultPartitioner UniformStickyPartitioner RoundRobinPartitioner","title":"Implementations"},{"location":"clients/producer/Producer/","text":"Producer \u00b6 Producer<K, V> is an interface to KafkaProducer for Kafka developers to use to send records (with K keys and V values) to a Kafka cluster. Contract (Subset) \u00b6 abortTransaction \u00b6 void abortTransaction () beginTransaction \u00b6 void beginTransaction () commitTransaction \u00b6 void commitTransaction () initTransactions \u00b6 void initTransactions () sendOffsetsToTransaction \u00b6 void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , ConsumerGroupMetadata groupMetadata ) Used when the producer is also a Consumer for a consume-transform-produce pattern","title":"Producer"},{"location":"clients/producer/Producer/#producer","text":"Producer<K, V> is an interface to KafkaProducer for Kafka developers to use to send records (with K keys and V values) to a Kafka cluster.","title":"Producer"},{"location":"clients/producer/Producer/#contract-subset","text":"","title":"Contract (Subset)"},{"location":"clients/producer/Producer/#aborttransaction","text":"void abortTransaction ()","title":" abortTransaction"},{"location":"clients/producer/Producer/#begintransaction","text":"void beginTransaction ()","title":" beginTransaction"},{"location":"clients/producer/Producer/#committransaction","text":"void commitTransaction ()","title":" commitTransaction"},{"location":"clients/producer/Producer/#inittransactions","text":"void initTransactions ()","title":" initTransactions"},{"location":"clients/producer/Producer/#sendoffsetstotransaction","text":"void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , ConsumerGroupMetadata groupMetadata ) Used when the producer is also a Consumer for a consume-transform-produce pattern","title":" sendOffsetsToTransaction"},{"location":"clients/producer/ProducerBatch/","text":"ProducerBatch \u00b6 Creating Instance \u00b6 ProducerBatch takes the following to be created: TopicPartition MemoryRecordsBuilder createdMs isSplitBatch flag (default: false ) ProducerBatch is created when: ProducerBatch is requested to createBatchOffAccumulatorForRecord RecordAccumulator is requested to append a record tryAppend \u00b6 FutureRecordMetadata tryAppend ( long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long now ) tryAppend ...FIXME tryAppend is used when: RecordAccumulator is requested to append a record","title":"ProducerBatch"},{"location":"clients/producer/ProducerBatch/#producerbatch","text":"","title":"ProducerBatch"},{"location":"clients/producer/ProducerBatch/#creating-instance","text":"ProducerBatch takes the following to be created: TopicPartition MemoryRecordsBuilder createdMs isSplitBatch flag (default: false ) ProducerBatch is created when: ProducerBatch is requested to createBatchOffAccumulatorForRecord RecordAccumulator is requested to append a record","title":"Creating Instance"},{"location":"clients/producer/ProducerBatch/#tryappend","text":"FutureRecordMetadata tryAppend ( long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long now ) tryAppend ...FIXME tryAppend is used when: RecordAccumulator is requested to append a record","title":" tryAppend"},{"location":"clients/producer/ProducerConfig/","text":"ProducerConfig \u00b6 acks \u00b6 batch.size \u00b6 The buffer size allocated for a partition. When records are received (which are smaller than this size) KafkaProducer will attempt to optimistically group them together until this size is reached. Default: 16384 Must be at least 0 Related to: linger.ms max-partition-memory-bytes ( ConsoleProducer ) Used when: KafkaProducer is created (to create a RecordAccumulator and an accompanying BufferPool ) KafkaLog4jAppender is requested to activateOptions enable.idempotence \u00b6 Default: false Used when: KafkaProducer is requested to configureTransactionState ProducerConfig is requested to maybeOverrideEnableIdempotence and idempotenceEnabled linger.ms \u00b6 max.block.ms \u00b6 max.in.flight.requests.per.connection \u00b6 The maximum number of unacknowledged requests the client will send on a single connection before blocking. Note that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled). Default: 5 Must be at least 1 Related to: enable.idempotence retries Used when: KafkaProducer is requested to configureInflightRequests partitioner.class \u00b6 The class of the Partitioner for a KafkaProducer Default: DefaultPartitioner retries \u00b6 retry.backoff.ms \u00b6 retry.backoff.ms transactional.id \u00b6 The ID of a KafkaProducer for transactional delivery Default: (undefined) This enables reliability semantics which span multiple producer sessions since it allows the client to guarantee that transactions using the same transactional.id have been completed prior to starting any new transactions. With no transactional.id , a producer is limited to idempotent delivery. When configured, enable.idempotence is implied (and configured when KafkaProducer is created ). With transactional.id , KafkaProducer uses a modified client.id (that includes the ID ). Note that, by default, transactions require a cluster of at least three brokers which is the recommended setting for production; for development you can change this, by adjusting broker setting transaction.state.log.replication.factor . transactional.id is required for the transactional methods . Used when: KafkaProducer prints out log messages (with the transactional ID included in the log prefix) KafkaProducer is created (and creates a TransactionManager ) transaction.state.log.replication.factor \u00b6 transaction.timeout.ms \u00b6 idempotenceEnabled \u00b6 boolean idempotenceEnabled () idempotenceEnabled is enabled ( true ) when one of the following holds: transactional.id is defined enable.idempotence is enabled idempotenceEnabled throws a ConfigException when enable.idempotence is disabled but transactional.id is defined: Cannot set a transactional.id without also enabling idempotence. idempotenceEnabled is used when: KafkaProducer is created (and requested to configureTransactionState , configureInflightRequests , configureAcks ) ProducerConfig is requested to maybeOverrideAcksAndRetries postProcessParsedConfig \u00b6 Map < String , Object > postProcessParsedConfig ( Map < String , Object > parsedValues ) postProcessParsedConfig maybeOverrideEnableIdempotence . postProcessParsedConfig maybeOverrideClientId . postProcessParsedConfig maybeOverrideAcksAndRetries . postProcessParsedConfig is part of the AbstractConfig abstraction. maybeOverrideClientId \u00b6 maybeOverrideAcksAndRetries overrides client.id configuration property unless already defined. The new value uses transactional.id (if defined) or the next available ID with the producer- prefix. maybeOverrideAcksAndRetries \u00b6 void maybeOverrideAcksAndRetries ( Map < String , Object > configs ) maybeOverrideAcksAndRetries ...FIXME maybeOverrideEnableIdempotence \u00b6 void maybeOverrideEnableIdempotence ( Map < String , Object > configs ) maybeOverrideEnableIdempotence sets enable.idempotence configuration property to true when transactional.id is defined with no enable.idempotence .","title":"ProducerConfig"},{"location":"clients/producer/ProducerConfig/#producerconfig","text":"","title":"ProducerConfig"},{"location":"clients/producer/ProducerConfig/#acks","text":"","title":" acks"},{"location":"clients/producer/ProducerConfig/#batchsize","text":"The buffer size allocated for a partition. When records are received (which are smaller than this size) KafkaProducer will attempt to optimistically group them together until this size is reached. Default: 16384 Must be at least 0 Related to: linger.ms max-partition-memory-bytes ( ConsoleProducer ) Used when: KafkaProducer is created (to create a RecordAccumulator and an accompanying BufferPool ) KafkaLog4jAppender is requested to activateOptions","title":" batch.size"},{"location":"clients/producer/ProducerConfig/#enableidempotence","text":"Default: false Used when: KafkaProducer is requested to configureTransactionState ProducerConfig is requested to maybeOverrideEnableIdempotence and idempotenceEnabled","title":" enable.idempotence"},{"location":"clients/producer/ProducerConfig/#lingerms","text":"","title":" linger.ms"},{"location":"clients/producer/ProducerConfig/#maxblockms","text":"","title":" max.block.ms"},{"location":"clients/producer/ProducerConfig/#maxinflightrequestsperconnection","text":"The maximum number of unacknowledged requests the client will send on a single connection before blocking. Note that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled). Default: 5 Must be at least 1 Related to: enable.idempotence retries Used when: KafkaProducer is requested to configureInflightRequests","title":" max.in.flight.requests.per.connection"},{"location":"clients/producer/ProducerConfig/#partitionerclass","text":"The class of the Partitioner for a KafkaProducer Default: DefaultPartitioner","title":" partitioner.class"},{"location":"clients/producer/ProducerConfig/#retries","text":"","title":" retries"},{"location":"clients/producer/ProducerConfig/#retrybackoffms","text":"retry.backoff.ms","title":" retry.backoff.ms"},{"location":"clients/producer/ProducerConfig/#transactionalid","text":"The ID of a KafkaProducer for transactional delivery Default: (undefined) This enables reliability semantics which span multiple producer sessions since it allows the client to guarantee that transactions using the same transactional.id have been completed prior to starting any new transactions. With no transactional.id , a producer is limited to idempotent delivery. When configured, enable.idempotence is implied (and configured when KafkaProducer is created ). With transactional.id , KafkaProducer uses a modified client.id (that includes the ID ). Note that, by default, transactions require a cluster of at least three brokers which is the recommended setting for production; for development you can change this, by adjusting broker setting transaction.state.log.replication.factor . transactional.id is required for the transactional methods . Used when: KafkaProducer prints out log messages (with the transactional ID included in the log prefix) KafkaProducer is created (and creates a TransactionManager )","title":" transactional.id"},{"location":"clients/producer/ProducerConfig/#transactionstatelogreplicationfactor","text":"","title":" transaction.state.log.replication.factor"},{"location":"clients/producer/ProducerConfig/#transactiontimeoutms","text":"","title":" transaction.timeout.ms"},{"location":"clients/producer/ProducerConfig/#idempotenceenabled","text":"boolean idempotenceEnabled () idempotenceEnabled is enabled ( true ) when one of the following holds: transactional.id is defined enable.idempotence is enabled idempotenceEnabled throws a ConfigException when enable.idempotence is disabled but transactional.id is defined: Cannot set a transactional.id without also enabling idempotence. idempotenceEnabled is used when: KafkaProducer is created (and requested to configureTransactionState , configureInflightRequests , configureAcks ) ProducerConfig is requested to maybeOverrideAcksAndRetries","title":" idempotenceEnabled"},{"location":"clients/producer/ProducerConfig/#postprocessparsedconfig","text":"Map < String , Object > postProcessParsedConfig ( Map < String , Object > parsedValues ) postProcessParsedConfig maybeOverrideEnableIdempotence . postProcessParsedConfig maybeOverrideClientId . postProcessParsedConfig maybeOverrideAcksAndRetries . postProcessParsedConfig is part of the AbstractConfig abstraction.","title":" postProcessParsedConfig"},{"location":"clients/producer/ProducerConfig/#maybeoverrideclientid","text":"maybeOverrideAcksAndRetries overrides client.id configuration property unless already defined. The new value uses transactional.id (if defined) or the next available ID with the producer- prefix.","title":" maybeOverrideClientId"},{"location":"clients/producer/ProducerConfig/#maybeoverrideacksandretries","text":"void maybeOverrideAcksAndRetries ( Map < String , Object > configs ) maybeOverrideAcksAndRetries ...FIXME","title":" maybeOverrideAcksAndRetries"},{"location":"clients/producer/ProducerConfig/#maybeoverrideenableidempotence","text":"void maybeOverrideEnableIdempotence ( Map < String , Object > configs ) maybeOverrideEnableIdempotence sets enable.idempotence configuration property to true when transactional.id is defined with no enable.idempotence .","title":" maybeOverrideEnableIdempotence"},{"location":"clients/producer/ProducerInterceptors/","text":"ProducerInterceptors \u00b6 ProducerInterceptors is...FIXME","title":"ProducerInterceptors"},{"location":"clients/producer/ProducerInterceptors/#producerinterceptors","text":"ProducerInterceptors is...FIXME","title":"ProducerInterceptors"},{"location":"clients/producer/RecordAccumulator/","text":"RecordAccumulator \u00b6 Creating Instance \u00b6 RecordAccumulator takes the following to be created: LogContext batch.size CompressionType linger.ms retry.backoff.ms configureDeliveryTimeout Metrics Name of the Metrics Group Time ApiVersions TransactionManager BufferPool RecordAccumulator is created along with KafkaProducer . TransactionManager \u00b6 RecordAccumulator is given a TransactionManager when created . RecordAccumulator uses the TransactionManager when requested for the following: reenqueue splitAndReenqueue insertInSequenceOrder drain ( drainBatchesForOneNode and shouldStopDrainBatchesForPartition ) abortUndrainedBatches appendsInProgress Counter \u00b6 RecordAccumulator creates an AtomicInteger ( Java ) for appendsInProgress internal counter when created . appendsInProgress simply marks a single execution of append (and is incremented at the beginning and decremented right at the end). appendsInProgress is used when flushInProgress . flushInProgress \u00b6 boolean appendsInProgress () appendsInProgress indicates if the appendsInProgress counter is above 0 . appendsInProgress is used when abortIncompleteBatches . flushesInProgress Counter \u00b6 RecordAccumulator creates an AtomicInteger ( Java ) for flushesInProgress internal counter when created . flushesInProgress is incremented when beginFlush and decremented when awaitFlushCompletion . flushesInProgress is used when flushInProgress . flushInProgress \u00b6 boolean flushInProgress () flushInProgress indicates if the flushesInProgress counter is above 0 . flushInProgress is used when: RecordAccumulator is requested to ready Sender is requested to maybeSendAndPollTransactionalRequest Appending Record \u00b6 RecordAppendResult append ( TopicPartition tp , long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long maxTimeToBlock , boolean abortOnNewBatch , long nowMs ) append ...FIXME append is used when: KafkaProducer is requested to send a record (and doSend ) tryAppend \u00b6 RecordAppendResult tryAppend ( long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , Deque < ProducerBatch > deque , long nowMs ) tryAppend ...FIXME ready \u00b6 ReadyCheckResult ready ( Cluster cluster , long nowMs ) ready is a list of partitions with data ready to send. ready ...FIXME ready is used when: Sender is requested to sendProducerData beginFlush \u00b6 void beginFlush () beginFlush atomically increments the flushesInProgress counter. beginFlush is used when: KafkaProducer is requested to flush Sender is requested to maybeSendAndPollTransactionalRequest awaitFlushCompletion \u00b6 void awaitFlushCompletion () awaitFlushCompletion ...FIXME awaitFlushCompletion is used when: KafkaProducer is requested to flush splitAndReenqueue \u00b6 int splitAndReenqueue ( ProducerBatch bigBatch ) splitAndReenqueue ...FIXME splitAndReenqueue is used when: Sender is requested to completeBatch deallocate \u00b6 void deallocate ( ProducerBatch batch ) deallocate ...FIXME deallocate is used when: RecordAccumulator is requested to abortBatches and abortUndrainedBatches Sender is requested to maybeRemoveAndDeallocateBatch abortBatches \u00b6 void abortBatches () // (1) void abortBatches ( RuntimeException reason ) Uses a KafkaException abortBatches ...FIXME abortBatches is used when: RecordAccumulator is requested to abortIncompleteBatches Sender is requested to maybeAbortBatches abortIncompleteBatches \u00b6 void abortIncompleteBatches () abortIncompleteBatches abortBatches as long as there are appendsInProgress . abortIncompleteBatches abortBatches one last time (after no thread was appending in case there was a new batch appended by the last appending thread). In the end, abortIncompleteBatches clears the batches registry. abortIncompleteBatches is used when: Sender is requested to run (and forceClose ) abortUndrainedBatches \u00b6 void abortUndrainedBatches ( RuntimeException reason ) abortUndrainedBatches ...FIXME abortUndrainedBatches is used when: Sender is requested to maybeSendAndPollTransactionalRequest Incomplete (Pending) Batches \u00b6 RecordAccumulator creates an IncompleteBatches for incomplete internal registry of pending batches when created . RecordAccumulator uses the IncompleteBatches when: append (to add a new ProducerBatch ) splitAndReenqueue (to add a new ProducerBatch ) deallocate (to remove a ProducerBatch ) awaitFlushCompletion , abortBatches and abortUndrainedBatches (to copy all ProducerBatch s) hasIncomplete \u00b6 boolean hasIncomplete () hasIncomplete is true when the incomplete registry is not empty. hasIncomplete is used when: Sender is requested to maybeSendAndPollTransactionalRequest and maybeAbortBatches In-Progress Batches \u00b6 ConcurrentMap < TopicPartition , Deque < ProducerBatch >> batches RecordAccumulator creates a ConcurrentMap ( Java ) for the batches internal registry of in-progress ProducerBatch es (per TopicPartition ). RecordAccumulator adds a new ArrayDeque ( Java ) when getOrCreateDeque . batches is used when: expiredBatches ready hasUndrained getDeque batches abortIncompleteBatches getOrCreateDeque \u00b6 Deque < ProducerBatch > getOrCreateDeque ( TopicPartition tp ) getOrCreateDeque ...FIXME getOrCreateDeque is used when: RecordAccumulator is requested to append , reenqueue , splitAndReenqueue reenqueue \u00b6 void reenqueue ( ProducerBatch batch , long now ) reenqueue ...FIXME reenqueue is used when: Sender is requested to reenqueueBatch insertInSequenceOrder \u00b6 void insertInSequenceOrder ( Deque < ProducerBatch > deque , ProducerBatch batch ) insertInSequenceOrder ...FIXME insertInSequenceOrder is used when: RecordAccumulator is requested to reenqueue and splitAndReenqueue drain \u00b6 Map < Integer , List < ProducerBatch >> drain ( Cluster cluster , Set < Node > nodes , int maxSize , long now ) drain ...FIXME drain is used when: Sender is requested to sendProducerData drainBatchesForOneNode \u00b6 List < ProducerBatch > drainBatchesForOneNode ( Cluster cluster , Node node , int maxSize , long now ) drainBatchesForOneNode ...FIXME shouldStopDrainBatchesForPartition \u00b6 boolean shouldStopDrainBatchesForPartition ( ProducerBatch first , TopicPartition tp ) shouldStopDrainBatchesForPartition ...FIXME","title":"RecordAccumulator"},{"location":"clients/producer/RecordAccumulator/#recordaccumulator","text":"","title":"RecordAccumulator"},{"location":"clients/producer/RecordAccumulator/#creating-instance","text":"RecordAccumulator takes the following to be created: LogContext batch.size CompressionType linger.ms retry.backoff.ms configureDeliveryTimeout Metrics Name of the Metrics Group Time ApiVersions TransactionManager BufferPool RecordAccumulator is created along with KafkaProducer .","title":"Creating Instance"},{"location":"clients/producer/RecordAccumulator/#transactionmanager","text":"RecordAccumulator is given a TransactionManager when created . RecordAccumulator uses the TransactionManager when requested for the following: reenqueue splitAndReenqueue insertInSequenceOrder drain ( drainBatchesForOneNode and shouldStopDrainBatchesForPartition ) abortUndrainedBatches","title":" TransactionManager"},{"location":"clients/producer/RecordAccumulator/#appendsinprogress-counter","text":"RecordAccumulator creates an AtomicInteger ( Java ) for appendsInProgress internal counter when created . appendsInProgress simply marks a single execution of append (and is incremented at the beginning and decremented right at the end). appendsInProgress is used when flushInProgress .","title":" appendsInProgress Counter"},{"location":"clients/producer/RecordAccumulator/#flushinprogress","text":"boolean appendsInProgress () appendsInProgress indicates if the appendsInProgress counter is above 0 . appendsInProgress is used when abortIncompleteBatches .","title":"flushInProgress"},{"location":"clients/producer/RecordAccumulator/#flushesinprogress-counter","text":"RecordAccumulator creates an AtomicInteger ( Java ) for flushesInProgress internal counter when created . flushesInProgress is incremented when beginFlush and decremented when awaitFlushCompletion . flushesInProgress is used when flushInProgress .","title":" flushesInProgress Counter"},{"location":"clients/producer/RecordAccumulator/#flushinprogress_1","text":"boolean flushInProgress () flushInProgress indicates if the flushesInProgress counter is above 0 . flushInProgress is used when: RecordAccumulator is requested to ready Sender is requested to maybeSendAndPollTransactionalRequest","title":" flushInProgress"},{"location":"clients/producer/RecordAccumulator/#appending-record","text":"RecordAppendResult append ( TopicPartition tp , long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long maxTimeToBlock , boolean abortOnNewBatch , long nowMs ) append ...FIXME append is used when: KafkaProducer is requested to send a record (and doSend )","title":" Appending Record"},{"location":"clients/producer/RecordAccumulator/#tryappend","text":"RecordAppendResult tryAppend ( long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , Deque < ProducerBatch > deque , long nowMs ) tryAppend ...FIXME","title":" tryAppend"},{"location":"clients/producer/RecordAccumulator/#ready","text":"ReadyCheckResult ready ( Cluster cluster , long nowMs ) ready is a list of partitions with data ready to send. ready ...FIXME ready is used when: Sender is requested to sendProducerData","title":" ready"},{"location":"clients/producer/RecordAccumulator/#beginflush","text":"void beginFlush () beginFlush atomically increments the flushesInProgress counter. beginFlush is used when: KafkaProducer is requested to flush Sender is requested to maybeSendAndPollTransactionalRequest","title":" beginFlush"},{"location":"clients/producer/RecordAccumulator/#awaitflushcompletion","text":"void awaitFlushCompletion () awaitFlushCompletion ...FIXME awaitFlushCompletion is used when: KafkaProducer is requested to flush","title":" awaitFlushCompletion"},{"location":"clients/producer/RecordAccumulator/#splitandreenqueue","text":"int splitAndReenqueue ( ProducerBatch bigBatch ) splitAndReenqueue ...FIXME splitAndReenqueue is used when: Sender is requested to completeBatch","title":" splitAndReenqueue"},{"location":"clients/producer/RecordAccumulator/#deallocate","text":"void deallocate ( ProducerBatch batch ) deallocate ...FIXME deallocate is used when: RecordAccumulator is requested to abortBatches and abortUndrainedBatches Sender is requested to maybeRemoveAndDeallocateBatch","title":" deallocate"},{"location":"clients/producer/RecordAccumulator/#abortbatches","text":"void abortBatches () // (1) void abortBatches ( RuntimeException reason ) Uses a KafkaException abortBatches ...FIXME abortBatches is used when: RecordAccumulator is requested to abortIncompleteBatches Sender is requested to maybeAbortBatches","title":" abortBatches"},{"location":"clients/producer/RecordAccumulator/#abortincompletebatches","text":"void abortIncompleteBatches () abortIncompleteBatches abortBatches as long as there are appendsInProgress . abortIncompleteBatches abortBatches one last time (after no thread was appending in case there was a new batch appended by the last appending thread). In the end, abortIncompleteBatches clears the batches registry. abortIncompleteBatches is used when: Sender is requested to run (and forceClose )","title":" abortIncompleteBatches"},{"location":"clients/producer/RecordAccumulator/#abortundrainedbatches","text":"void abortUndrainedBatches ( RuntimeException reason ) abortUndrainedBatches ...FIXME abortUndrainedBatches is used when: Sender is requested to maybeSendAndPollTransactionalRequest","title":" abortUndrainedBatches"},{"location":"clients/producer/RecordAccumulator/#incomplete-pending-batches","text":"RecordAccumulator creates an IncompleteBatches for incomplete internal registry of pending batches when created . RecordAccumulator uses the IncompleteBatches when: append (to add a new ProducerBatch ) splitAndReenqueue (to add a new ProducerBatch ) deallocate (to remove a ProducerBatch ) awaitFlushCompletion , abortBatches and abortUndrainedBatches (to copy all ProducerBatch s)","title":" Incomplete (Pending) Batches"},{"location":"clients/producer/RecordAccumulator/#hasincomplete","text":"boolean hasIncomplete () hasIncomplete is true when the incomplete registry is not empty. hasIncomplete is used when: Sender is requested to maybeSendAndPollTransactionalRequest and maybeAbortBatches","title":" hasIncomplete"},{"location":"clients/producer/RecordAccumulator/#in-progress-batches","text":"ConcurrentMap < TopicPartition , Deque < ProducerBatch >> batches RecordAccumulator creates a ConcurrentMap ( Java ) for the batches internal registry of in-progress ProducerBatch es (per TopicPartition ). RecordAccumulator adds a new ArrayDeque ( Java ) when getOrCreateDeque . batches is used when: expiredBatches ready hasUndrained getDeque batches abortIncompleteBatches","title":" In-Progress Batches"},{"location":"clients/producer/RecordAccumulator/#getorcreatedeque","text":"Deque < ProducerBatch > getOrCreateDeque ( TopicPartition tp ) getOrCreateDeque ...FIXME getOrCreateDeque is used when: RecordAccumulator is requested to append , reenqueue , splitAndReenqueue","title":" getOrCreateDeque"},{"location":"clients/producer/RecordAccumulator/#reenqueue","text":"void reenqueue ( ProducerBatch batch , long now ) reenqueue ...FIXME reenqueue is used when: Sender is requested to reenqueueBatch","title":" reenqueue"},{"location":"clients/producer/RecordAccumulator/#insertinsequenceorder","text":"void insertInSequenceOrder ( Deque < ProducerBatch > deque , ProducerBatch batch ) insertInSequenceOrder ...FIXME insertInSequenceOrder is used when: RecordAccumulator is requested to reenqueue and splitAndReenqueue","title":" insertInSequenceOrder"},{"location":"clients/producer/RecordAccumulator/#drain","text":"Map < Integer , List < ProducerBatch >> drain ( Cluster cluster , Set < Node > nodes , int maxSize , long now ) drain ...FIXME drain is used when: Sender is requested to sendProducerData","title":" drain"},{"location":"clients/producer/RecordAccumulator/#drainbatchesforonenode","text":"List < ProducerBatch > drainBatchesForOneNode ( Cluster cluster , Node node , int maxSize , long now ) drainBatchesForOneNode ...FIXME","title":" drainBatchesForOneNode"},{"location":"clients/producer/RecordAccumulator/#shouldstopdrainbatchesforpartition","text":"boolean shouldStopDrainBatchesForPartition ( ProducerBatch first , TopicPartition tp ) shouldStopDrainBatchesForPartition ...FIXME","title":" shouldStopDrainBatchesForPartition"},{"location":"clients/producer/Sender/","text":"Sender \u00b6 Sender is a Runnable ( Java ) that is executed as a separate thread alongside KafkaProducer to send records to a Kafka cluster. Creating Instance \u00b6 Sender takes the following to be created: LogContext KafkaClient ProducerMetadata RecordAccumulator guaranteeMessageOrder flag maxRequestSize acks retries SenderMetricsRegistry Time requestTimeoutMs retryBackoffMs TransactionManager ApiVersions Sender is created along with KafkaProducer . KafkaClient \u00b6 Sender is given a KafkaClient when created . Running Thread \u00b6 void run () run prints out the following DEBUG message to the logs: Starting Kafka producer I/O thread. run runs once and repeats until the running flag is turned off. Right after the running flag is off, run prints out the following DEBUG message to the logs: Beginning shutdown of Kafka producer I/O thread, sending remaining records. run ...FIXME In the end, run prints out the following DEBUG message to the logs: Shutdown of Kafka producer I/O thread has completed. run is part of the Runnable ( Java ) abstraction. runOnce \u00b6 void runOnce () If executed with a TransactionManager , runOnce ...FIXME runOnce sendProducerData . runOnce requests the KafkaClient to poll . sendProducerData \u00b6 long sendProducerData ( long now ) sendProducerData requests the ProducerMetadata for the current cluster info sendProducerData requests the RecordAccumulator for the partitions with data ready to send . sendProducerData requests a metadata update when there are partitions with no leaders. sendProducerData removes nodes not ready to send to. sendProducerData requests the RecordAccumulator to drain (and create ProducerBatch s). sendProducerData registers the batches (in the inFlightBatches registry). With guaranteeMessageOrder , sendProducerData mutes all the partitions drained. sendProducerData requests the RecordAccumulator to resetNextBatchExpiryTime . sendProducerData requests the RecordAccumulator for the expired batches and adds all expired InflightBatches . If there are any expired batches, sendProducerData ...FIXME sendProducerData requests the SenderMetrics to updateProduceRequestMetrics . With at least one broker to send batches to, sendProducerData prints out the following TRACE message to the logs: Nodes with data ready to send: [readyNodes] sendProducerData sendProduceRequests . sendProduceRequests \u00b6 void sendProduceRequests ( Map < Integer , List < ProducerBatch >> collated , long now ) For every pair of a broker node and an associated ProducerBatch (in the given collated collection), sendProduceRequests sendProduceRequest with the broker node, the acks , the requestTimeoutMs and the ProducerBatch . sendProduceRequest \u00b6 void sendProduceRequest ( long now , int destination , short acks , int timeout , List < ProducerBatch > batches ) sendProduceRequest creates a collection of ProducerBatch es by TopicPartition from the given batches . sendProduceRequest requests the KafkaClient for a new ClientRequest (for the destination broker) and to send it . sendProduceRequest registers a [handleProduceResponse] callback to invoke when a response arrives. sendProduceRequest expects a response for all the acks but 0 . In the end, sendProduceRequest prints out the following TRACE message to the logs: Sent produce request to [nodeId]: [requestBuilder] handleProduceResponse \u00b6 void handleProduceResponse ( ClientResponse response , Map < TopicPartition , ProducerBatch > batches , long now ) maybeSendAndPollTransactionalRequest \u00b6 boolean maybeSendAndPollTransactionalRequest () running Flag \u00b6 Sender runs as long as the running internal flag is on. The running flag is on from when Sender is created until requested to initiateClose . initiateClose \u00b6 void initiateClose () initiateClose requests the RecordAccumulator to close and turns the running flag off. In the end, initiateClose wakes up the KafkaClient . initiateClose is used when: KafkaProducer is requested to close Sender is requested to forceClose Waking Up \u00b6 void wakeup () wakeup requests the KafkaClient to wakeup . wakeup is used when: KafkaProducer is requested to initTransactions , sendOffsetsToTransaction , commitTransaction , abortTransaction , doSend , waitOnMetadata , flush Sender is requested to initiateClose Logging \u00b6 Enable ALL logging level for org.apache.kafka.clients.producer.internals.Sender logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.org.apache.kafka.clients.producer.internals.Sender=ALL Refer to Logging .","title":"Sender"},{"location":"clients/producer/Sender/#sender","text":"Sender is a Runnable ( Java ) that is executed as a separate thread alongside KafkaProducer to send records to a Kafka cluster.","title":"Sender"},{"location":"clients/producer/Sender/#creating-instance","text":"Sender takes the following to be created: LogContext KafkaClient ProducerMetadata RecordAccumulator guaranteeMessageOrder flag maxRequestSize acks retries SenderMetricsRegistry Time requestTimeoutMs retryBackoffMs TransactionManager ApiVersions Sender is created along with KafkaProducer .","title":"Creating Instance"},{"location":"clients/producer/Sender/#kafkaclient","text":"Sender is given a KafkaClient when created .","title":" KafkaClient"},{"location":"clients/producer/Sender/#running-thread","text":"void run () run prints out the following DEBUG message to the logs: Starting Kafka producer I/O thread. run runs once and repeats until the running flag is turned off. Right after the running flag is off, run prints out the following DEBUG message to the logs: Beginning shutdown of Kafka producer I/O thread, sending remaining records. run ...FIXME In the end, run prints out the following DEBUG message to the logs: Shutdown of Kafka producer I/O thread has completed. run is part of the Runnable ( Java ) abstraction.","title":" Running Thread"},{"location":"clients/producer/Sender/#runonce","text":"void runOnce () If executed with a TransactionManager , runOnce ...FIXME runOnce sendProducerData . runOnce requests the KafkaClient to poll .","title":" runOnce"},{"location":"clients/producer/Sender/#sendproducerdata","text":"long sendProducerData ( long now ) sendProducerData requests the ProducerMetadata for the current cluster info sendProducerData requests the RecordAccumulator for the partitions with data ready to send . sendProducerData requests a metadata update when there are partitions with no leaders. sendProducerData removes nodes not ready to send to. sendProducerData requests the RecordAccumulator to drain (and create ProducerBatch s). sendProducerData registers the batches (in the inFlightBatches registry). With guaranteeMessageOrder , sendProducerData mutes all the partitions drained. sendProducerData requests the RecordAccumulator to resetNextBatchExpiryTime . sendProducerData requests the RecordAccumulator for the expired batches and adds all expired InflightBatches . If there are any expired batches, sendProducerData ...FIXME sendProducerData requests the SenderMetrics to updateProduceRequestMetrics . With at least one broker to send batches to, sendProducerData prints out the following TRACE message to the logs: Nodes with data ready to send: [readyNodes] sendProducerData sendProduceRequests .","title":" sendProducerData"},{"location":"clients/producer/Sender/#sendproducerequests","text":"void sendProduceRequests ( Map < Integer , List < ProducerBatch >> collated , long now ) For every pair of a broker node and an associated ProducerBatch (in the given collated collection), sendProduceRequests sendProduceRequest with the broker node, the acks , the requestTimeoutMs and the ProducerBatch .","title":" sendProduceRequests"},{"location":"clients/producer/Sender/#sendproducerequest","text":"void sendProduceRequest ( long now , int destination , short acks , int timeout , List < ProducerBatch > batches ) sendProduceRequest creates a collection of ProducerBatch es by TopicPartition from the given batches . sendProduceRequest requests the KafkaClient for a new ClientRequest (for the destination broker) and to send it . sendProduceRequest registers a [handleProduceResponse] callback to invoke when a response arrives. sendProduceRequest expects a response for all the acks but 0 . In the end, sendProduceRequest prints out the following TRACE message to the logs: Sent produce request to [nodeId]: [requestBuilder]","title":" sendProduceRequest"},{"location":"clients/producer/Sender/#handleproduceresponse","text":"void handleProduceResponse ( ClientResponse response , Map < TopicPartition , ProducerBatch > batches , long now )","title":" handleProduceResponse"},{"location":"clients/producer/Sender/#maybesendandpolltransactionalrequest","text":"boolean maybeSendAndPollTransactionalRequest ()","title":" maybeSendAndPollTransactionalRequest"},{"location":"clients/producer/Sender/#running-flag","text":"Sender runs as long as the running internal flag is on. The running flag is on from when Sender is created until requested to initiateClose .","title":" running Flag"},{"location":"clients/producer/Sender/#initiateclose","text":"void initiateClose () initiateClose requests the RecordAccumulator to close and turns the running flag off. In the end, initiateClose wakes up the KafkaClient . initiateClose is used when: KafkaProducer is requested to close Sender is requested to forceClose","title":" initiateClose"},{"location":"clients/producer/Sender/#waking-up","text":"void wakeup () wakeup requests the KafkaClient to wakeup . wakeup is used when: KafkaProducer is requested to initTransactions , sendOffsetsToTransaction , commitTransaction , abortTransaction , doSend , waitOnMetadata , flush Sender is requested to initiateClose","title":" Waking Up"},{"location":"clients/producer/Sender/#logging","text":"Enable ALL logging level for org.apache.kafka.clients.producer.internals.Sender logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.org.apache.kafka.clients.producer.internals.Sender=ALL Refer to Logging .","title":"Logging"},{"location":"clients/producer/TransactionManager/","text":"TransactionManager \u00b6 Creating Instance \u00b6 TransactionManager takes the following to be created: LogContext transactional.id transaction.timeout.ms retry.backoff.ms ApiVersions TransactionManager is created along with KafkaProducer (with idempotenceEnabled ). States \u00b6 TransactionManager can be in one of the following states: UNINITIALIZED INITIALIZING READY IN_TRANSACTION COMMITTING_TRANSACTION ABORTING_TRANSACTION ABORTABLE_ERROR FATAL_ERROR Valid Transitions \u00b6 Source (Current) State Target States transitionTo ABORTABLE_ERROR ABORTING_TRANSACTION ABORTABLE_ERROR ABORTING_TRANSACTION INITIALIZING READY beginAbort COMMITTING_TRANSACTION READY ABORTABLE_ERROR beginCommit IN_TRANSACTION COMMITTING_TRANSACTION ABORTING_TRANSACTION ABORTABLE_ERROR beginTransaction INITIALIZING READY initializeTransactions bumpIdempotentEpochAndResetIdIfNeeded completeTransaction READY UNINITIALIZED IN_TRANSACTION completeTransaction InitProducerIdHandler UNINITIALIZED INITIALIZING resetIdempotentProducerId any state FATAL_ERROR beginAbort \u00b6 TransactionalRequestResult beginAbort () beginAbort ...FIXME beginAbort is used when: KafkaProducer is requested to abortTransaction Sender is requested to run (and is shuting down) beginCommit \u00b6 TransactionalRequestResult beginCommit () beginCommit ...FIXME beginCommit is used when: KafkaProducer is requested to commitTransaction beginCompletingTransaction \u00b6 TransactionalRequestResult beginCompletingTransaction ( TransactionResult transactionResult ) beginCompletingTransaction ...FIXME beginCompletingTransaction is used when: TransactionManager is requested to beginCommit and beginAbort beginTransaction \u00b6 void beginTransaction () beginTransaction makes sure that the producer is transactional and transition to IN_TRANSACTION state. beginTransaction is used when: KafkaProducer is requested to beginTransaction initializeTransactions \u00b6 TransactionalRequestResult initializeTransactions () // (1) TransactionalRequestResult initializeTransactions ( ProducerIdAndEpoch producerIdAndEpoch ) Uses ProducerIdAndEpoch.NONE initializeTransactions ...FIXME initializeTransactions is used when: KafkaProducer is requested to initTransactions TransactionManager is requested to beginCompletingTransaction isTransactional \u00b6 boolean isTransactional () isTransactional is enabled ( true ) when the transactional.id configuration property is defined (for the producer and the transactionalId was given when created ). maybeAddPartitionToTransaction \u00b6 void maybeAddPartitionToTransaction ( TopicPartition topicPartition ) maybeAddPartitionToTransaction ...FIXME maybeAddPartitionToTransaction is used when: KafkaProducer is requested to doSend sendOffsetsToTransaction \u00b6 TransactionalRequestResult sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , ConsumerGroupMetadata groupMetadata ) sendOffsetsToTransaction ...FIXME sendOffsetsToTransaction is used when: KafkaProducer is requested to sendOffsetsToTransaction","title":"TransactionManager"},{"location":"clients/producer/TransactionManager/#transactionmanager","text":"","title":"TransactionManager"},{"location":"clients/producer/TransactionManager/#creating-instance","text":"TransactionManager takes the following to be created: LogContext transactional.id transaction.timeout.ms retry.backoff.ms ApiVersions TransactionManager is created along with KafkaProducer (with idempotenceEnabled ).","title":"Creating Instance"},{"location":"clients/producer/TransactionManager/#states","text":"TransactionManager can be in one of the following states: UNINITIALIZED INITIALIZING READY IN_TRANSACTION COMMITTING_TRANSACTION ABORTING_TRANSACTION ABORTABLE_ERROR FATAL_ERROR","title":" States"},{"location":"clients/producer/TransactionManager/#valid-transitions","text":"Source (Current) State Target States transitionTo ABORTABLE_ERROR ABORTING_TRANSACTION ABORTABLE_ERROR ABORTING_TRANSACTION INITIALIZING READY beginAbort COMMITTING_TRANSACTION READY ABORTABLE_ERROR beginCommit IN_TRANSACTION COMMITTING_TRANSACTION ABORTING_TRANSACTION ABORTABLE_ERROR beginTransaction INITIALIZING READY initializeTransactions bumpIdempotentEpochAndResetIdIfNeeded completeTransaction READY UNINITIALIZED IN_TRANSACTION completeTransaction InitProducerIdHandler UNINITIALIZED INITIALIZING resetIdempotentProducerId any state FATAL_ERROR","title":" Valid Transitions"},{"location":"clients/producer/TransactionManager/#beginabort","text":"TransactionalRequestResult beginAbort () beginAbort ...FIXME beginAbort is used when: KafkaProducer is requested to abortTransaction Sender is requested to run (and is shuting down)","title":" beginAbort"},{"location":"clients/producer/TransactionManager/#begincommit","text":"TransactionalRequestResult beginCommit () beginCommit ...FIXME beginCommit is used when: KafkaProducer is requested to commitTransaction","title":" beginCommit"},{"location":"clients/producer/TransactionManager/#begincompletingtransaction","text":"TransactionalRequestResult beginCompletingTransaction ( TransactionResult transactionResult ) beginCompletingTransaction ...FIXME beginCompletingTransaction is used when: TransactionManager is requested to beginCommit and beginAbort","title":" beginCompletingTransaction"},{"location":"clients/producer/TransactionManager/#begintransaction","text":"void beginTransaction () beginTransaction makes sure that the producer is transactional and transition to IN_TRANSACTION state. beginTransaction is used when: KafkaProducer is requested to beginTransaction","title":" beginTransaction"},{"location":"clients/producer/TransactionManager/#initializetransactions","text":"TransactionalRequestResult initializeTransactions () // (1) TransactionalRequestResult initializeTransactions ( ProducerIdAndEpoch producerIdAndEpoch ) Uses ProducerIdAndEpoch.NONE initializeTransactions ...FIXME initializeTransactions is used when: KafkaProducer is requested to initTransactions TransactionManager is requested to beginCompletingTransaction","title":" initializeTransactions"},{"location":"clients/producer/TransactionManager/#istransactional","text":"boolean isTransactional () isTransactional is enabled ( true ) when the transactional.id configuration property is defined (for the producer and the transactionalId was given when created ).","title":" isTransactional"},{"location":"clients/producer/TransactionManager/#maybeaddpartitiontotransaction","text":"void maybeAddPartitionToTransaction ( TopicPartition topicPartition ) maybeAddPartitionToTransaction ...FIXME maybeAddPartitionToTransaction is used when: KafkaProducer is requested to doSend","title":" maybeAddPartitionToTransaction"},{"location":"clients/producer/TransactionManager/#sendoffsetstotransaction","text":"TransactionalRequestResult sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , ConsumerGroupMetadata groupMetadata ) sendOffsetsToTransaction ...FIXME sendOffsetsToTransaction is used when: KafkaProducer is requested to sendOffsetsToTransaction","title":" sendOffsetsToTransaction"},{"location":"demo/","text":"Demos \u00b6 The following demos are available: Kafka and kcat in Docker Transactional Kafka Producer","title":"Demos"},{"location":"demo/#demos","text":"The following demos are available: Kafka and kcat in Docker Transactional Kafka Producer","title":"Demos"},{"location":"demo/kafka-and-kcat-in-docker/","text":"Demo: Kafka and kcat in Docker \u00b6 This demo uses Docker to run Apache Kafka and kcat utility. kafka-docker \u00b6 Pull kafka-docker project (or create a docker-compose.yml file yourself). Running Kafka Cluster \u00b6 Start Zookeeper and Kafka containers. docker-compose up $ docker-compose ps Name Command State Ports ---------------------------------------------------------------------------------------------------------------------------------------- kafka-docker_kafka_1 start-kafka.sh Up 0.0.0.0:62687->9092/tcp kafka-docker_zookeeper_1 /bin/sh -c /usr/sbin/sshd ... Up 0.0.0.0:2181->2181/tcp,:::2181->2181/tcp, 22/tcp, 2888/tcp, 3888/tcp Docker Network \u00b6 The above creates a Docker network kafka-docker_default (if ran from kafka-docker directory as described in the official documentation of docker-compose ). $ docker network ls NETWORK ID NAME DRIVER SCOPE b8b255710858 bridge bridge local 3c9c3a969ef2 cda bridge local 398f9f3196aa host host local 68611503fde8 kafka-docker_default bridge local db43a5e50281 none null local kcat \u00b6 Connect kcat container to the network (using --network option as described in the official documentation of docker-compose ). Metadata Listing \u00b6 docker run -it --rm \\ --network kafka-docker_default \\ edenhill/kcat:1.7.0 \\ -b kafka-docker_kafka_1:9092 -L Metadata for all topics (from broker -1: kafka-docker_kafka_1:9092/bootstrap): 1 brokers: broker 1001 at 09cc8de4d067:9092 (controller) 0 topics: Producer \u00b6 docker run -it --rm \\ --network kafka-docker_default \\ --name producer \\ edenhill/kcat:1.7.0 \\ -b kafka-docker_kafka_1:9092 -P -t t1 Caution For some reason the above command couldn't send messages whenever I pressed ENTER but expected Ctrl+D instead (that terminates the shell and the container). Switching to confluentinc/cp-kafkacat made things working fine. docker run -it --rm \\ --network kafka-docker_default \\ --name producer \\ confluentinc/cp-kafkacat \\ kafkacat \\ -b kafka-docker_kafka_1:9092 -P -t t1 Consumer \u00b6 docker run -it --rm \\ --network kafka-docker_default \\ --name consumer \\ edenhill/kcat:1.7.0 \\ -b kafka-docker_kafka_1:9092 -C -t t1 Clean Up \u00b6 docker-compose down","title":"Kafka and kcat in Docker"},{"location":"demo/kafka-and-kcat-in-docker/#demo-kafka-and-kcat-in-docker","text":"This demo uses Docker to run Apache Kafka and kcat utility.","title":"Demo: Kafka and kcat in Docker"},{"location":"demo/kafka-and-kcat-in-docker/#kafka-docker","text":"Pull kafka-docker project (or create a docker-compose.yml file yourself).","title":"kafka-docker"},{"location":"demo/kafka-and-kcat-in-docker/#running-kafka-cluster","text":"Start Zookeeper and Kafka containers. docker-compose up $ docker-compose ps Name Command State Ports ---------------------------------------------------------------------------------------------------------------------------------------- kafka-docker_kafka_1 start-kafka.sh Up 0.0.0.0:62687->9092/tcp kafka-docker_zookeeper_1 /bin/sh -c /usr/sbin/sshd ... Up 0.0.0.0:2181->2181/tcp,:::2181->2181/tcp, 22/tcp, 2888/tcp, 3888/tcp","title":"Running Kafka Cluster"},{"location":"demo/kafka-and-kcat-in-docker/#docker-network","text":"The above creates a Docker network kafka-docker_default (if ran from kafka-docker directory as described in the official documentation of docker-compose ). $ docker network ls NETWORK ID NAME DRIVER SCOPE b8b255710858 bridge bridge local 3c9c3a969ef2 cda bridge local 398f9f3196aa host host local 68611503fde8 kafka-docker_default bridge local db43a5e50281 none null local","title":"Docker Network"},{"location":"demo/kafka-and-kcat-in-docker/#kcat","text":"Connect kcat container to the network (using --network option as described in the official documentation of docker-compose ).","title":"kcat"},{"location":"demo/kafka-and-kcat-in-docker/#metadata-listing","text":"docker run -it --rm \\ --network kafka-docker_default \\ edenhill/kcat:1.7.0 \\ -b kafka-docker_kafka_1:9092 -L Metadata for all topics (from broker -1: kafka-docker_kafka_1:9092/bootstrap): 1 brokers: broker 1001 at 09cc8de4d067:9092 (controller) 0 topics:","title":"Metadata Listing"},{"location":"demo/kafka-and-kcat-in-docker/#producer","text":"docker run -it --rm \\ --network kafka-docker_default \\ --name producer \\ edenhill/kcat:1.7.0 \\ -b kafka-docker_kafka_1:9092 -P -t t1 Caution For some reason the above command couldn't send messages whenever I pressed ENTER but expected Ctrl+D instead (that terminates the shell and the container). Switching to confluentinc/cp-kafkacat made things working fine. docker run -it --rm \\ --network kafka-docker_default \\ --name producer \\ confluentinc/cp-kafkacat \\ kafkacat \\ -b kafka-docker_kafka_1:9092 -P -t t1","title":"Producer"},{"location":"demo/kafka-and-kcat-in-docker/#consumer","text":"docker run -it --rm \\ --network kafka-docker_default \\ --name consumer \\ edenhill/kcat:1.7.0 \\ -b kafka-docker_kafka_1:9092 -C -t t1","title":"Consumer"},{"location":"demo/kafka-and-kcat-in-docker/#clean-up","text":"docker-compose down","title":"Clean Up"},{"location":"demo/transactional-kafka-producer/","text":"Demo: Transactional Kafka Producer \u00b6 This demo shows the internals of transactional KafkaProducer that is a Kafka producer with transaction.id defined. KafkaProducer \u00b6 Start Up Kafka Producer \u00b6 import org.apache.kafka.clients.producer.KafkaProducer import org.apache.kafka.clients.producer.ProducerConfig import org.apache.kafka.common.serialization.StringSerializer import java.util.Properties val props = new Properties() props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \":9092\") props.put(ProducerConfig.CLIENT_ID_CONFIG, \"txn-demo\") // Define transaction.id val transactionalId = \"my-custom-txnId\" props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId) val producer = new KafkaProducer[String, String](props) Initialize Transactions \u00b6 producer.initTransactions Once initialized, the transactional producer must not be initialized again. scala> producer.initTransactions org.apache.kafka.common.KafkaException: TransactionalId my-custom-txnId: Invalid transition attempted from state READY to state INITIALIZING at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:1078) at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:1071) at org.apache.kafka.clients.producer.internals.TransactionManager.lambda$initializeTransactions$1(TransactionManager.java:337) at org.apache.kafka.clients.producer.internals.TransactionManager.handleCachedTransactionRequestResult(TransactionManager.java:1200) at org.apache.kafka.clients.producer.internals.TransactionManager.initializeTransactions(TransactionManager.java:334) at org.apache.kafka.clients.producer.internals.TransactionManager.initializeTransactions(TransactionManager.java:329) at org.apache.kafka.clients.producer.KafkaProducer.initTransactions(KafkaProducer.java:596) ... 31 elided Start Transaction \u00b6 Next up is starting a transaction using KafkaProducer.beginTransaction producer.beginTransaction Transactional send \u00b6 import org.apache.kafka.clients.producer.ProducerRecord val topic = \"txn-demo\" val record = new ProducerRecord[String, String](topic, \"Hello from transactional producer\") producer.send(record) Logs \u00b6 Kafka Producer \u00b6 You should see the following INFO messages in the logs of the Kafka producer: INFO [Producer clientId=producer-my-custom-txnId, transactionalId=my-custom-txnId] Invoking InitProducerId for the first time in order to acquire a producer ID (org.apache.kafka.clients.producer.internals.TransactionManager) INFO [Producer clientId=producer-my-custom-txnId, transactionalId=my-custom-txnId] Discovered transaction coordinator localhost:9092 (id: 0 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager) INFO [Producer clientId=producer-my-custom-txnId, transactionalId=my-custom-txnId] ProducerId set to 0 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager) Kafka Cluster \u00b6 You should see the following INFO message in the logs of a Kafka cluster: INFO [TransactionCoordinator id=0] Initialized transactionalId my-custom-txnId with producerId 0 and producer epoch 0 on partition __transaction_state-20 (kafka.coordinator.transaction.TransactionCoordinator) The calculation to determine the transactional partition ( __transaction_state-20 ) is as follows: Math . abs ( transactionalId . hashCode ) % 50 Start Up Consumer \u00b6 kcat -C -b localhost -t txn-demo You should see no records produced yet (since the transaction has not been committed yet). Commit Transaction \u00b6 Let's commit the transaction using KafkaProducer.commitTransaction . producer.commitTransaction Immediately after committing the transaction you should see the record printed out by the Kafka consumer. Another Kafka Producer (WIP) \u00b6 Todo This is a work-in-progress. scala> producer.commitTransaction [2021-09-23 11:38:29,001] INFO [Producer clientId=txn-demo, transactionalId=my-custom-txnId] Transiting to fatal error state due to org.apache.kafka.common.errors.ProducerFencedException: There is a newer producer with the same transactionalId which fences the current one. (org.apache.kafka.clients.producer.internals.TransactionManager) org.apache.kafka.common.errors.ProducerFencedException: There is a newer producer with the same transactionalId which fences the current one.","title":"Transactional Kafka Producer"},{"location":"demo/transactional-kafka-producer/#demo-transactional-kafka-producer","text":"This demo shows the internals of transactional KafkaProducer that is a Kafka producer with transaction.id defined.","title":"Demo: Transactional Kafka Producer"},{"location":"demo/transactional-kafka-producer/#kafkaproducer","text":"","title":"KafkaProducer"},{"location":"demo/transactional-kafka-producer/#start-up-kafka-producer","text":"import org.apache.kafka.clients.producer.KafkaProducer import org.apache.kafka.clients.producer.ProducerConfig import org.apache.kafka.common.serialization.StringSerializer import java.util.Properties val props = new Properties() props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \":9092\") props.put(ProducerConfig.CLIENT_ID_CONFIG, \"txn-demo\") // Define transaction.id val transactionalId = \"my-custom-txnId\" props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId) val producer = new KafkaProducer[String, String](props)","title":"Start Up Kafka Producer"},{"location":"demo/transactional-kafka-producer/#initialize-transactions","text":"producer.initTransactions Once initialized, the transactional producer must not be initialized again. scala> producer.initTransactions org.apache.kafka.common.KafkaException: TransactionalId my-custom-txnId: Invalid transition attempted from state READY to state INITIALIZING at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:1078) at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:1071) at org.apache.kafka.clients.producer.internals.TransactionManager.lambda$initializeTransactions$1(TransactionManager.java:337) at org.apache.kafka.clients.producer.internals.TransactionManager.handleCachedTransactionRequestResult(TransactionManager.java:1200) at org.apache.kafka.clients.producer.internals.TransactionManager.initializeTransactions(TransactionManager.java:334) at org.apache.kafka.clients.producer.internals.TransactionManager.initializeTransactions(TransactionManager.java:329) at org.apache.kafka.clients.producer.KafkaProducer.initTransactions(KafkaProducer.java:596) ... 31 elided","title":"Initialize Transactions"},{"location":"demo/transactional-kafka-producer/#start-transaction","text":"Next up is starting a transaction using KafkaProducer.beginTransaction producer.beginTransaction","title":"Start Transaction"},{"location":"demo/transactional-kafka-producer/#transactional-send","text":"import org.apache.kafka.clients.producer.ProducerRecord val topic = \"txn-demo\" val record = new ProducerRecord[String, String](topic, \"Hello from transactional producer\") producer.send(record)","title":"Transactional send"},{"location":"demo/transactional-kafka-producer/#logs","text":"","title":"Logs"},{"location":"demo/transactional-kafka-producer/#kafka-producer","text":"You should see the following INFO messages in the logs of the Kafka producer: INFO [Producer clientId=producer-my-custom-txnId, transactionalId=my-custom-txnId] Invoking InitProducerId for the first time in order to acquire a producer ID (org.apache.kafka.clients.producer.internals.TransactionManager) INFO [Producer clientId=producer-my-custom-txnId, transactionalId=my-custom-txnId] Discovered transaction coordinator localhost:9092 (id: 0 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager) INFO [Producer clientId=producer-my-custom-txnId, transactionalId=my-custom-txnId] ProducerId set to 0 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)","title":"Kafka Producer"},{"location":"demo/transactional-kafka-producer/#kafka-cluster","text":"You should see the following INFO message in the logs of a Kafka cluster: INFO [TransactionCoordinator id=0] Initialized transactionalId my-custom-txnId with producerId 0 and producer epoch 0 on partition __transaction_state-20 (kafka.coordinator.transaction.TransactionCoordinator) The calculation to determine the transactional partition ( __transaction_state-20 ) is as follows: Math . abs ( transactionalId . hashCode ) % 50","title":"Kafka Cluster"},{"location":"demo/transactional-kafka-producer/#start-up-consumer","text":"kcat -C -b localhost -t txn-demo You should see no records produced yet (since the transaction has not been committed yet).","title":"Start Up Consumer"},{"location":"demo/transactional-kafka-producer/#commit-transaction","text":"Let's commit the transaction using KafkaProducer.commitTransaction . producer.commitTransaction Immediately after committing the transaction you should see the record printed out by the Kafka consumer.","title":"Commit Transaction"},{"location":"demo/transactional-kafka-producer/#another-kafka-producer-wip","text":"Todo This is a work-in-progress. scala> producer.commitTransaction [2021-09-23 11:38:29,001] INFO [Producer clientId=txn-demo, transactionalId=my-custom-txnId] Transiting to fatal error state due to org.apache.kafka.common.errors.ProducerFencedException: There is a newer producer with the same transactionalId which fences the current one. (org.apache.kafka.clients.producer.internals.TransactionManager) org.apache.kafka.common.errors.ProducerFencedException: There is a newer producer with the same transactionalId which fences the current one.","title":"Another Kafka Producer (WIP)"},{"location":"tools/","text":"Tools \u00b6","title":"Tools"},{"location":"tools/#tools","text":"","title":"Tools"},{"location":"tools/ReplicaVerificationTool/","text":"ReplicaVerificationTool \u00b6 ReplicaVerificationTool is...FIXME","title":"ReplicaVerificationTool"},{"location":"tools/ReplicaVerificationTool/#replicaverificationtool","text":"ReplicaVerificationTool is...FIXME","title":"ReplicaVerificationTool"},{"location":"transactions/","text":"Transactions \u00b6 Apache Kafka supports transactional record delivery (and consumption if in consumer-process-produce processing mode). Every Kafka broker runs a TransactionCoordinator to manage ( coordinate ) transactions. Transactional Producer \u00b6 A KafkaProducer is transactional when transactional.id configuration property is specified. Any record sending has to be after KafkaProducer.initTransactions followed by KafkaProducer.beginTransaction . Otherwise, the underlying TransactionManager is going to be in a wrong state (that will inevitably lead to exceptions). Transaction-Aware Consumer \u00b6 A KafkaConsumer supports transactions using isolation.level configuration property. kafka-console-consumer \u00b6 kafka-console-consumer supports --isolation-level option for isolation.level configuration property. Demo \u00b6 Demo: Transactional Kafka Producer Transactional Configuration Properties \u00b6 TransactionConfig Transaction Topic \u00b6 Kafka brokers use __transaction_state internal topic for managing transactions (as records ). __transaction_state is auto-created at the first transaction. The number of partitions is configured using transaction.state.log.num.partitions configuration property. A transaction (record) is assigned a partition ( txn topic partition ) based on the absolute hash code of the transactional.id . Learning Resources \u00b6 Transactions in Apache Kafka by Confluent","title":"Transactions"},{"location":"transactions/#transactions","text":"Apache Kafka supports transactional record delivery (and consumption if in consumer-process-produce processing mode). Every Kafka broker runs a TransactionCoordinator to manage ( coordinate ) transactions.","title":"Transactions"},{"location":"transactions/#transactional-producer","text":"A KafkaProducer is transactional when transactional.id configuration property is specified. Any record sending has to be after KafkaProducer.initTransactions followed by KafkaProducer.beginTransaction . Otherwise, the underlying TransactionManager is going to be in a wrong state (that will inevitably lead to exceptions).","title":"Transactional Producer"},{"location":"transactions/#transaction-aware-consumer","text":"A KafkaConsumer supports transactions using isolation.level configuration property.","title":"Transaction-Aware Consumer"},{"location":"transactions/#kafka-console-consumer","text":"kafka-console-consumer supports --isolation-level option for isolation.level configuration property.","title":"kafka-console-consumer"},{"location":"transactions/#demo","text":"Demo: Transactional Kafka Producer","title":"Demo"},{"location":"transactions/#transactional-configuration-properties","text":"TransactionConfig","title":" Transactional Configuration Properties"},{"location":"transactions/#transaction-topic","text":"Kafka brokers use __transaction_state internal topic for managing transactions (as records ). __transaction_state is auto-created at the first transaction. The number of partitions is configured using transaction.state.log.num.partitions configuration property. A transaction (record) is assigned a partition ( txn topic partition ) based on the absolute hash code of the transactional.id .","title":" Transaction Topic"},{"location":"transactions/#learning-resources","text":"Transactions in Apache Kafka by Confluent","title":"Learning Resources"},{"location":"transactions/TransactionConfig/","text":"TransactionConfig \u00b6 TransactionConfig holds the values of the transactional configuration properties. transactional.id.expiration.ms \u00b6 transactional.id.expiration.ms Default: 7 days transaction.max.timeout.ms \u00b6 transaction.max.timeout.ms Default: 15 minutes transaction.state.log.num.partitions \u00b6 transaction.state.log.num.partitions Default: 50 transaction.state.log.replication.factor \u00b6 transaction.state.log.replication.factor Default: 3 transaction.state.log.segment.bytes \u00b6 transaction.state.log.segment.bytes Default: 100 * 1024 * 1024 transaction.state.log.load.buffer.size \u00b6 transaction.state.log.load.buffer.size Default: 5 * 1024 * 1024 transaction.state.log.min.isr \u00b6 transaction.state.log.min.isr Default: 2 transaction.abort.timed.out.transaction.cleanup.interval.ms \u00b6 transaction.abort.timed.out.transaction.cleanup.interval.ms Default: 10 seconds transaction.remove.expired.transaction.cleanup.interval.ms \u00b6 transaction.remove.expired.transaction.cleanup.interval.ms Default: 1 hour request.timeout.ms \u00b6 request.timeout.ms Default: 30000","title":"TransactionConfig"},{"location":"transactions/TransactionConfig/#transactionconfig","text":"TransactionConfig holds the values of the transactional configuration properties.","title":"TransactionConfig"},{"location":"transactions/TransactionConfig/#transactionalidexpirationms","text":"transactional.id.expiration.ms Default: 7 days","title":" transactional.id.expiration.ms"},{"location":"transactions/TransactionConfig/#transactionmaxtimeoutms","text":"transaction.max.timeout.ms Default: 15 minutes","title":" transaction.max.timeout.ms"},{"location":"transactions/TransactionConfig/#transactionstatelognumpartitions","text":"transaction.state.log.num.partitions Default: 50","title":" transaction.state.log.num.partitions"},{"location":"transactions/TransactionConfig/#transactionstatelogreplicationfactor","text":"transaction.state.log.replication.factor Default: 3","title":" transaction.state.log.replication.factor"},{"location":"transactions/TransactionConfig/#transactionstatelogsegmentbytes","text":"transaction.state.log.segment.bytes Default: 100 * 1024 * 1024","title":" transaction.state.log.segment.bytes"},{"location":"transactions/TransactionConfig/#transactionstatelogloadbuffersize","text":"transaction.state.log.load.buffer.size Default: 5 * 1024 * 1024","title":" transaction.state.log.load.buffer.size"},{"location":"transactions/TransactionConfig/#transactionstatelogminisr","text":"transaction.state.log.min.isr Default: 2","title":" transaction.state.log.min.isr"},{"location":"transactions/TransactionConfig/#transactionaborttimedouttransactioncleanupintervalms","text":"transaction.abort.timed.out.transaction.cleanup.interval.ms Default: 10 seconds","title":" transaction.abort.timed.out.transaction.cleanup.interval.ms"},{"location":"transactions/TransactionConfig/#transactionremoveexpiredtransactioncleanupintervalms","text":"transaction.remove.expired.transaction.cleanup.interval.ms Default: 1 hour","title":" transaction.remove.expired.transaction.cleanup.interval.ms"},{"location":"transactions/TransactionConfig/#requesttimeoutms","text":"request.timeout.ms Default: 30000","title":" request.timeout.ms"},{"location":"transactions/TransactionCoordinator/","text":"TransactionCoordinator \u00b6 TransactionCoordinator runs on every Kafka broker ( BrokerServer or KafkaServer ). Creating Instance \u00b6 TransactionCoordinator takes the following to be created: Broker Id TransactionConfig Scheduler createProducerIdGenerator function ( () => ProducerIdGenerator ) TransactionStateManager TransactionMarkerChannelManager Time LogContext TransactionCoordinator is created using apply factory. Creating TransactionCoordinator \u00b6 apply ( config : KafkaConfig , replicaManager : ReplicaManager , scheduler : Scheduler , createProducerIdGenerator : () => ProducerIdGenerator , metrics : Metrics , metadataCache : MetadataCache , time : Time ): TransactionCoordinator apply creates a TransactionConfig . apply creates a TransactionStateManager (with the brokerId and the other Kafka services). apply creates a LogContext that uses the following log prefix (with the brokerId ): [TransactionCoordinator id=[brokerId]] apply creates a TransactionMarkerChannelManager . In the end, apply creates a TransactionCoordinator . apply is used when: BrokerServer is requested to start up KafkaServer is requested to start up Starting Up \u00b6 startup ( retrieveTransactionTopicPartitionCount : () => Int , enableTransactionalIdExpiration : Boolean = true ): Unit startup ...FIXME startup is used when: BrokerServer is requested to start up KafkaServer is requested to start up onElection \u00b6 onElection ( txnTopicPartitionId : Int , coordinatorEpoch : Int ): Unit onElection prints out the following INFO message to the logs: Elected as the txn coordinator for partition [txnTopicPartitionId] at epoch [coordinatorEpoch] onElection requests the TransactionMarkerChannelManager to removeMarkersForTxnTopicPartition for the given txnTopicPartitionId partition. In the end, onElection requests the TransactionStateManager to loadTransactionsForTxnTopicPartition . onElection is used when: RequestHandlerHelper is requested to onLeadershipChange onResignation \u00b6 onResignation ( txnTopicPartitionId : Int , coordinatorEpoch : Option [ Int ]): Unit onResignation ...FIXME onResignation is used when: KafkaApis is requested to handleStopReplicaRequest RequestHandlerHelper is requested to onLeadershipChange handleInitProducerId \u00b6 handleInitProducerId ( transactionalId : String , transactionTimeoutMs : Int , expectedProducerIdAndEpoch : Option [ ProducerIdAndEpoch ], responseCallback : InitProducerIdCallback ): Unit For transactionalId undefined ( null ), handleInitProducerId requests the ProducerIdGenerator to generateProducerId and sends it back (using the given InitProducerIdCallback ). handleInitProducerId requests the TransactionStateManager to getTransactionState for the given transactionalId . handleInitProducerId prints out the following INFO message to the logs: Initialized transactionalId [transactionalId] with producerId [producerId] and producer epoch [producerEpoch] on partition __transaction_state-[partition] In the end, handleInitProducerId requests the TransactionStateManager to appendTransactionToLog . handleInitProducerId is used when: KafkaApis is requested to handleInitProducerIdRequest Logging \u00b6 Enable ALL logging level for kafka.coordinator.transaction.TransactionCoordinator logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.kafka.coordinator.transaction.TransactionCoordinator=ALL Refer to Logging .","title":"TransactionCoordinator"},{"location":"transactions/TransactionCoordinator/#transactioncoordinator","text":"TransactionCoordinator runs on every Kafka broker ( BrokerServer or KafkaServer ).","title":"TransactionCoordinator"},{"location":"transactions/TransactionCoordinator/#creating-instance","text":"TransactionCoordinator takes the following to be created: Broker Id TransactionConfig Scheduler createProducerIdGenerator function ( () => ProducerIdGenerator ) TransactionStateManager TransactionMarkerChannelManager Time LogContext TransactionCoordinator is created using apply factory.","title":"Creating Instance"},{"location":"transactions/TransactionCoordinator/#creating-transactioncoordinator","text":"apply ( config : KafkaConfig , replicaManager : ReplicaManager , scheduler : Scheduler , createProducerIdGenerator : () => ProducerIdGenerator , metrics : Metrics , metadataCache : MetadataCache , time : Time ): TransactionCoordinator apply creates a TransactionConfig . apply creates a TransactionStateManager (with the brokerId and the other Kafka services). apply creates a LogContext that uses the following log prefix (with the brokerId ): [TransactionCoordinator id=[brokerId]] apply creates a TransactionMarkerChannelManager . In the end, apply creates a TransactionCoordinator . apply is used when: BrokerServer is requested to start up KafkaServer is requested to start up","title":" Creating TransactionCoordinator"},{"location":"transactions/TransactionCoordinator/#starting-up","text":"startup ( retrieveTransactionTopicPartitionCount : () => Int , enableTransactionalIdExpiration : Boolean = true ): Unit startup ...FIXME startup is used when: BrokerServer is requested to start up KafkaServer is requested to start up","title":" Starting Up"},{"location":"transactions/TransactionCoordinator/#onelection","text":"onElection ( txnTopicPartitionId : Int , coordinatorEpoch : Int ): Unit onElection prints out the following INFO message to the logs: Elected as the txn coordinator for partition [txnTopicPartitionId] at epoch [coordinatorEpoch] onElection requests the TransactionMarkerChannelManager to removeMarkersForTxnTopicPartition for the given txnTopicPartitionId partition. In the end, onElection requests the TransactionStateManager to loadTransactionsForTxnTopicPartition . onElection is used when: RequestHandlerHelper is requested to onLeadershipChange","title":" onElection"},{"location":"transactions/TransactionCoordinator/#onresignation","text":"onResignation ( txnTopicPartitionId : Int , coordinatorEpoch : Option [ Int ]): Unit onResignation ...FIXME onResignation is used when: KafkaApis is requested to handleStopReplicaRequest RequestHandlerHelper is requested to onLeadershipChange","title":" onResignation"},{"location":"transactions/TransactionCoordinator/#handleinitproducerid","text":"handleInitProducerId ( transactionalId : String , transactionTimeoutMs : Int , expectedProducerIdAndEpoch : Option [ ProducerIdAndEpoch ], responseCallback : InitProducerIdCallback ): Unit For transactionalId undefined ( null ), handleInitProducerId requests the ProducerIdGenerator to generateProducerId and sends it back (using the given InitProducerIdCallback ). handleInitProducerId requests the TransactionStateManager to getTransactionState for the given transactionalId . handleInitProducerId prints out the following INFO message to the logs: Initialized transactionalId [transactionalId] with producerId [producerId] and producer epoch [producerEpoch] on partition __transaction_state-[partition] In the end, handleInitProducerId requests the TransactionStateManager to appendTransactionToLog . handleInitProducerId is used when: KafkaApis is requested to handleInitProducerIdRequest","title":" handleInitProducerId"},{"location":"transactions/TransactionCoordinator/#logging","text":"Enable ALL logging level for kafka.coordinator.transaction.TransactionCoordinator logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.kafka.coordinator.transaction.TransactionCoordinator=ALL Refer to Logging .","title":"Logging"},{"location":"transactions/TransactionMarkerChannelManager/","text":"TransactionMarkerChannelManager \u00b6 TransactionMarkerChannelManager is...FIXME","title":"TransactionMarkerChannelManager"},{"location":"transactions/TransactionMarkerChannelManager/#transactionmarkerchannelmanager","text":"TransactionMarkerChannelManager is...FIXME","title":"TransactionMarkerChannelManager"},{"location":"transactions/TransactionStateManager/","text":"TransactionStateManager \u00b6 Creating Instance \u00b6 TransactionStateManager takes the following to be created: Broker ID Scheduler ReplicaManager TransactionConfig Time Metrics TransactionStateManager is created when: TransactionCoordinator utility is used to create a TransactionCoordinator Starting Up \u00b6 startup ( retrieveTransactionTopicPartitionCount : () => Int , enableTransactionalIdExpiration : Boolean = true ): Unit startup ...FIXME startup is used when: TransactionCoordinator is requested to start up enableTransactionalIdExpiration \u00b6 enableTransactionalIdExpiration (): Unit enableTransactionalIdExpiration ...FIXME appendTransactionToLog \u00b6 appendTransactionToLog ( transactionalId : String , coordinatorEpoch : Int , newMetadata : TxnTransitMetadata , responseCallback : Errors => Unit , retryOnError : Errors => Boolean = _ => false ): Unit appendTransactionToLog generates the key and the value (of the record to represent the transaction in the topic) based on the given transactionalId and the TxnTransitMetadata , respectively. appendTransactionToLog ...FIXME appendTransactionToLog requests the ReplicaManager to appendRecords (with -1 acks, internalTopicsAllowed enabled annd Coordinator origin) and prints out the following TRACE message to the logs: Appending new metadata [newMetadata] for transaction id [transactionalId] with coordinator epoch [coordinatorEpoch] to the local transaction log appendTransactionToLog is used when: TransactionCoordinator is requested to handleInitProducerId , handleAddPartitionsToTransaction , endTransaction TransactionMarkerChannelManager is requested to tryAppendToLog partitionFor \u00b6 partitionFor ( transactionalId : String ): Int partitionFor calculates the partition for the given transactionalId . partitionFor gets the absolute value of the hashCode of the transactionalId string modulo the number of partitions of the __transaction_state topic. partitionFor is used when: TransactionStateManager is requested to appendTransactionToLog , enableTransactionalIdExpiration , getAndMaybeAddTransactionState TransactionCoordinator is requested to handleInitProducerId TransactionMarkerChannelManager is requested to addTxnMarkersToBrokerQueue loadTransactionsForTxnTopicPartition \u00b6 loadTransactionsForTxnTopicPartition ( partitionId : Int , coordinatorEpoch : Int , sendTxnMarkers : SendTxnMarkersCallback ): Unit loadTransactionsForTxnTopicPartition ...FIXME loadTransactionsForTxnTopicPartition is used when: TransactionCoordinator is requested to onElection removeTransactionsForTxnTopicPartition \u00b6 removeTransactionsForTxnTopicPartition ( partitionId : Int ): Unit removeTransactionsForTxnTopicPartition ( partitionId : Int , coordinatorEpoch : Int ): Unit removeTransactionsForTxnTopicPartition ...FIXME removeTransactionsForTxnTopicPartition is used when: TransactionCoordinator is requested to onResignation","title":"TransactionStateManager"},{"location":"transactions/TransactionStateManager/#transactionstatemanager","text":"","title":"TransactionStateManager"},{"location":"transactions/TransactionStateManager/#creating-instance","text":"TransactionStateManager takes the following to be created: Broker ID Scheduler ReplicaManager TransactionConfig Time Metrics TransactionStateManager is created when: TransactionCoordinator utility is used to create a TransactionCoordinator","title":"Creating Instance"},{"location":"transactions/TransactionStateManager/#starting-up","text":"startup ( retrieveTransactionTopicPartitionCount : () => Int , enableTransactionalIdExpiration : Boolean = true ): Unit startup ...FIXME startup is used when: TransactionCoordinator is requested to start up","title":" Starting Up"},{"location":"transactions/TransactionStateManager/#enabletransactionalidexpiration","text":"enableTransactionalIdExpiration (): Unit enableTransactionalIdExpiration ...FIXME","title":" enableTransactionalIdExpiration"},{"location":"transactions/TransactionStateManager/#appendtransactiontolog","text":"appendTransactionToLog ( transactionalId : String , coordinatorEpoch : Int , newMetadata : TxnTransitMetadata , responseCallback : Errors => Unit , retryOnError : Errors => Boolean = _ => false ): Unit appendTransactionToLog generates the key and the value (of the record to represent the transaction in the topic) based on the given transactionalId and the TxnTransitMetadata , respectively. appendTransactionToLog ...FIXME appendTransactionToLog requests the ReplicaManager to appendRecords (with -1 acks, internalTopicsAllowed enabled annd Coordinator origin) and prints out the following TRACE message to the logs: Appending new metadata [newMetadata] for transaction id [transactionalId] with coordinator epoch [coordinatorEpoch] to the local transaction log appendTransactionToLog is used when: TransactionCoordinator is requested to handleInitProducerId , handleAddPartitionsToTransaction , endTransaction TransactionMarkerChannelManager is requested to tryAppendToLog","title":" appendTransactionToLog"},{"location":"transactions/TransactionStateManager/#partitionfor","text":"partitionFor ( transactionalId : String ): Int partitionFor calculates the partition for the given transactionalId . partitionFor gets the absolute value of the hashCode of the transactionalId string modulo the number of partitions of the __transaction_state topic. partitionFor is used when: TransactionStateManager is requested to appendTransactionToLog , enableTransactionalIdExpiration , getAndMaybeAddTransactionState TransactionCoordinator is requested to handleInitProducerId TransactionMarkerChannelManager is requested to addTxnMarkersToBrokerQueue","title":" partitionFor"},{"location":"transactions/TransactionStateManager/#loadtransactionsfortxntopicpartition","text":"loadTransactionsForTxnTopicPartition ( partitionId : Int , coordinatorEpoch : Int , sendTxnMarkers : SendTxnMarkersCallback ): Unit loadTransactionsForTxnTopicPartition ...FIXME loadTransactionsForTxnTopicPartition is used when: TransactionCoordinator is requested to onElection","title":" loadTransactionsForTxnTopicPartition"},{"location":"transactions/TransactionStateManager/#removetransactionsfortxntopicpartition","text":"removeTransactionsForTxnTopicPartition ( partitionId : Int ): Unit removeTransactionsForTxnTopicPartition ( partitionId : Int , coordinatorEpoch : Int ): Unit removeTransactionsForTxnTopicPartition ...FIXME removeTransactionsForTxnTopicPartition is used when: TransactionCoordinator is requested to onResignation","title":" removeTransactionsForTxnTopicPartition"}]}