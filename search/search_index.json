{"config":{"indexing":"full","jieba_dict":null,"jieba_dict_user":null,"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"<p>Welcome to The Internals of Apache Kafka online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, an IT freelancer specializing in Apache Spark, Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB, mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Kafka as much as I have.</p>  <p>Flannery O'Connor</p> <p>I write to discover what I know.</p>   \"The Internals Of\" series <p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p>  <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into Apache Kafka \ud83d\udd25</p>  <p>Last update: 2022-08-08</p>","title":"The Internals of Apache Kafka 3.2.1"},{"location":"AbstractConfig/","text":"<p><code>AbstractConfig</code> is...FIXME</p>","title":"AbstractConfig"},{"location":"AutoTopicCreationManager/","text":"<p><code>AutoTopicCreationManager</code> is an abstraction of managers that can create topics.</p>","title":"AutoTopicCreationManager"},{"location":"AutoTopicCreationManager/#contract","text":"","title":"Contract"},{"location":"AutoTopicCreationManager/#createtopics","text":"","title":"createTopics <pre><code>createTopics(\n  topicNames: Set[String],\n  controllerMutationQuota: ControllerMutationQuota): Seq[MetadataResponseTopic]\n</code></pre> <p>Used when:</p> <ul> <li><code>KafkaApis</code> is requested to getTopicMetadata and handleFindCoordinatorRequest</li> </ul>"},{"location":"AutoTopicCreationManager/#shutdown","text":"","title":"shutdown <pre><code>shutdown(): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>BrokerServer</code> is requested to shutdown</li> <li><code>KafkaServer</code> is requested to shutdown</li> </ul>"},{"location":"AutoTopicCreationManager/#start","text":"","title":"start <pre><code>start(): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>BrokerServer</code> is requested to startup</li> <li><code>KafkaServer</code> is requested to startup</li> </ul>"},{"location":"AutoTopicCreationManager/#implementations","text":"<ul> <li>DefaultAutoTopicCreationManager</li> </ul>","title":"Implementations"},{"location":"AutoTopicCreationManager/#creating-autotopiccreationmanager","text":"","title":"Creating AutoTopicCreationManager <pre><code>apply(\n  config: KafkaConfig,\n  metadataCache: MetadataCache,\n  time: Time,\n  metrics: Metrics,\n  threadNamePrefix: Option[String],\n  adminManager: Option[ZkAdminManager],\n  controller: Option[KafkaController],\n  groupCoordinator: GroupCoordinator,\n  txnCoordinator: TransactionCoordinator,\n  enableForwarding: Boolean): AutoTopicCreationManager\n</code></pre> <p><code>apply</code> creates a DefaultAutoTopicCreationManager.</p> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>KafkaServer</code> is requested to startup</li> </ul>"},{"location":"BrokerMetadataCheckpoint/","text":"<p><code>BrokerMetadataCheckpoint</code> is...FIXME</p>","title":"BrokerMetadataCheckpoint"},{"location":"Configurable/","text":"<p><code>Configurable</code> is...FIXME</p>","title":"Configurable"},{"location":"DefaultAutoTopicCreationManager/","text":"<p><code>DefaultAutoTopicCreationManager</code> is a AutoTopicCreationManager.</p>","title":"DefaultAutoTopicCreationManager"},{"location":"DefaultAutoTopicCreationManager/#creating-instance","text":"<p><code>DefaultAutoTopicCreationManager</code> takes the following to be created:</p> <ul> <li> KafkaConfig <li> Optional <code>BrokerToControllerChannelManager</code> <li> Optional <code>ZkAdminManager</code> <li> Optional <code>KafkaController</code> <li> <code>GroupCoordinator</code> <li>TransactionCoordinator</li>  <p><code>DefaultAutoTopicCreationManager</code> is created\u00a0when:</p> <ul> <li><code>AutoTopicCreationManager</code> utility is used to create an AutoTopicCreationManager</li> <li><code>BrokerServer</code> is requested to startup</li> </ul>","title":"Creating Instance"},{"location":"DefaultAutoTopicCreationManager/#transactioncoordinator","text":"","title":"TransactionCoordinator <p><code>DefaultAutoTopicCreationManager</code> is given a TransactionCoordinator when created.</p> <p><code>DefaultAutoTopicCreationManager</code> uses the <code>TransactionCoordinator</code> for the transactionTopicConfigs when requested to creatableTopic for __transaction_state topic.</p>"},{"location":"FetchResponseData/","text":"<p><code>FetchResponseData</code> is a <code>ApiMessage</code>.</p>","title":"FetchResponseData"},{"location":"InterBrokerSendThread/","text":"","title":"InterBrokerSendThread"},{"location":"Kafka/","text":"<p><code>Kafka</code> is a command-line application.</p>","title":"Kafka Utility"},{"location":"Kafka/#main","text":"","title":"main <pre><code>main(\n  args: Array[String]): Unit\n</code></pre> <p><code>main</code>...FIXME</p>"},{"location":"Kafka/#buildserver","text":"","title":"buildServer <pre><code>buildServer(\n  props: Properties): Server\n</code></pre> <p><code>buildServer</code> creates a KafkaServer or KafkaRaftServer based on process.roles configuration property (in the given <code>Properties</code>).</p>"},{"location":"KafkaApis/","text":"","title":"KafkaApis"},{"location":"KafkaApis/#creating-instance","text":"<p><code>KafkaApis</code> takes the following to be created:</p> <ul> <li> <code>RequestChannel</code> <li> <code>MetadataSupport</code> <li> <code>ReplicaManager</code> <li> <code>GroupCoordinator</code> <li>TransactionCoordinator</li> <li> AutoTopicCreationManager <li> Broker ID <li> KafkaConfig <li> <code>ConfigRepository</code> <li> <code>MetadataCache</code> <li> <code>Metrics</code> <li> Optional <code>Authorizer</code> <li> <code>QuotaManagers</code> <li> <code>FetchManager</code> <li> <code>BrokerTopicStats</code> <li> Cluster ID <li> <code>Time</code> <li> <code>DelegationTokenManager</code> <li> <code>ApiVersionManager</code>  <p><code>KafkaApis</code> is created\u00a0when:</p> <ul> <li><code>BrokerServer</code> is requested to startup (for the dataPlaneRequestProcessor and the controlPlaneRequestProcessor)</li> <li><code>KafkaServer</code> is requested to startup (for the dataPlaneRequestProcessor and the controlPlaneRequestProcessor)</li> </ul>","title":"Creating Instance"},{"location":"KafkaApis/#transactioncoordinator","text":"","title":"TransactionCoordinator <p><code>KafkaApis</code> is given a TransactionCoordinator when created.</p> <p>The <code>TransactionCoordinator</code> is used for the following:</p> <ul> <li>handleAddOffsetsToTxnRequest</li> <li>handleAddPartitionToTxnRequest</li> <li>handleEndTxnRequest</li> <li>handleFindCoordinatorRequest</li> <li>handleInitProducerIdRequest</li> <li>handleLeaderAndIsrRequest</li> <li>handleStopReplicaRequest</li> </ul>"},{"location":"KafkaApis/#handleinitproduceridrequest","text":"","title":"handleInitProducerIdRequest <pre><code>handleInitProducerIdRequest(\n  request: RequestChannel.Request): Unit\n</code></pre> <p><code>handleInitProducerIdRequest</code> assumes that the given <code>RequestChannel.Request</code> is an <code>InitProducerIdRequest</code>.</p> <p><code>handleInitProducerIdRequest</code> authorizes the request.</p> <p>With <code>producerId</code> and <code>producerEpoch</code> set either to <code>-1</code>s (<code>NO_PRODUCER_ID</code> and <code>NO_PRODUCER_EPOCH</code>) or some non-<code>-1</code> values, <code>handleInitProducerIdRequest</code> requests the TransactionCoordinator to handleInitProducerId.</p> <p>Otherwise, <code>handleInitProducerIdRequest</code> sends an error back.</p> <p><code>handleInitProducerIdRequest</code>\u00a0is used when:</p> <ul> <li><code>KafkaApis</code> is requested to handle a INIT_PRODUCER_ID request</li> </ul>"},{"location":"KafkaApis/#handlefetchrequest","text":"","title":"handleFetchRequest <pre><code>handleFetchRequest(\n  request: RequestChannel.Request): Unit\n</code></pre> <p><code>handleFetchRequest</code> assumes that the given <code>RequestChannel.Request</code> is an <code>FetchRequest</code>.</p> <p><code>handleFetchRequest</code> authorizes the request.</p> <p>In the end, <code>handleFetchRequest</code> requests the ReplicaManager to fetchMessages.</p> <p><code>handleFetchRequest</code>\u00a0is used when:</p> <ul> <li><code>KafkaApis</code> is requested to handle a FETCH request</li> </ul>"},{"location":"KafkaBroker/","text":"<p><code>KafkaBroker</code> is...FIXME</p>","title":"KafkaBroker"},{"location":"KafkaConfig/","text":"","title":"KafkaConfig"},{"location":"KafkaConfig/#transactionalidexpirationms","text":"","title":"transactional.id.expiration.ms"},{"location":"KafkaConfig/#transactionmaxtimeoutms","text":"","title":"transaction.max.timeout.ms"},{"location":"KafkaConfig/#transactionstatelognumpartitions","text":"","title":"transaction.state.log.num.partitions <p>The number of partitions for the transaction topic</p> <p>Default: 50</p> <p>Must be at least 1</p>"},{"location":"KafkaConfig/#transactionstatelogreplicationfactor","text":"","title":"transaction.state.log.replication.factor"},{"location":"KafkaConfig/#transactionstatelogsegmentbytes","text":"","title":"transaction.state.log.segment.bytes"},{"location":"KafkaConfig/#transactionstatelogloadbuffersize","text":"","title":"transaction.state.log.load.buffer.size"},{"location":"KafkaConfig/#transactionstatelogminisr","text":"","title":"transaction.state.log.min.isr"},{"location":"KafkaConfig/#transactionaborttimedouttransactioncleanupintervalms","text":"","title":"transaction.abort.timed.out.transaction.cleanup.interval.ms"},{"location":"KafkaConfig/#transactionremoveexpiredtransactioncleanupintervalms","text":"","title":"transaction.remove.expired.transaction.cleanup.interval.ms"},{"location":"KafkaConfig/#processroles","text":"","title":"process.roles <p>A comma-separated list of the roles that this process plays in a Kafka cluster:</p> <p>Supported values:</p> <ul> <li><code>broker</code></li> <li><code>controller</code></li> </ul> <p>Default: (empty)</p> <ol> <li>When empty, the process requires Zookeeper (runs with Zookeeper).</li> <li>Only applicable for clusters in KRaft (Kafka Raft) mode</li> <li>If used, controller.quorum.voters must contain a parseable set of voters</li> <li>advertised.listeners config must not contain KRaft controller listeners from controller.listener.names when <code>process.roles</code> contains <code>broker</code> role because Kafka clients that send requests via advertised listeners do not send requests to KRaft controllers -- they only send requests to KRaft brokers</li> <li>If <code>process.roles</code> contains <code>controller</code> role, the node.id must be included in the set of voters controller.quorum.voters</li> <li>If <code>process.roles</code> contains just the <code>broker</code> role, the node.id must not be included in the set of voters controller.quorum.voters</li> <li>If controller.listener.names has multiple entries; only the first will be used when <code>process.roles</code> is <code>broker</code></li> <li>The advertised listeners (advertised.listeners or listeners) config must only contain KRaft controller listeners from controller.listener.names when <code>process.roles</code> is <code>controller</code></li> </ol>"},{"location":"KafkaConfig/#requesttimeoutms","text":"","title":"request.timeout.ms <p>request.timeout.ms</p>"},{"location":"KafkaServer/","text":"<p><code>KafkaServer</code> is a Server that Kafka command-line application uses in Zookeeper mode (when executed with the process.roles configuration property undefined).</p>","title":"KafkaServer"},{"location":"KafkaServer/#creating-instance","text":"<p><code>KafkaServer</code> takes the following to be created:</p> <ul> <li> KafkaConfig <li> <code>Time</code> (default: <code>SYSTEM</code>) <li> Optional Thread Name Prefix (default: undefined) <li> <code>enableForwarding</code> flag (default: <code>false</code>)  <p><code>KafkaServer</code> is created\u00a0when:</p> <ul> <li><code>Kafka</code> command-line application is launched (and builds a server)</li> </ul>","title":"Creating Instance"},{"location":"KafkaServer/#transactioncoordinator","text":"","title":"TransactionCoordinator <p><code>KafkaServer</code> creates and starts a TransactionCoordinator when created.</p> <p><code>KafkaServer</code> uses the <code>TransactionCoordinator</code> to create the following:</p> <ul> <li>data-plane and the control-plane request processors</li> <li>AutoTopicCreationManager</li> </ul> <p>The <code>TransactionCoordinator</code> is requested to shutdown along with KafkaServer.</p>"},{"location":"KafkaServer/#data-plane-request-processor","text":"","title":"Data-Plane Request Processor <p><code>KafkaServer</code> creates a KafkaApis for data-related communication.</p> <p><code>KafkaApis</code> is used to create data-plane request handler pool.</p>"},{"location":"KafkaServer/#kafkarequesthandlerpool","text":"","title":"KafkaRequestHandlerPool"},{"location":"KafkaServer/#control-plane-request-processor","text":"","title":"Control-Plane Request Processor <p><code>KafkaServer</code> creates a KafkaApis for control-related communication.</p>"},{"location":"KafkaServer/#startup","text":"","title":"startup <pre><code>startup(): Unit\n</code></pre> <p><code>startup</code> prints out the following INFO message to the logs:</p> <pre><code>starting\n</code></pre> <p><code>startup</code> initZkClient and creates a ZkConfigRepository.</p> <p><code>startup</code>...FIXME</p> <p><code>startup</code> prints out the following INFO message to the logs:</p> <pre><code>Cluster ID = [clusterId]\n</code></pre> <p><code>startup</code>...FIXME</p> <p><code>startup</code> creates a TransactionCoordinator (with the ReplicaManager) and requests it to startup.</p> <p><code>startup</code>...FIXME</p> <p><code>startup</code>\u00a0is part of the Server abstraction.</p>"},{"location":"KafkaServer/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>kafka.server.KafkaServer</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.server.KafkaServer=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"Log/","text":"","title":"Log"},{"location":"Log/#creating-instance","text":"<p><code>Log</code> takes the following to be created:</p> <ul> <li> Directory <li> <code>LogConfig</code> <li> <code>LogSegments</code> <li> logStartOffset <li> recoveryPoint <li> <code>LogOffsetMetadata</code> <li> <code>Scheduler</code> <li> <code>BrokerTopicStats</code> <li> <code>Time</code> <li> producerIdExpirationCheckIntervalMs <li> <code>TopicPartition</code> <li> Optional <code>LeaderEpochFileCache</code> <li> <code>ProducerStateManager</code> <li> <code>LogDirFailureChannel</code> <li> Optional Topic ID <li> keepPartitionMetadataFile  <p><code>Log</code> is created\u00a0using apply utility.</p>","title":"Creating Instance"},{"location":"Log/#creating-log","text":"","title":"Creating Log <pre><code>apply(\n  dir: File,\n  config: LogConfig,\n  logStartOffset: Long,\n  recoveryPoint: Long,\n  scheduler: Scheduler,\n  brokerTopicStats: BrokerTopicStats,\n  time: Time = Time.SYSTEM,\n  maxProducerIdExpirationMs: Int,\n  producerIdExpirationCheckIntervalMs: Int,\n  logDirFailureChannel: LogDirFailureChannel,\n  lastShutdownClean: Boolean = true,\n  topicId: Option[Uuid],\n  keepPartitionMetadataFile: Boolean): Log\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>LogManager</code> is requested to <code>loadLog</code> and <code>getOrCreateLog</code></li> <li><code>KafkaMetadataLog</code> is requested to <code>apply</code></li> </ul>"},{"location":"Log/#reading-messages","text":"","title":"Reading Messages <pre><code>read(\n  startOffset: Long,\n  maxLength: Int,\n  isolation: FetchIsolation,\n  minOneMessage: Boolean): FetchDataInfo\n</code></pre> <p><code>read</code> prints out the following TRACE message to the logs:</p> <pre><code>Reading maximum [maxLength] bytes at offset [startOffset] from log with total length [size] bytes\n</code></pre> <p><code>read</code>...FIXME</p> <p><code>read</code> requests the <code>LogSegment</code> to read messages.</p> <p><code>read</code>...FIXME</p> <p><code>read</code>\u00a0is used when:</p> <ul> <li><code>Partition</code> is requested to readRecords</li> <li><code>GroupMetadataManager</code> is requested to <code>doLoadGroupsAndOffsets</code></li> <li><code>TransactionStateManager</code> is requested to <code>loadTransactionMetadata</code></li> <li><code>Log</code> is requested to convertToOffsetMetadataOrThrow</li> <li><code>KafkaMetadataLog</code> is requested to <code>read</code></li> </ul>"},{"location":"Log/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>kafka.log.Log</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.log.Log=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"LogSegment/","text":"","title":"LogSegment"},{"location":"LogSegment/#creating-instance","text":"<p><code>LogSegment</code> takes the following to be created:</p> <ul> <li> <code>FileRecords</code> <li> <code>OffsetIndex</code> <li> <code>TimeIndex</code> <li> <code>TransactionIndex</code> <li> baseOffset <li> indexIntervalBytes <li> rollJitterMs <li> <code>Time</code>  <p><code>LogSegment</code> is created\u00a0using open utility.</p>","title":"Creating Instance"},{"location":"LogSegment/#opening-logsegment","text":"","title":"Opening LogSegment <pre><code>open(\n  dir: File,\n  baseOffset: Long,\n  config: LogConfig,\n  time: Time,\n  fileAlreadyExists: Boolean = false,\n  initFileSize: Int = 0,\n  preallocate: Boolean = false,\n  fileSuffix: String = \"\"): LogSegment\n</code></pre> <p><code>open</code>...FIXME</p> <p><code>open</code>\u00a0is used when:</p> <ul> <li><code>Log</code> is requested to roll a log segment and truncateFullyAndStartAt</li> <li><code>LogCleaner</code> is requested to <code>createNewCleanedSegment</code></li> <li><code>LogLoader</code> is requested to <code>load</code>, <code>loadSegmentFiles</code> and <code>recoverLog</code></li> </ul>"},{"location":"LogSegment/#reading-messages","text":"","title":"Reading Messages <pre><code>read(\n  startOffset: Long,\n  maxSize: Int,\n  maxPosition: Long = size,\n  minOneMessage: Boolean = false): FetchDataInfo\n</code></pre> <p><code>read</code>...FIXME</p> <p><code>read</code>\u00a0is used when:</p> <ul> <li><code>Log</code> is requested to read</li> <li><code>LogSegment</code> is requested to readNextOffset</li> </ul>"},{"location":"Partition/","text":"","title":"Partition"},{"location":"Partition/#readrecords","text":"","title":"readRecords <pre><code>readRecords(\n  lastFetchedEpoch: Optional[Integer],\n  fetchOffset: Long,\n  currentLeaderEpoch: Optional[Integer],\n  maxBytes: Int,\n  fetchIsolation: FetchIsolation,\n  fetchOnlyFromLeader: Boolean,\n  minOneMessage: Boolean): LogReadInfo\n</code></pre> <p><code>readRecords</code>...FIXME</p> <p>In the end, <code>readRecords</code> requests the <code>Log</code> to read messages.</p> <p><code>readRecords</code>\u00a0is used when:</p> <ul> <li><code>ReplicaManager</code> is requested to readFromLocalLog</li> </ul>"},{"location":"PartitionData/","text":"<p><code>PartitionData</code> is a <code>Message</code> of FetchResponseData.</p>","title":"PartitionData"},{"location":"PartitionData/#abortedtransactions","text":"","title":"abortedTransactions <pre><code>List&lt;AbortedTransaction&gt; abortedTransactions\nList&lt;AbortedTransaction&gt; abortedTransactions()\n</code></pre> <p><code>abortedTransactions</code>...FIXME</p> <p><code>abortedTransactions</code>\u00a0is used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"PartitionData/#preferred-read-replica","text":"","title":"Preferred Read Replica <p>Default: <code>-1</code></p> <p><code>preferredReadReplica</code>\u00a0is used when:</p> <ul> <li><code>Fetcher</code> is requested to initializeCompletedFetch</li> </ul>"},{"location":"ReplicaAlterLogDirsThread/","text":"<p><code>ReplicaAlterLogDirsThread</code> is...FIXME</p>","title":"ReplicaAlterLogDirsThread"},{"location":"ReplicaManager/","text":"","title":"ReplicaManager"},{"location":"ReplicaManager/#fetchmessages","text":"","title":"fetchMessages <pre><code>fetchMessages(\n  timeout: Long,\n  replicaId: Int,\n  fetchMinBytes: Int,\n  fetchMaxBytes: Int,\n  hardMaxBytesLimit: Boolean,\n  fetchInfos: Seq[(TopicPartition, PartitionData)],\n  quota: ReplicaQuota,\n  responseCallback: Seq[(TopicPartition, FetchPartitionData)] =&gt; Unit,\n  isolationLevel: IsolationLevel,\n  clientMetadata: Option[ClientMetadata]): Unit\n</code></pre> <p><code>fetchMessages</code> determines whether the request comes from a follower or a consumer (based on the given <code>replicaId</code>).</p> <p><code>fetchMessages</code> determines <code>FetchIsolation</code>:</p> <ul> <li><code>FetchLogEnd</code> if the request comes from a follower</li> <li><code>FetchTxnCommitted</code> if the request comes from a consumer with <code>READ_COMMITTED</code> isolation level</li> <li><code>FetchHighWatermark</code> otherwise</li> </ul> <p><code>fetchMessages</code> readFromLocalLog (passing in the <code>FetchIsolation</code> among the others).</p> <p><code>fetchMessages</code>...FIXME</p> <p><code>fetchMessages</code>\u00a0is used when:</p> <ul> <li><code>KafkaApis</code> is requested to handle a Fetch request</li> <li><code>ReplicaAlterLogDirsThread</code> is requested to fetchFromLeader</li> </ul>"},{"location":"ReplicaManager/#readfromlocallog","text":"","title":"readFromLocalLog <pre><code>readFromLocalLog(\n  replicaId: Int,\n  fetchOnlyFromLeader: Boolean,\n  fetchIsolation: FetchIsolation,\n  fetchMaxBytes: Int,\n  hardMaxBytesLimit: Boolean,\n  readPartitionInfo: Seq[(TopicPartition, PartitionData)],\n  quota: ReplicaQuota,\n  clientMetadata: Option[ClientMetadata]): Seq[(TopicPartition, LogReadResult)]\n</code></pre> <p><code>readFromLocalLog</code>...FIXME</p> <p><code>readFromLocalLog</code> finds the <code>Partition</code> and requests it to readRecords.</p> <p><code>readFromLocalLog</code>...FIXME</p> <p><code>readFromLocalLog</code>\u00a0is used when:</p> <ul> <li><code>DelayedFetch</code> is requested to <code>onComplete</code></li> <li><code>ReplicaManager</code> is requested to fetchMessages</li> </ul>"},{"location":"RequestHandlerHelper/","text":"","title":"RequestHandlerHelper"},{"location":"RequestHandlerHelper/#creating-instance","text":"<p><code>RequestHandlerHelper</code> takes the following to be created:</p> <ul> <li> <code>RequestChannel</code> <li> <code>QuotaManagers</code> <li> <code>Time</code> <li> Log Prefix  <p><code>RequestHandlerHelper</code> is created\u00a0when:</p> <ul> <li><code>ControllerApis</code> is created (<code>requestHelper</code>)</li> <li><code>KafkaApis</code> is created</li> </ul>","title":"Creating Instance"},{"location":"RequestHandlerHelper/#onleadershipchange","text":"","title":"onLeadershipChange <pre><code>onLeadershipChange(\n  groupCoordinator: GroupCoordinator,\n  txnCoordinator: TransactionCoordinator,\n  updatedLeaders: Iterable[Partition],\n  updatedFollowers: Iterable[Partition]): Unit\n</code></pre> <p><code>onLeadershipChange</code>...FIXME</p> <p><code>onLeadershipChange</code>\u00a0is used when:</p> <ul> <li><code>BrokerServer</code> is requested to start up</li> <li><code>KafkaApis</code> is requested to handleLeaderAndIsrRequest</li> <li><code>BrokerMetadataListener</code> is requested to <code>handleCommits</code> and <code>execCommits</code></li> </ul>"},{"location":"Serde/","text":"<p><code>Serde</code> is an abstraction of wrappers with Serializers and Deserializers.</p>  <p>Note</p> <p><code>Serde</code> seems to be of more use in libraries like Kafka Streams or Kafka Connect (that in the Kafka Core).</p>","title":"Serde"},{"location":"Serde/#contract","text":"","title":"Contract"},{"location":"Serde/#configure","text":"","title":"configure <pre><code>void configure(\n  Map&lt;String, ?&gt; configs,\n  boolean isKey)\n</code></pre>"},{"location":"Serde/#deserializer","text":"","title":"deserializer <pre><code>Deserializer&lt;T&gt; deserializer()\n</code></pre>"},{"location":"Serde/#serializer","text":"","title":"serializer <pre><code>Serializer&lt;T&gt; serializer()\n</code></pre>"},{"location":"Serde/#implementations","text":"<ul> <li>WrapperSerde</li> </ul>","title":"Implementations"},{"location":"Serdes/","text":"<p><code>Serdes</code> is a utility with the serializers and deserializers for many built-in types in Java and allows defining new ones.</p> <pre><code>import org.apache.kafka.common.serialization.Serdes\n\nval longSerde = Serdes.Long\nscala&gt; :type longSerde\norg.apache.kafka.common.serialization.Serde[Long]\n\nscala&gt; :type longSerde.serializer\norg.apache.kafka.common.serialization.Serializer[Long]\n\nscala&gt; :type longSerde.deserializer\norg.apache.kafka.common.serialization.Deserializer[Long]\n</code></pre>","title":"Serdes Utility"},{"location":"Serdes/#wrapperserde","text":"","title":"WrapperSerde <p><code>Serdes</code> defines a <code>static public class WrapperSerde&lt;T&gt;</code> that is a Serde from and to <code>T</code> values.</p> <ul> <li><code>ShortSerde</code></li> <li><code>BytesSerde</code></li> <li><code>IntegerSerde</code></li> <li><code>ListSerde</code></li> <li><code>UUIDSerde</code></li> <li><code>FloatSerde</code></li> <li><code>FullTimeWindowedSerde</code> (Kafka Streams)</li> <li><code>VoidSerde</code></li> <li><code>LongSerde</code></li> <li><code>DoubleSerde</code></li> <li><code>ByteArraySerde</code></li> <li><code>TimeWindowedSerde</code> (Kafka Streams)</li> <li><code>SessionWindowedSerde</code> (Kafka Streams)</li> <li><code>ByteBufferSerde</code></li> <li><code>StringSerde</code></li> </ul>"},{"location":"Serdes/#serdefrom","text":"","title":"serdeFrom <pre><code>Serde&lt;T&gt; serdeFrom(\n  Class&lt;T&gt; type)\nSerde&lt;T&gt; serdeFrom(\n  Serializer&lt;T&gt; serializer,\n  Deserializer&lt;T&gt; deserializer)\n</code></pre> <p><code>serdeFrom</code> looks up the Serde for a given type (if supported) or creates a new one based on the given pair of <code>Serializer</code> and <code>Deserializer</code>.</p>"},{"location":"Server/","text":"<p><code>Server</code> is an abstraction of Kafka servers (brokers).</p>","title":"Server"},{"location":"Server/#contract","text":"","title":"Contract"},{"location":"Server/#awaitshutdown","text":"","title":"awaitShutdown <pre><code>awaitShutdown(): Unit\n</code></pre> <p>Used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"Server/#startup","text":"","title":"startup <pre><code>startup(): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>Kafka</code> utility is executed (on command line)</li> </ul>"},{"location":"Server/#shutdown","text":"","title":"shutdown <pre><code>shutdown(): Unit\n</code></pre> <p>Used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"Server/#implementations","text":"<ul> <li>KafkaRaftServer</li> <li>KafkaServer</li> </ul>","title":"Implementations"},{"location":"Utils/","text":"","title":"Utils"},{"location":"Utils/#murmur2","text":"","title":"murmur2 <pre><code>int murmur2(\n  byte[] data)\n</code></pre> <p><code>murmur2</code> generates a 32-bit murmur2 hash for the given byte array.</p> <p><code>murmur2</code>\u00a0is used when:</p> <ul> <li><code>DefaultPartitioner</code> is requested to compute a partition for a record</li> </ul>"},{"location":"Utils/#demo","text":"","title":"Demo <pre><code>import org.apache.kafka.common.utils.Utils\n\nval keyBytes = \"hello\".getBytes\nval hash = Utils.murmur2(keyBytes)\n\nprintln(hash)\n</code></pre>"},{"location":"Utils/#topositive","text":"","title":"toPositive <pre><code>int toPositive(\n  int number)\n</code></pre> <p><code>toPositive</code> converts a number to a positive value.</p>"},{"location":"building-from-sources/","text":"<p>Based on README.md:</p> <pre><code>KAFKA_VERSION=3.2.1\nSCALA_VERSION=2.13\n</code></pre> <pre><code>$ java -version\nopenjdk version \"11.0.12\" 2021-07-20\nOpenJDK Runtime Environment Temurin-11.0.12+7 (build 11.0.12+7)\nOpenJDK 64-Bit Server VM Temurin-11.0.12+7 (build 11.0.12+7, mixed mode)\n</code></pre> <pre><code>./gradlew clean releaseTarGz install -PskipSigning=true &amp;&amp; \\\ntar -zxvf core/build/distributions/kafka_$SCALA_VERSION-$KAFKA_VERSION.tgz\n</code></pre> <pre><code>cd kafka_$SCALA_VERSION-$KAFKA_VERSION\n</code></pre> <pre><code>$ ./bin/kafka-server-start.sh --version | tail -1\n3.0.0 (Commit:8cb0a5e9d3441962)\n</code></pre>","title":"Building from Sources"},{"location":"logging/","text":"","title":"Logging"},{"location":"logging/#brokers","text":"<p>Kafka brokers use Apache Log4j 2 for logging and use <code>config/log4j.properties</code> by default.</p> <p>The default logging level is <code>INFO</code> with <code>stdout</code> appender.</p> <pre><code>log4j.rootLogger=INFO, stdout, kafkaAppender\n\nlog4j.logger.kafka=INFO\nlog4j.logger.org.apache.kafka=INFO\n</code></pre>","title":"Brokers"},{"location":"logging/#tools","text":"<p>Kafka tools (e.g. <code>kafka-console-producer</code>) use <code>config/tools-log4j.properties</code> as the logging configuration file.</p>","title":"Tools"},{"location":"logging/#clients","text":"","title":"Clients"},{"location":"logging/#buildsbt","text":"<pre><code>libraryDependencies += \"org.apache.kafka\" % \"kafka-clients\" % \"2.8.0\"\n\nval slf4j = \"1.7.32\"\nlibraryDependencies += \"org.slf4j\" % \"slf4j-api\" % slf4j\nlibraryDependencies += \"org.slf4j\" % \"slf4j-log4j12\" % slf4j\n</code></pre>","title":"build.sbt"},{"location":"logging/#log4jproperties","text":"<p>In <code>src/main/resources/log4j.properties</code> use the following:</p> <pre><code>log4j.rootLogger=INFO, stdout\n\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n\n\nlog4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL\n</code></pre>","title":"log4j.properties"},{"location":"overview/","text":"<p>Apache Kafka is an open source project for a distributed publish-subscribe messaging system rethought as a distributed commit log.</p>","title":"Apache Kafka"},{"location":"overview/#messages","text":"<p>Messages (records, events) are byte arrays (String, JSON, and Avro are among the most common formats). If a message has a key, Kafka (uses Partitioner) to make sure that all messages of the same key are in the same partition.</p>","title":"Messages"},{"location":"overview/#topics","text":"<p>Kafka stores messages in topics that are partitioned and replicated across multiple brokers in a cluster.</p>","title":"Topics"},{"location":"overview/#kafka-clients","text":"<p>Producers send messages to topics from which consumers read.</p>","title":"Kafka Clients"},{"location":"overview/#language-agnostic","text":"<p>Kafka clients use binary protocol to talk to a Kafka cluster.</p>","title":"Language Agnostic"},{"location":"overview/#consumer-groups","text":"<p>Consumers may be grouped in a consumer group with multiple consumers. Each consumer in a consumer group will read messages from a unique subset of partitions in each topic they subscribe to. Each message is delivered to one consumer in the group, and all messages with the same key arrive to the same consumer.</p>","title":"Consumer Groups"},{"location":"overview/#durability","text":"<p>Kafka does not track which messages were read by consumers. Kafka keeps all messages for a finite amount of time, and it is consumers' responsibility to track their location per topic (offsets).</p>","title":"Durability"},{"location":"clients/","text":"","title":"Kafka Clients"},{"location":"clients/CommonClientConfigs/","text":"","title":"CommonClientConfigs"},{"location":"clients/CommonClientConfigs/#clientid","text":"","title":"client.id"},{"location":"clients/CommonClientConfigs/#groupid","text":"","title":"group.id <p>A unique identifier of the consumer group a consumer belongs to. Required if the consumer uses either the group management functionality by using Consumer.subscribe or the Kafka-based offset management strategy.</p> <p>Default: (undefined)</p>"},{"location":"clients/CommonClientConfigs/#retries","text":"","title":"retries"},{"location":"clients/CommonClientConfigs/#retrybackoffms","text":"","title":"retry.backoff.ms"},{"location":"clients/CommonClientConfigs/#requesttimeoutms","text":"","title":"request.timeout.ms"},{"location":"clients/KafkaClient/","text":"<p><code>KafkaClient</code> is an interface to NetworkClient.</p>","title":"KafkaClient"},{"location":"clients/KafkaClient/#contract","text":"","title":"Contract"},{"location":"clients/KafkaClient/#inflightrequestcount","text":"","title":"inFlightRequestCount <pre><code>int inFlightRequestCount()\nint inFlightRequestCount(\n  String nodeId)\n</code></pre> <p>Used when:</p> <ul> <li><code>ConsumerNetworkClient</code> is requested to pendingRequestCount and poll</li> <li><code>Sender</code> is requested to run</li> <li><code>SenderMetrics</code> is requested for <code>requests-in-flight</code> performance metric</li> </ul>"},{"location":"clients/KafkaClient/#leastloadednode","text":"","title":"leastLoadedNode <pre><code>Node leastLoadedNode(\n  long now)\n</code></pre> <p>Used when:</p> <ul> <li><code>ConsumerNetworkClient</code> is requested for the leastLoadedNode</li> <li><code>DefaultMetadataUpdater</code> is requested to <code>maybeUpdate</code></li> <li><code>KafkaAdminClient</code> is used</li> <li><code>Sender</code> is requested for the maybeSendAndPollTransactionalRequest</li> </ul>"},{"location":"clients/KafkaClient/#newclientrequest","text":"","title":"newClientRequest <pre><code>ClientRequest newClientRequest(\n  String nodeId,\n  AbstractRequest.Builder&lt;?&gt; requestBuilder,\n  long createdTimeMs,\n  boolean expectResponse)\nClientRequest newClientRequest(\n  String nodeId,\n  AbstractRequest.Builder&lt;?&gt; requestBuilder,\n  long createdTimeMs,\n  boolean expectResponse,\n  int requestTimeoutMs,\n  RequestCompletionHandler callback)\n</code></pre> <p>Used when:</p> <ul> <li><code>AdminClientRunnable</code> is requested to <code>sendEligibleCalls</code></li> <li><code>ConsumerNetworkClient</code> is requested to send</li> <li><code>NetworkClient</code> is requested to newClientRequest, sendInternalMetadataRequest and handleInitiateApiVersionRequests</li> <li><code>RequestSendThread</code> is requested to <code>doWork</code></li> <li><code>Sender</code> is requested to run</li> <li><code>KafkaServer</code> is requested to <code>controlledShutdown</code></li> <li><code>ReplicaFetcherBlockingSend</code> is requested to <code>sendRequest</code></li> <li>ReplicaVerificationTool is used</li> </ul>"},{"location":"clients/KafkaClient/#poll","text":"","title":"poll <pre><code>List&lt;ClientResponse&gt; poll(\n  long timeout,\n  long now)\n</code></pre> <p>Used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"clients/KafkaClient/#polldelayms","text":"","title":"pollDelayMs <pre><code>long pollDelayMs(\n  Node node,\n  long now)\n</code></pre> <p>Used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"clients/KafkaClient/#is-node-ready-and-connected","text":"","title":"Is Node Ready and Connected <pre><code>boolean ready(\n  Node node,\n  long now);\n</code></pre> <p>Used when:</p> <ul> <li><code>AdminClientRunnable</code> is requested to sendEligibleCalls</li> <li><code>ConsumerNetworkClient</code> is requested to tryConnect and trySend</li> <li><code>InterBrokerSendThread</code> is requested to sendRequests</li> <li><code>NetworkClientUtils</code> is requested to awaitReady</li> <li><code>Sender</code> is requested to sendProducerData</li> </ul>"},{"location":"clients/KafkaClient/#send","text":"","title":"send <pre><code>void send(\n  ClientRequest request,\n  long now)\n</code></pre> <p>Used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"clients/KafkaClient/#wakeup","text":"","title":"wakeup <pre><code>void wakeup()\n</code></pre> <p>Used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"clients/KafkaClient/#implementations","text":"<ul> <li>NetworkClient</li> </ul>","title":"Implementations"},{"location":"clients/KafkaClient/#closeable","text":"","title":"Closeable <p><code>KafkaClient</code> is a <code>Closeable</code> (Java).</p>"},{"location":"clients/Metadata/","text":"","title":"Metadata"},{"location":"clients/Metadata/#update","text":"","title":"update <pre><code>void update(\n  int requestVersion,\n  MetadataResponse response,\n  boolean isPartialUpdate,\n  long nowMs)\n</code></pre> <p><code>update</code>...FIXME</p> <p><code>update</code>\u00a0is used when:</p> <ul> <li><code>ProducerMetadata</code> is requested to <code>update</code></li> <li><code>Metadata</code> is requested to updateWithCurrentRequestVersion</li> <li><code>DefaultMetadataUpdater</code> is requested to <code>handleSuccessfulResponse</code></li> </ul>"},{"location":"clients/MetadataUpdater/","text":"<p><code>MetadataUpdater</code> is...FIXME</p>","title":"MetadataUpdater"},{"location":"clients/NetworkClient/","text":"<p><code>NetworkClient</code> is a KafkaClient.</p>","title":"NetworkClient"},{"location":"clients/NetworkClient/#leastloadednode","text":"","title":"leastLoadedNode <pre><code>Node leastLoadedNode(\n  long now)\n</code></pre> <p><code>leastLoadedNode</code> requests the MetadataUpdater for the nodes (in a non-blocking fashion).</p> <p><code>leastLoadedNode</code> generates a random number to offset the first node to start checking node candidates from.</p> <p><code>leastLoadedNode</code> finds three nodes:</p> <ol> <li><code>foundReady</code> that is ready (perhaps with some in-flight requests)</li> <li><code>foundConnecting</code> with a connection already being established (using the <code>ClusterConnectionStates</code> registry)</li> <li><code>foundCanConnect</code> that can be connected</li> </ol> <p>When a node is found that is ready and has no in-flight requests, <code>leastLoadedNode</code> prints out the following TRACE message to the logs and returns the node immediately:</p> <pre><code>Found least loaded node [node] connected with no in-flight requests\n</code></pre> <p>When a node candidate does not meet any of the above requirements, <code>leastLoadedNode</code> prints out the following TRACE message to the logs:</p> <pre><code>Removing node [node] from least loaded node selection since it is neither ready for sending nor connecting\n</code></pre> <p><code>leastLoadedNode</code> prefers the <code>foundReady</code> node over the <code>foundConnecting</code> with the <code>foundCanConnect</code> as the last resort.</p> <p>When no node could be found, <code>leastLoadedNode</code> prints out the following TRACE message to the logs (and returns <code>null</code>):</p> <pre><code>Least loaded node selection failed to find an available node\n</code></pre> <p><code>leastLoadedNode</code>\u00a0is part of the KafkaClient abstraction.</p>"},{"location":"clients/NetworkClientUtils/","text":"","title":"NetworkClientUtils"},{"location":"clients/admin/","text":"","title":"Kafka Admin"},{"location":"clients/admin/AdminClientRunnable/","text":"","title":"AdminClientRunnable"},{"location":"clients/consumer/","text":"<p>KafkaConsumer uses Fetcher to fetch records from a Kafka cluster. One could say that <code>KafkaConsumer</code> is a developer-oriented interface to <code>Fetcher</code>.</p> <p><code>KafkaConsumer</code> is assigned a <code>IsolationLevel</code> based on isolation.level configuration property.</p>","title":"Kafka Consumers"},{"location":"clients/consumer/AbstractCoordinator/","text":"<p><code>AbstractCoordinator</code> is an abstraction of consumer group coordination manager.</p>","title":"AbstractCoordinator"},{"location":"clients/consumer/AbstractCoordinator/#contract","text":"","title":"Contract"},{"location":"clients/consumer/AbstractCoordinator/#metadata","text":"","title":"metadata <pre><code>JoinGroupRequestData.JoinGroupRequestProtocolCollection metadata()\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractCoordinator</code> is requested to ensureActiveGroup (and sendJoinGroupRequest)</li> </ul>"},{"location":"clients/consumer/AbstractCoordinator/#onjoincomplete","text":"","title":"onJoinComplete <pre><code>void onJoinComplete(\n  int generation,\n  String memberId,\n  String protocol,\n  ByteBuffer memberAssignment)\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractCoordinator</code> is requested to ensureActiveGroup (and joinGroupIfNeeded)</li> </ul>"},{"location":"clients/consumer/AbstractCoordinator/#onjoinprepare","text":"","title":"onJoinPrepare <pre><code>void onJoinPrepare(\n  int generation,\n  String memberId)\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractCoordinator</code> is requested to ensureActiveGroup (and joinGroupIfNeeded)</li> </ul>"},{"location":"clients/consumer/AbstractCoordinator/#performassignment","text":"","title":"performAssignment <pre><code>Map&lt;String, ByteBuffer&gt; performAssignment(\n  String leaderId,\n  String protocol,\n  List&lt;JoinGroupResponseData.JoinGroupResponseMember&gt; allMemberMetadata)\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractCoordinator</code> is requested to handle a JoinGroup response (having joined the group as the leader)</li> </ul>"},{"location":"clients/consumer/AbstractCoordinator/#protocoltype","text":"","title":"protocolType <pre><code>String protocolType()\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractCoordinator</code> is requested to sendJoinGroupRequest, onJoinFollower, onJoinLeader, isProtocolTypeInconsistent</li> </ul>"},{"location":"clients/consumer/AbstractCoordinator/#implementations","text":"<ul> <li>ConsumerCoordinator</li> <li><code>WorkerCoordinator</code> (Kafka Connect)</li> </ul>","title":"Implementations"},{"location":"clients/consumer/AbstractCoordinator/#handling-joingroup-response","text":"","title":"Handling JoinGroup Response"},{"location":"clients/consumer/AbstractCoordinator/#group-leader","text":"","title":"Group Leader <pre><code>RequestFuture&lt;ByteBuffer&gt; onJoinLeader(\n  JoinGroupResponse joinResponse)\n</code></pre> <p><code>onJoinLeader</code> performAssignment (with the leader ID, the group protocol and the members).</p> <p><code>onJoinLeader</code> prints out the following DEBUG message to the logs:</p> <pre><code>Sending leader SyncGroup to coordinator [coordinator] at generation [generation]: [SyncGroupRequest]\n</code></pre> <p>In the end, <code>onJoinLeader</code> sends a <code>SyncGroupRequest</code> to the coordinator.</p> <p><code>onJoinLeader</code>\u00a0is used when:</p> <ul> <li>JoinGroupResponseHandler is requested to handle a <code>JoinGroup</code> response (after joining group successfully as the leader)</li> </ul>"},{"location":"clients/consumer/AbstractCoordinator/#group-follower","text":"","title":"Group Follower <pre><code>RequestFuture&lt;ByteBuffer&gt; onJoinFollower()\n</code></pre> <p><code>onJoinFollower</code>...FIXME</p> <p><code>onJoinFollower</code>\u00a0is used when:</p> <ul> <li>JoinGroupResponseHandler is requested to handle a <code>JoinGroup</code> response (after joining group successfully as a follower)</li> </ul>"},{"location":"clients/consumer/AbstractCoordinator/#ensureactivegroup","text":"","title":"ensureActiveGroup <pre><code>void ensureActiveGroup()\nboolean ensureActiveGroup(\n  Timer timer)\n</code></pre> <p><code>ensureActiveGroup</code> ensureCoordinatorReady (and returns <code>false</code> if not).</p> <p><code>ensureActiveGroup</code>...FIXME</p> <p><code>ensureActiveGroup</code>\u00a0is used when:</p> <ul> <li><code>ConsumerCoordinator</code> is requested to poll</li> <li><code>WorkerCoordinator</code> (Kafka Connect) is requested to <code>poll</code></li> </ul>"},{"location":"clients/consumer/AbstractCoordinator/#joingroupifneeded","text":"","title":"joinGroupIfNeeded <pre><code>boolean joinGroupIfNeeded(\n  Timer timer)\n</code></pre> <p><code>joinGroupIfNeeded</code>...FIXME</p>"},{"location":"clients/consumer/AbstractCoordinator/#initiatejoingroup","text":"","title":"initiateJoinGroup <pre><code>RequestFuture&lt;ByteBuffer&gt; initiateJoinGroup()\n</code></pre> <p><code>initiateJoinGroup</code>...FIXME</p>"},{"location":"clients/consumer/AbstractCoordinator/#sendjoingrouprequest","text":"","title":"sendJoinGroupRequest <pre><code>RequestFuture&lt;ByteBuffer&gt; sendJoinGroupRequest()\n</code></pre> <p><code>sendJoinGroupRequest</code>...FIXME</p>"},{"location":"clients/consumer/AbstractCoordinator/#waiting-for-consumer-group-coordinator-known-and-ready","text":"","title":"Waiting for Consumer Group Coordinator Known and Ready <pre><code>boolean ensureCoordinatorReady(\n  Timer timer)\n</code></pre> <p><code>ensureCoordinatorReady</code> returns <code>true</code> immediately when the consumer group coordinator is known and available.</p> <p>Otherwise, <code>ensureCoordinatorReady</code> keeps looking up the group coordinator (by sending <code>FindCoordinator</code> requests to the least loaded broker) until the coordinator is available or the timer timed out.</p> <p>In the end, <code>ensureCoordinatorReady</code> returns whether the coordinator is known and available or not.</p> <p><code>ensureCoordinatorReady</code>\u00a0is used when:</p> <ul> <li><code>AbstractCoordinator</code> is requested to ensureActiveGroup (and joinGroupIfNeeded)</li> <li><code>ConsumerCoordinator</code> is requested to poll, fetchCommittedOffsets, close, commitOffsetsSync</li> </ul>"},{"location":"clients/consumer/AbstractCoordinator/#joingroupresponsehandler","text":"","title":"JoinGroupResponseHandler <p><code>JoinGroupResponseHandler</code> is a <code>CoordinatorResponseHandler</code> to handle responses from the group coordinator after sendJoinGroupRequest.</p>"},{"location":"clients/consumer/AbstractPartitionAssignor/","text":"<p><code>AbstractPartitionAssignor</code> is...FIXME</p>","title":"AbstractPartitionAssignor"},{"location":"clients/consumer/Consumer/","text":"<p><code>Consumer&lt;K, V&gt;</code> is an interface to KafkaConsumer for Kafka developers to use to consume records (with <code>K</code> keys and <code>V</code> values) from a Kafka cluster.</p>","title":"Consumer"},{"location":"clients/consumer/Consumer/#contract-subset","text":"","title":"Contract (Subset)"},{"location":"clients/consumer/Consumer/#enforcerebalance","text":"","title":"enforceRebalance <pre><code>void enforceRebalance()\n</code></pre>"},{"location":"clients/consumer/Consumer/#groupmetadata","text":"","title":"groupMetadata <pre><code>ConsumerGroupMetadata groupMetadata()\n</code></pre>"},{"location":"clients/consumer/Consumer/#subscribing-to-topics","text":"","title":"Subscribing to Topics <pre><code>void subscribe(\n  Collection&lt;String&gt; topics)\nvoid subscribe(\n  Collection&lt;String&gt; topics,\n  ConsumerRebalanceListener callback)\nvoid subscribe(\n  Pattern pattern)\nvoid subscribe(\n  Pattern pattern,\n  ConsumerRebalanceListener callback)\n</code></pre>"},{"location":"clients/consumer/Consumer/#waking-up","text":"","title":"Waking Up <pre><code>void wakeup()\n</code></pre>"},{"location":"clients/consumer/ConsumerConfig/","text":"","title":"ConsumerConfig"},{"location":"clients/consumer/ConsumerConfig/#autocommitintervalms","text":"","title":"auto.commit.interval.ms"},{"location":"clients/consumer/ConsumerConfig/#autooffsetreset","text":"","title":"auto.offset.reset <p>What to do when there is no initial offset in Kafka or if the current offset does not exist anymore on a broker (e.g. because that data has been deleted)</p> <p>Default: <code>latest</code></p> <p>Supported values:</p> <ul> <li><code>latest</code> - reset the offset to the latest offset</li> <li><code>earliest</code> - reset the offset to the earliest offset</li> <li><code>none</code> - throw exception to the consumer if no previous offset is found for the consumer's group</li> </ul>"},{"location":"clients/consumer/ConsumerConfig/#checkcrcs","text":"","title":"check.crcs"},{"location":"clients/consumer/ConsumerConfig/#clientrack","text":"","title":"client.rack"},{"location":"clients/consumer/ConsumerConfig/#enableautocommit","text":"","title":"enable.auto.commit <p>Controls whether consumer offsets should be periodically committed in the background or not</p> <p>Default: <code>true</code></p> <p>kafka-console-consumer supports the property using <code>--consumer-property</code> or <code>--consumer.config</code> options.</p> <p>Used when:</p> <ul> <li><code>ConsumerConfig</code> is requested to maybeOverrideEnableAutoCommit</li> </ul>"},{"location":"clients/consumer/ConsumerConfig/#fetchmaxbytes","text":"","title":"fetch.max.bytes"},{"location":"clients/consumer/ConsumerConfig/#fetchmaxwaitms","text":"","title":"fetch.max.wait.ms"},{"location":"clients/consumer/ConsumerConfig/#fetchminbytes","text":"","title":"fetch.min.bytes"},{"location":"clients/consumer/ConsumerConfig/#groupid","text":"","title":"group.id <p>See CommonClientConfigs</p>"},{"location":"clients/consumer/ConsumerConfig/#internalthrowonfetchstableoffsetunsupported","text":"","title":"internal.throw.on.fetch.stable.offset.unsupported"},{"location":"clients/consumer/ConsumerConfig/#isolationlevel","text":"","title":"isolation.level <p>Controls how KafkaConsumer should read messages written transactionally</p> <p>Default: <code>read_uncommitted</code></p> <p>Supported values:</p> <ul> <li> <p><code>read_uncommitted</code> - Consumer.poll() will only return transactional messages which have been committed (filtering out transactional messages which are not committed).</p> </li> <li> <p><code>read_committed</code> - Consumer.poll() will return all messages, even transactional messages which have been not committed yet or even aborted.</p> </li> </ul> <p>Non-transactional messages will be returned unconditionally in either mode.</p> <p>Messages will always be returned in offset order. Hence, in <code>read_committed</code> mode, <code>consumer.poll()</code> will only return messages up to the last stable offset (LSO), which is the one less than the offset of the first open transaction.</p> <p>In particular any messages appearing after messages belonging to ongoing transactions will be withheld until the relevant transaction has been completed.</p> <p>As a result, <code>read_committed</code> consumers will not be able to read up to the high watermark when there are in-flight transactions.</p> <p>Further, when in <code>read_committed</code> the <code>seekToEnd</code> method will return the last stable offset.</p> <p>kafka-console-consumer supports the property using <code>--isolation-level</code> option.</p>"},{"location":"clients/consumer/ConsumerConfig/#maxpartitionfetchbytes","text":"","title":"max.partition.fetch.bytes"},{"location":"clients/consumer/ConsumerConfig/#maxpollrecords","text":"","title":"max.poll.records"},{"location":"clients/consumer/ConsumerConfig/#requesttimeoutms","text":"","title":"request.timeout.ms"},{"location":"clients/consumer/ConsumerConfig/#retrybackoffms","text":"","title":"retry.backoff.ms"},{"location":"clients/consumer/ConsumerConfig/#maybeoverrideenableautocommit","text":"","title":"maybeOverrideEnableAutoCommit <pre><code>boolean maybeOverrideEnableAutoCommit()\n</code></pre> <p><code>maybeOverrideEnableAutoCommit</code> returns <code>false</code> when neither group.id nor enable.auto.commit are specified. Otherwise, <code>maybeOverrideEnableAutoCommit</code> returns the value of enable.auto.commit configuration property.</p> <p><code>maybeOverrideEnableAutoCommit</code> throws an <code>InvalidConfigurationException</code> when no group.id is given with enable.auto.commit enabled:</p> <pre><code>enable.auto.commit cannot be set to true when default group id (null) is used.\n</code></pre> <p><code>maybeOverrideEnableAutoCommit</code>\u00a0is used when:</p> <ul> <li><code>KafkaConsumer</code> is created (and creates a ConsumerCoordinator)</li> </ul>"},{"location":"clients/consumer/ConsumerCoordinator/","text":"<p><code>ConsumerCoordinator</code> is a consumer group coordination manager.</p>","title":"ConsumerCoordinator"},{"location":"clients/consumer/ConsumerCoordinator/#creating-instance","text":"<p><code>ConsumerCoordinator</code> takes the following to be created:</p> <ul> <li> <code>GroupRebalanceConfig</code> <li> <code>LogContext</code> <li> ConsumerNetworkClient <li> ConsumerPartitionAssignors <li> ConsumerMetadata <li> SubscriptionState <li> <code>Metrics</code> <li> Metrics Group Prefix <li> <code>Time</code> <li>autoCommitEnabled</li> <li> auto.commit.interval.ms <li> <code>ConsumerInterceptors</code> <li> internal.throw.on.fetch.stable.offset.unsupported  <p>While being created, <code>ConsumerCoordinator</code> requests the ConsumerMetadata for an update of the current cluster metadata.</p> <p><code>ConsumerCoordinator</code> is created\u00a0when:</p> <ul> <li><code>KafkaConsumer</code> is created (and group.id configuration property is specified)</li> </ul>","title":"Creating Instance"},{"location":"clients/consumer/ConsumerCoordinator/#autocommitenabled","text":"","title":"autoCommitEnabled <p><code>ConsumerCoordinator</code> is given <code>autoCommitEnabled</code> flag when created with the value based on group.id and enable.auto.commit configuration properties.</p>"},{"location":"clients/consumer/ConsumerCoordinator/#metadata","text":"","title":"metadata <pre><code>JoinGroupRequestData.JoinGroupRequestProtocolCollection metadata()\n</code></pre> <p><code>metadata</code>...FIXME</p> <p><code>metadata</code>\u00a0is part of the AbstractCoordinator abstraction.</p>"},{"location":"clients/consumer/ConsumerMetadata/","text":"<p><code>ConsumerMetadata</code> is...FIXME</p>","title":"ConsumerMetadata"},{"location":"clients/consumer/ConsumerNetworkClient/","text":"","title":"ConsumerNetworkClient"},{"location":"clients/consumer/ConsumerPartitionAssignor/","text":"<p><code>ConsumerPartitionAssignor</code> is an abstraction of partition assignors.</p>","title":"ConsumerPartitionAssignor"},{"location":"clients/consumer/ConsumerPartitionAssignor/#contract","text":"","title":"Contract"},{"location":"clients/consumer/ConsumerPartitionAssignor/#assign","text":"","title":"assign <pre><code>GroupAssignment assign(\n  Cluster metadata,\n  GroupSubscription groupSubscription)\n</code></pre> <p>Used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"clients/consumer/ConsumerPartitionAssignor/#name","text":"","title":"name <pre><code>String name()\n</code></pre> <p>Used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"clients/consumer/ConsumerPartitionAssignor/#implementations","text":"<ul> <li>AbstractPartitionAssignor</li> <li><code>StreamsPartitionAssignor</code> (Kafka Streams)</li> </ul>","title":"Implementations"},{"location":"clients/consumer/ConsumerPartitionAssignor/#onassignment","text":"","title":"onAssignment <pre><code>void onAssignment(\n  Assignment assignment,\n  ConsumerGroupMetadata metadata)\n</code></pre> <p><code>onAssignment</code>...FIXME</p> <p><code>onAssignment</code> is used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"clients/consumer/ConsumerPartitionAssignor/#supportedprotocols","text":"","title":"supportedProtocols <pre><code>List&lt;RebalanceProtocol&gt; supportedProtocols()\n</code></pre> <p>Default: <code>RebalanceProtocol.EAGER</code></p> <p><code>supportedProtocols</code> is used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"clients/consumer/ConsumerPartitionAssignor/#subscriptionuserdata","text":"","title":"subscriptionUserData <pre><code>ByteBuffer subscriptionUserData(\n  Set&lt;String&gt; topics)\n</code></pre> <p><code>subscriptionUserData</code> is <code>null</code> by default.</p> <p><code>subscriptionUserData</code> is used when:</p> <ul> <li><code>ConsumerCoordinator</code> is requested for metadata</li> </ul>"},{"location":"clients/consumer/Fetcher/","text":"<p><code>Fetcher&lt;K, V&gt;</code> is used by KafkaConsumer for fetching records.</p>","title":"Fetcher"},{"location":"clients/consumer/Fetcher/#creating-instance","text":"<p><code>Fetcher</code> takes the following to be created:</p> <ul> <li> <code>LogContext</code> <li> ConsumerNetworkClient <li> fetch.min.bytes <li> fetch.max.bytes <li> fetch.max.wait.ms <li> max.partition.fetch.bytes <li> max.poll.records <li> check.crcs <li> client.rack <li> Key <code>Deserializer</code> <li> Value <code>Deserializer</code> <li> <code>ConsumerMetadata</code> <li> SubscriptionState <li> <code>Metrics</code> <li> <code>FetcherMetricsRegistry</code> <li> <code>Time</code> <li> retry.backoff.ms <li> request.timeout.ms <li>IsolationLevel</li> <li> <code>ApiVersions</code>  <p><code>Fetcher</code> is created\u00a0along with KafkaConsumer.</p>","title":"Creating Instance"},{"location":"clients/consumer/Fetcher/#isolationlevel","text":"","title":"IsolationLevel <p><code>Fetcher</code> is given an <code>IsolationLevel</code> when created (based on isolation.level configuration property)</p> <p><code>Fetcher</code> uses the <code>IsolationLevel</code> for the following:</p> <ul> <li>sendFetches (and prepareFetchRequests)</li> <li>fetchOffsetsByTimes</li> <li>fetchRecords</li> <li>sendListOffsetRequest</li> </ul>"},{"location":"clients/consumer/Fetcher/#sending-fetch-requests","text":"","title":"Sending Fetch Requests <pre><code>int sendFetches()\n</code></pre> <p><code>sendFetches</code> prepare fetch requests for the nodes with the assigned partitions (preferred read replicas or leaders).</p> <p>For every fetch request,  <code>sendFetches</code> prints out the following DEBUG message to the logs:</p> <pre><code>Sending [isolationLevel] [data] to broker [fetchTarget]\n</code></pre> <p><code>sendFetches</code> requests the ConsumerNetworkClient to send the fetch request (to the <code>fetchTarget</code> ndoe) and registers the node in the nodesWithPendingFetchRequests.</p> <p>On successful response, for every partitions <code>sendFetches</code> prints out the following DEBUG message to the logs and adds a new <code>CompletedFetch</code> to the completedFetches registry.</p> <pre><code>Fetch [isolationLevel] at offset [fetchOffset] for partition [partition]\nreturned fetch data [partitionData]\n</code></pre> <p>In the end, <code>sendFetches</code> removes the request from the nodesWithPendingFetchRequests registry.</p> <p><code>sendFetches</code>\u00a0is used when:</p> <ul> <li><code>KafkaConsumer</code> is requested to poll (and pollForFetches)</li> </ul>"},{"location":"clients/consumer/Fetcher/#preparefetchrequests","text":"","title":"prepareFetchRequests <pre><code>Map&lt;Node, FetchSessionHandler.FetchRequestData&gt; prepareFetchRequests()\n</code></pre> <p><code>prepareFetchRequests</code>...FIXME</p>"},{"location":"clients/consumer/Fetcher/#preferred-read-replica","text":"","title":"Preferred Read Replica <pre><code>Node selectReadReplica(\n  TopicPartition partition,\n  Node leaderReplica,\n  long currentTimeMs)\n</code></pre> <p><code>selectReadReplica</code> requests the SubscriptionState for the preferredReadReplica of the given <code>TopicPartition</code>.</p>"},{"location":"clients/consumer/Fetcher/#offsetsfortimes","text":"","title":"offsetsForTimes <pre><code>Map&lt;TopicPartition, OffsetAndTimestamp&gt; offsetsForTimes(\n  Map&lt;TopicPartition, Long&gt; timestampsToSearch,\n  Timer timer)\n</code></pre> <p><code>offsetsForTimes</code>...FIXME</p> <p><code>offsetsForTimes</code>\u00a0is used when:</p> <ul> <li><code>KafkaConsumer</code> is requested to offsetsForTimes</li> </ul>"},{"location":"clients/consumer/Fetcher/#beginningoffsets","text":"","title":"beginningOffsets <pre><code>Map&lt;TopicPartition, Long&gt; beginningOffsets(\n  Collection&lt;TopicPartition&gt; partitions,\n  Timer timer)\n</code></pre> <p><code>beginningOffsets</code>...FIXME</p> <p><code>beginningOffsets</code>\u00a0is used when:</p> <ul> <li><code>KafkaConsumer</code> is requested to beginningOffsets</li> </ul>"},{"location":"clients/consumer/Fetcher/#endoffsets","text":"","title":"endOffsets <pre><code>Map&lt;TopicPartition, Long&gt; endOffsets(\n  Collection&lt;TopicPartition&gt; partitions,\n  Timer timer)\n</code></pre> <p><code>endOffsets</code>...FIXME</p> <p><code>endOffsets</code>\u00a0is used when:</p> <ul> <li><code>KafkaConsumer</code> is requested to endOffsets and currentLag</li> </ul>"},{"location":"clients/consumer/Fetcher/#beginningorendoffset","text":"","title":"beginningOrEndOffset <pre><code>Map&lt;TopicPartition, Long&gt; beginningOrEndOffset(\n  Collection&lt;TopicPartition&gt; partitions,\n  long timestamp,\n  Timer timer)\n</code></pre> <p><code>beginningOrEndOffset</code>...FIXME</p> <p><code>beginningOrEndOffset</code>\u00a0is used when:</p> <ul> <li><code>Fetcher</code> is requested to beginningOffsets and endOffsets</li> </ul>"},{"location":"clients/consumer/Fetcher/#fetchoffsetsbytimes","text":"","title":"fetchOffsetsByTimes <pre><code>ListOffsetResult fetchOffsetsByTimes(\n  Map&lt;TopicPartition, Long&gt; timestampsToSearch,\n  Timer timer,\n  boolean requireTimestamps)\n</code></pre> <p><code>fetchOffsetsByTimes</code>...FIXME</p> <p><code>fetchOffsetsByTimes</code>\u00a0is used when:</p> <ul> <li><code>Fetcher</code> is requested to offsetsForTimes and beginningOrEndOffset</li> </ul>"},{"location":"clients/consumer/Fetcher/#sendlistoffsetsrequests","text":"","title":"sendListOffsetsRequests <pre><code>RequestFuture&lt;ListOffsetResult&gt; sendListOffsetsRequests(\n  Map&lt;TopicPartition, Long&gt; timestampsToSearch,\n  boolean requireTimestamps)\n</code></pre> <p><code>sendListOffsetsRequests</code>...FIXME</p>"},{"location":"clients/consumer/Fetcher/#fetched-records","text":"","title":"Fetched Records <pre><code>Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; fetchedRecords()\n</code></pre> <p><code>fetchedRecords</code> returns up to max.poll.records number of records from the CompletedFetch Queue.</p>  <p>For <code>nextInLineFetch</code> unintialized or consumed already, <code>fetchedRecords</code> takes a peek at a <code>CompletedFetch</code> collection of records (in the CompletedFetch Queue). If uninitialized, <code>fetchedRecords</code> initializeCompletedFetch with the records. <code>fetchedRecords</code> saves the <code>CompletedFetch</code> records to the nextInLineFetch internal registry. <code>fetchedRecords</code> takes the <code>CompletedFetch</code> collection of records out (off the CompletedFetch Queue).</p> <p>For the partition of the nextInLineFetch collection of records paused, <code>fetchedRecords</code> prints out the following DEBUG message to the logs and <code>null</code>s the nextInLineFetch registry.</p> <pre><code>Skipping fetching records for assigned partition [p] because it is paused\n</code></pre> <p>For all the other cases, <code>fetchedRecords</code> fetches the records out of the nextInLineFetch collection of records (up to the number of records left to fetch).</p> <p>In the end, <code>fetchedRecords</code> returns the <code>ConsumerRecord</code>s per <code>TopicPartition</code> (out of the CompletedFetch Queue).</p> <p><code>fetchedRecords</code>\u00a0is used when:</p> <ul> <li><code>KafkaConsumer</code> is requested to pollForFetches</li> </ul>"},{"location":"clients/consumer/Fetcher/#fetchrecords","text":"","title":"fetchRecords <pre><code>List&lt;ConsumerRecord&lt;K, V&gt;&gt; fetchRecords(\n  CompletedFetch completedFetch,\n  int maxRecords)\n</code></pre> <p><code>fetchRecords</code>...FIXME</p>"},{"location":"clients/consumer/Fetcher/#initializecompletedfetch","text":"","title":"initializeCompletedFetch <pre><code>CompletedFetch initializeCompletedFetch(\n  CompletedFetch nextCompletedFetch)\n</code></pre> <p><code>initializeCompletedFetch</code> returns the given <code>CompletedFetch</code> if there were no errors. <code>initializeCompletedFetch</code> updates the SubscriptionState with the current metadata about the partition.</p>  <p><code>initializeCompletedFetch</code> takes the partition, the <code>PartitionData</code> and the fetch offset from the given <code>CompletedFetch</code>.</p> <p><code>initializeCompletedFetch</code> prints out the following TRACE message to the logs:</p> <pre><code>Preparing to read [n] bytes of data for partition [p] with offset [o]\n</code></pre> <p><code>initializeCompletedFetch</code> takes the <code>RecordBatch</code>es from the <code>PartitionData</code>.</p> <p>With a high watermark given, <code>initializeCompletedFetch</code> prints out the following TRACE message to the logs and requests the SubscriptionState to updateHighWatermark for the partition.</p> <pre><code>Updating high watermark for partition [p] to [highWatermark]\n</code></pre> <p>With a log start offset given, <code>initializeCompletedFetch</code> prints out the following TRACE message to the logs and requests the SubscriptionState to updateLogStartOffset for the partition.</p> <pre><code>Updating log start offset for partition [p] to [logStartOffset]\n</code></pre> <p>With a last stable offset given, <code>initializeCompletedFetch</code> prints out the following TRACE message to the logs and requests the SubscriptionState to updateLastStableOffset for the partition.</p> <pre><code>Updating last stable offset for partition [p] to [lastStableOffset]\n</code></pre> <p>With a preferred read replica given, <code>initializeCompletedFetch</code> prints out the following DEBUG message to the logs and requests the SubscriptionState to updatePreferredReadReplica for the partition.</p> <pre><code>Updating preferred read replica for partition [p] to [preferredReadReplica] set to expire at [expireTimeMs]\n</code></pre> <p>For errors like <code>NOT_LEADER_OR_FOLLOWER</code>, <code>REPLICA_NOT_AVAILABLE</code>, <code>FENCED_LEADER_EPOCH</code>, <code>KAFKA_STORAGE_ERROR</code>, <code>OFFSET_NOT_AVAILABLE</code>, <code>initializeCompletedFetch</code> prints out the following DEBUG message to the logs and requests the ConsumerMetadata to <code>requestUpdate</code>.</p> <pre><code>Error in fetch for partition [p]: [exceptionName]\n</code></pre> <p>For <code>OFFSET_OUT_OF_RANGE</code> error, <code>initializeCompletedFetch</code> requests the SubscriptionState to clearPreferredReadReplica for the partition. With no preferred read replica, it is assumed that the fetch came from the leader.</p>"},{"location":"clients/consumer/Fetcher/#resetoffsetsifneeded","text":"","title":"resetOffsetsIfNeeded <pre><code>void resetOffsetsIfNeeded()\n</code></pre> <p><code>resetOffsetsIfNeeded</code>...FIXME</p> <p><code>resetOffsetsIfNeeded</code>\u00a0is used when:</p> <ul> <li><code>KafkaConsumer</code> is requested to updateFetchPositions</li> </ul>"},{"location":"clients/consumer/Fetcher/#resetoffsetsasync","text":"","title":"resetOffsetsAsync <pre><code>void resetOffsetsAsync(\n  Map&lt;TopicPartition, Long&gt; partitionResetTimestamps)\n</code></pre> <p><code>resetOffsetsAsync</code>...FIXME</p>"},{"location":"clients/consumer/Fetcher/#sendlistoffsetrequest","text":"","title":"sendListOffsetRequest <pre><code>RequestFuture&lt;ListOffsetResult&gt; sendListOffsetRequest(\n  Node node,\n  Map&lt;TopicPartition, ListOffsetsPartition&gt; timestampsToSearch,\n  boolean requireTimestamp)\n</code></pre> <p><code>sendListOffsetRequest</code>...FIXME</p> <p><code>sendListOffsetRequest</code>\u00a0is used when:</p> <ul> <li><code>Fetcher</code> is requested to resetOffsetsIfNeeded (via resetOffsetsAsync) and fetchOffsetsByTimes (via sendListOffsetsRequests)</li> </ul>"},{"location":"clients/consumer/Fetcher/#clearbuffereddataforunassignedtopics","text":"","title":"clearBufferedDataForUnassignedTopics <pre><code>void clearBufferedDataForUnassignedTopics(\n  Collection&lt;String&gt; assignedTopics)\n</code></pre> <p><code>clearBufferedDataForUnassignedTopics</code>...FIXME</p> <p><code>clearBufferedDataForUnassignedTopics</code>\u00a0is used when:</p> <ul> <li><code>KafkaConsumer</code> is requested to subscribe</li> </ul>"},{"location":"clients/consumer/Fetcher/#completedfetch-queue","text":"","title":"CompletedFetch Queue <p><code>Fetcher</code> creates an empty <code>ConcurrentLinkedQueue</code> (Java) of <code>CompletedFetch</code>es when created.</p> <p>New <code>CompletedFetch</code>es (one per partition) are added to the queue in sendFetches (on a successful receipt of response from a Kafka cluster).</p>"},{"location":"clients/consumer/Fetcher/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.clients.consumer.internals.Fetcher</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.clients.consumer.internals.Fetcher=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"clients/consumer/KafkaConsumer/","text":"<p><code>KafkaConsumer</code> is a Consumer.</p>","title":"KafkaConsumer"},{"location":"clients/consumer/KafkaConsumer/#creating-instance","text":"<p><code>KafkaConsumer</code> takes the following to be created:</p> <ul> <li> Configuration (ConsumerConfig or <code>Map&lt;String, Object&gt;</code> or <code>Properties</code>) <li> <code>Deserializer&lt;K&gt;</code> <li> <code>Deserializer&lt;V&gt;</code>","title":"Creating Instance"},{"location":"clients/consumer/KafkaConsumer/#group-id","text":"","title":"Group ID <p><code>KafkaConsumer</code> can be given a group ID using group.id (indirectly in the config) configuration property when created.</p>"},{"location":"clients/consumer/KafkaConsumer/#isolationlevel","text":"","title":"IsolationLevel <p><code>KafkaConsumer</code> can be given an <code>IsolationLevel</code> using isolation.level configuration property (indirectly in the config) when created.</p> <p><code>KafkaConsumer</code> uses the <code>IsolationLevel</code> for the following:</p> <ul> <li>Creating a Fetcher (when created)</li> <li>currentLag</li> </ul>"},{"location":"clients/consumer/KafkaConsumer/#fetcher","text":"","title":"Fetcher <p><code>KafkaConsumer</code> creates a Fetcher when created.</p>"},{"location":"clients/consumer/KafkaConsumer/#consumercoordinator","text":"","title":"ConsumerCoordinator <p><code>KafkaConsumer</code> creates a ConsumerCoordinator when created with the group.id specified.</p>"},{"location":"clients/consumer/KafkaConsumer/#enforcerebalance","text":"","title":"enforceRebalance <pre><code>void enforceRebalance()\n</code></pre> <p><code>enforceRebalance</code> requests the ConsumerCoordinator to requestRejoin with the following reason:</p> <pre><code>rebalance enforced by user\n</code></pre> <p><code>enforceRebalance</code>\u00a0is part of the Consumer abstraction.</p>"},{"location":"clients/consumer/KafkaConsumer/#groupmetadata","text":"","title":"groupMetadata <pre><code>ConsumerGroupMetadata groupMetadata()\n</code></pre> <p><code>groupMetadata</code>...FIXME</p> <p><code>groupMetadata</code>\u00a0is part of the Consumer abstraction.</p>"},{"location":"clients/consumer/KafkaConsumer/#polling-for-records","text":"","title":"Polling for Records <pre><code>ConsumerRecords&lt;K, V&gt; poll(\n  Duration timeout) // (1)\nConsumerRecords&lt;K, V&gt; poll(\n  Timer timer,\n  boolean includeMetadataInTimeout) // (2)\n</code></pre> <ol> <li>Uses <code>includeMetadataInTimeout</code> enabled (<code>true</code>)</li> <li>A private method</li> </ol> <p><code>poll</code>...FIXME</p> <p><code>poll</code>\u00a0is part of the Consumer abstraction.</p>"},{"location":"clients/consumer/KafkaConsumer/#polling-for-fetches","text":"","title":"Polling for Fetches <pre><code>Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; pollForFetches(\n  Timer timer)\n</code></pre> <p><code>pollForFetches</code> requests the Fetcher for fetched records and returns them immediately if available.</p> <p>Otherwise, <code>pollForFetches</code> requests the Fetcher to sendFetches.</p> <p><code>pollForFetches</code> prints out the following TRACE message to the logs:</p> <pre><code>Polling for fetches with timeout [pollTimeout]\n</code></pre> <p><code>pollForFetches</code> requests the ConsumerNetworkClient to poll (with the <code>pollTimeout</code> until it expires or the Fetcher has some available fetches ready).</p> <p>In the end, <code>pollForFetches</code> requests the Fetcher for the fetched records again.</p>"},{"location":"clients/consumer/KafkaConsumer/#subscribing-to-topics","text":"","title":"Subscribing to Topics <pre><code>void subscribe(\n  Collection&lt;String&gt; topics) // (1)\nvoid subscribe(\n  Collection&lt;String&gt; topics,\n  ConsumerRebalanceListener listener)\nvoid subscribe(\n  Pattern pattern) // (2)\nvoid subscribe(\n  Pattern pattern,\n  ConsumerRebalanceListener callback)\n</code></pre> <ol> <li>Uses <code>NoOpConsumerRebalanceListener</code></li> <li>Uses <code>NoOpConsumerRebalanceListener</code></li> </ol> <p><code>subscribe</code>...FIXME</p> <p><code>subscribe</code>\u00a0is part of the Consumer abstraction.</p>"},{"location":"clients/consumer/KafkaConsumer/#waking-up","text":"","title":"Waking Up <pre><code>void wakeup()\n</code></pre> <p><code>wakeup</code>...FIXME</p> <p><code>wakeup</code>\u00a0is part of the Consumer abstraction.</p>"},{"location":"clients/consumer/SubscriptionState/","text":"","title":"SubscriptionState"},{"location":"clients/consumer/SubscriptionState/#preferred-read-replica","text":"","title":"Preferred Read Replica"},{"location":"clients/consumer/SubscriptionState/#preferredreadreplica","text":"","title":"preferredReadReplica <pre><code>Optional&lt;Integer&gt; preferredReadReplica(\n  TopicPartition tp,\n  long timeMs)\n</code></pre> <p><code>preferredReadReplica</code> looks up the state of the given <code>TopicPartition</code> and, if found, requests it for the preferredReadReplica. Otherwise, <code>preferredReadReplica</code> returns an undefined preferred read replica.</p> <p><code>preferredReadReplica</code>\u00a0is used when:</p> <ul> <li><code>Fetcher</code> is requested to selectReadReplica</li> </ul>"},{"location":"clients/consumer/SubscriptionState/#updatepreferredreadreplica","text":"","title":"updatePreferredReadReplica <pre><code>void updatePreferredReadReplica(\n  TopicPartition tp,\n  int preferredReadReplicaId,\n  LongSupplier timeMs)\n</code></pre> <p><code>updatePreferredReadReplica</code> looks up the state of the given <code>TopicPartition</code> and requests it to updatePreferredReadReplica.</p> <p><code>updatePreferredReadReplica</code>\u00a0is used when:</p> <ul> <li><code>Fetcher</code> is requested to initializeCompletedFetch</li> </ul>"},{"location":"clients/consumer/SubscriptionState/#clearpreferredreadreplica","text":"","title":"clearPreferredReadReplica <pre><code>Optional&lt;Integer&gt; clearPreferredReadReplica(\n  TopicPartition tp)\n</code></pre> <p><code>clearPreferredReadReplica</code> looks up the state of the given <code>TopicPartition</code> and requests it to clearPreferredReadReplica.</p> <p><code>clearPreferredReadReplica</code>\u00a0is used when:</p> <ul> <li><code>Fetcher</code> is requested to selectReadReplica and initializeCompletedFetch</li> </ul>"},{"location":"clients/consumer/TopicPartitionState/","text":"","title":"TopicPartitionState"},{"location":"clients/consumer/TopicPartitionState/#preferredreadreplica","text":"","title":"preferredReadReplica <pre><code>Integer preferredReadReplica\n</code></pre> <p><code>TopicPartitionState</code> manages the preferred read replica (of a <code>TopicPartition</code>) for a specified amount of time (until expires or is cleared out).</p> <p><code>preferredReadReplica</code> is used when:</p> <ul> <li><code>SubscriptionState</code> is requested for the preferredReadReplica, updatePreferredReadReplica and clearPreferredReadReplica</li> </ul>"},{"location":"clients/consumer/preferred-read-replica/","text":"<p>Preferred Read Replica is the broker ID of one of the in-sync replicas of a partition for Kafka Consumer to read records from.</p>","title":"Preferred Read Replica"},{"location":"clients/producer/","text":"<p>KafkaProducer uses Sender to send records to a Kafka cluster.</p> <p><code>KafkaProducer</code> can be transactional or idempotent (and associated with a TransactionManager).</p>","title":"Kafka Producers"},{"location":"clients/producer/BufferPool/","text":"<p><code>BufferPool</code> is...FIXME</p>","title":"BufferPool"},{"location":"clients/producer/Callback/","text":"<p><code>Callback</code> is...FIXME</p>","title":"Callback"},{"location":"clients/producer/DefaultPartitioner/","text":"<p><code>DefaultPartitioner</code> is a Partitioner.</p>","title":"DefaultPartitioner"},{"location":"clients/producer/DefaultPartitioner/#demo","text":"<pre><code>import org.apache.kafka.clients.producer.internals.DefaultPartitioner\nval partitioner = new DefaultPartitioner\n\nval keyBytes = \"hello\".getBytes\nval numPartitions = 3\n\nval p = partitioner.partition(null, null, keyBytes, null, null, null, numPartitions)\n\nprintln(p)\n</code></pre> <p>The following snippet should generate the same partition value (since it is exactly how <code>DefaultPartitioner</code> does it).</p> <pre><code>import org.apache.kafka.common.utils.Utils\n\nval keyBytes = \"hello\".getBytes\nval numPartitions = 3\n\nval p = Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions\n\nprintln(p)\n</code></pre>","title":"Demo"},{"location":"clients/producer/KafkaProducer/","text":"<p><code>KafkaProducer&lt;K, V&gt;</code> is a concrete Producer.</p>","title":"KafkaProducer"},{"location":"clients/producer/KafkaProducer/#creating-instance","text":"<p><code>KafkaProducer</code> takes the following to be created:</p> <ul> <li> ProducerConfig <li> Key <code>Serializer&lt;K&gt;</code> <li> Value <code>Serializer&lt;V&gt;</code> <li> ProducerMetadata <li> KafkaClient <li> <code>ProducerInterceptor&lt;K, V&gt;</code>s <li> Time","title":"Creating Instance"},{"location":"clients/producer/KafkaProducer/#configuretransactionstate","text":"","title":"configureTransactionState <pre><code>TransactionManager configureTransactionState(\n  ProducerConfig config,\n  LogContext logContext)\n</code></pre> <p><code>configureTransactionState</code> creates a new TransactionManager or returns <code>null</code>.</p>  <p><code>configureTransactionState</code> checks whether the following configuration properties are specified in the given ProducerConfig:</p> <ol> <li>enable.idempotence</li> <li>transactional.id</li> </ol> <p>With transactional.id specified, <code>configureTransactionState</code> turns the enable.idempotence on and prints out the following INFO message to the logs:</p> <pre><code>Overriding the default [enable.idempotence] to true since transactional.id is specified.\n</code></pre> <p>With idempotence enabled, <code>configureTransactionState</code> creates a TransactionManager with the values of the following configuration properties:</p> <ol> <li>transactional.id</li> <li>transaction.timeout.ms</li> <li>retry.backoff.ms</li> </ol> <p>When the <code>TransactionManager</code> is transactional, <code>configureTransactionState</code> prints out the following INFO message to the logs:</p> <pre><code>Instantiated a transactional producer.\n</code></pre> <p>Otherwise, <code>configureTransactionState</code> prints out the following INFO message to the logs:</p> <pre><code>Instantiated an idempotent producer.\n</code></pre> <p>In the end, <code>configureTransactionState</code> returns the <code>TransactionManager</code> or <code>null</code>.</p>"},{"location":"clients/producer/KafkaProducer/#newsender","text":"","title":"newSender <pre><code>Sender newSender(\n  LogContext logContext,\n  KafkaClient kafkaClient,\n  ProducerMetadata metadata)\n</code></pre> <p><code>newSender</code>...FIXME</p>"},{"location":"clients/producer/KafkaProducer/#configureinflightrequests","text":"","title":"configureInflightRequests <pre><code>int configureInflightRequests(\n  ProducerConfig config)\n</code></pre> <p><code>configureInflightRequests</code> gives the value of the max.in.flight.requests.per.connection (in the given <code>ProducerConfig</code>).</p> <p><code>configureInflightRequests</code> throws a <code>ConfigException</code> when the idempotence is enabled and the value of the max.in.flight.requests.per.connection is above 5:</p> <pre><code>Must set max.in.flight.requests.per.connection to at most 5 to use the idempotent producer.\n</code></pre>"},{"location":"clients/producer/KafkaProducer/#configureacks","text":"","title":"configureAcks <pre><code>short configureAcks(\n  ProducerConfig config,\n  Logger log)\n</code></pre> <p><code>configureAcks</code> returns the value of acks configuration property (in the given ProducerConfig).</p> <p>With idempotenceEnabled, <code>configureAcks</code> prints out the following INFO message to the logs when there is no acks configuration property defined:</p> <pre><code>Overriding the default [acks] to all since idempotence is enabled.\n</code></pre> <p>With idempotenceEnabled and the <code>acks</code> not <code>-1</code>, <code>configureAcks</code> throws a <code>ConfigException</code>:</p> <pre><code>Must set acks to all in order to use the idempotent producer.\nOtherwise we cannot guarantee idempotence.\n</code></pre>"},{"location":"clients/producer/KafkaProducer/#configuredeliverytimeout","text":"","title":"configureDeliveryTimeout <pre><code>int configureDeliveryTimeout(\n  ProducerConfig config,\n  Logger log)\n</code></pre> <p><code>configureDeliveryTimeout</code>...FIXME</p>"},{"location":"clients/producer/KafkaProducer/#transactionmanager","text":"","title":"TransactionManager <p><code>KafkaProducer</code> may create a TransactionManager when created (with idempotenceEnabled).</p> <p><code>TransactionManager</code> is used to create the following:</p> <ul> <li>RecordAccumulator</li> <li>Sender</li> </ul> <p><code>KafkaProducer</code> uses the <code>TransactionManager</code> for the following transactional methods:</p> <ul> <li>abortTransaction</li> <li>beginTransaction</li> <li>commitTransaction</li> <li>initTransactions</li> <li>sendOffsetsToTransaction</li> <li>doSend</li> </ul>"},{"location":"clients/producer/KafkaProducer/#throwifnotransactionmanager","text":"","title":"throwIfNoTransactionManager <p><code>KafkaProducer</code> throws an <code>IllegalStateException</code> for the transactional methods but TransactionManager is not configured.</p> <pre><code>Cannot use transactional methods without enabling transactions by setting the transactional.id configuration property\n</code></pre>"},{"location":"clients/producer/KafkaProducer/#sender-thread","text":"","title":"Sender Thread <p><code>KafkaProducer</code> creates a Sender when created.</p> <p><code>Sender</code> is immediately started as a daemon thread with the following name (using the clientId):</p> <pre><code>kafka-producer-network-thread | [clientId]\n</code></pre> <p><code>KafkaProducer</code> is actually considered open (and usable) as long as the <code>Sender</code> is running.</p> <p><code>KafkaProducer</code> simply requests the <code>Sender</code> to wake up for the following:</p> <ul> <li>initTransactions</li> <li>sendOffsetsToTransaction</li> <li>commitTransaction</li> <li>abortTransaction</li> <li>doSend</li> <li>waitOnMetadata</li> <li>flush</li> </ul>"},{"location":"clients/producer/KafkaProducer/#recordaccumulator","text":"","title":"RecordAccumulator <p><code>KafkaProducer</code> creates a RecordAccumulator when created.</p> <p>This <code>RecordAccumulator</code> is used for the following:</p> <ul> <li>Create a Sender</li> <li>append when doSend</li> <li>beginFlush when flush</li> </ul>"},{"location":"clients/producer/KafkaProducer/#maxblockms","text":"","title":"max.block.ms <p><code>KafkaProducer</code> uses max.block.ms configuration property.</p>"},{"location":"clients/producer/KafkaProducer/#transactional-methods","text":"","title":"Transactional Methods"},{"location":"clients/producer/KafkaProducer/#aborttransaction","text":"","title":"abortTransaction <pre><code>void abortTransaction()\n</code></pre> <p><code>abortTransaction</code> prints out the following INFO message to the logs:</p> <pre><code>Aborting incomplete transaction\n</code></pre> <p><code>abortTransaction</code>...FIXME</p> <p><code>abortTransaction</code>\u00a0is part of the Producer abstraction.</p>"},{"location":"clients/producer/KafkaProducer/#begintransaction","text":"","title":"beginTransaction <pre><code>void beginTransaction()\n</code></pre> <p><code>beginTransaction</code> requests the TransactionManager to beginTransaction.</p> <p><code>beginTransaction</code>\u00a0is part of the Producer abstraction.</p>"},{"location":"clients/producer/KafkaProducer/#inittransactions","text":"","title":"initTransactions <pre><code>void initTransactions()\n</code></pre> <p><code>initTransactions</code> requests the TransactionManager to initializeTransactions and requests the Sender to wakeup.</p> <p>In the end, <code>initTransactions</code> waits max.block.ms until transaction initialization is completed (successfully or not).</p> <p><code>initTransactions</code>\u00a0is part of the Producer abstraction.</p>"},{"location":"clients/producer/KafkaProducer/#sendoffsetstotransaction","text":"","title":"sendOffsetsToTransaction <pre><code>void sendOffsetsToTransaction(\n  Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,\n  ConsumerGroupMetadata groupMetadata)\n</code></pre> <p><code>sendOffsetsToTransaction</code> requests the TransactionManager to sendOffsetsToTransaction and requests the Sender to wakeup.</p> <p>In the end, <code>sendOffsetsToTransaction</code> waits max.block.ms for the send to be completed (successfully or not).</p> <p><code>sendOffsetsToTransaction</code>\u00a0is part of the Producer abstraction.</p>"},{"location":"clients/producer/KafkaProducer/#sending-record","text":"","title":"Sending Record <pre><code>Future&lt;RecordMetadata&gt; send(\n  ProducerRecord&lt;K, V&gt; record) // (1)\nFuture&lt;RecordMetadata&gt; send(\n  ProducerRecord&lt;K, V&gt; record,\n  Callback callback)\n</code></pre> <ol> <li>Uses uninitialized Callback (<code>null</code>)</li> </ol> <p><code>send</code> requests the interceptors to onSend with the given <code>record</code> (possibly modifying it) followed by doSend.</p> <p><code>send</code>\u00a0is part of the Producer abstraction.</p>"},{"location":"clients/producer/KafkaProducer/#dosend","text":"","title":"doSend <pre><code>Future&lt;RecordMetadata&gt; doSend(\n  ProducerRecord&lt;K, V&gt; record,\n  Callback callback)\n</code></pre> <p><code>doSend</code> waitOnMetadata for the topic and partition of the given record.</p> <p><code>doSend</code> requests the key Serializer to <code>serialize</code> the record (passing in the topic, the headers and the key of the record).</p> <p><code>doSend</code> requests the value Serializer to <code>serialize</code> the record (passing in the topic, the headers and the value of the record).</p> <p><code>doSend</code> determines the partition for the record.</p> <p><code>doSend</code> ensureValidRecordSize for the record (upper bound estimate).</p> <p><code>doSend</code> prints out the following TRACE message to the logs:</p> <pre><code>Attempting to append record [r] with callback [c] to topic [t] partition [p]\n</code></pre> <p><code>doSend</code> requests the RecordAccumulator to append the record (with the <code>abortOnNewBatch</code> flag enabled).</p> <p>When aborted for a new batch, <code>doSend</code>...FIXME (repeats the steps)...and prints out the following TRACE message to the logs:</p> <pre><code>Retrying append due to new batch creation for topic [t] partition [p].\nThe old partition was [prev]\n</code></pre> <p>When transactional, <code>doSend</code> requests the TransactionManager to maybeAddPartitionToTransaction.</p> <p>For <code>batchIsFull</code> or a new batch created, <code>doSend</code> prints out the following TRACE message to the logs and requests the Sender to wakeup.</p> <pre><code>Waking up the sender since topic [t] partition [p] is either full or getting a new batch\n</code></pre>"},{"location":"clients/producer/KafkaProducer/#partition","text":"","title":"partition <pre><code>int partition(\n  ProducerRecord&lt;K, V&gt; record,\n  byte[] serializedKey,\n  byte[] serializedValue,\n  Cluster cluster)\n</code></pre> <p><code>partition</code> is the <code>partition</code> (of the given <code>ProducerRecord</code>) if defined or requests the Partitioner for the partition.</p>"},{"location":"clients/producer/KafkaProducer/#flushing","text":"","title":"Flushing <pre><code>void flush()\n</code></pre> <p><code>flush</code> requests the RecordAccumulator to beginFlush.</p> <p><code>flush</code> requests the Sender to wakeup.</p> <p><code>flush</code> requests the RecordAccumulator to awaitFlushCompletion.</p> <p><code>flush</code>\u00a0is part of the Producer abstraction.</p>"},{"location":"clients/producer/KafkaProducer/#waitonmetadata","text":"","title":"waitOnMetadata <pre><code>ClusterAndWaitTime waitOnMetadata(\n  String topic,\n  Integer partition,\n  long nowMs,\n  long maxWaitMs)\n</code></pre> <p><code>waitOnMetadata</code> requests the ProducerMetadata for the current cluster info.</p> <p><code>waitOnMetadata</code>...FIXME</p> <p><code>waitOnMetadata</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to doSend and partitionsFor</li> </ul>"},{"location":"clients/producer/KafkaProducer/#demo","text":"","title":"Demo <pre><code>// Necessary imports\nimport org.apache.kafka.clients.producer.KafkaProducer\nimport org.apache.kafka.clients.producer.ProducerConfig\nimport org.apache.kafka.common.serialization.StringSerializer\n\n// Creating a KafkaProducer\nimport java.util.Properties\nval props = new Properties()\nprops.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName)\nprops.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName)\nprops.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \":9092\")\nval producer = new KafkaProducer[String, String](props)\n\n// Creating a record to be sent\nimport org.apache.kafka.clients.producer.ProducerRecord\nval r = new ProducerRecord[String, String](\"0\", \"this is a message\")\n\n// Sending the record (with no Callback)\nimport java.util.concurrent.Future\nimport org.apache.kafka.clients.producer.RecordMetadata\nval metadataF: Future[RecordMetadata] = producer.send(r)\n</code></pre>"},{"location":"clients/producer/KafkaProducer/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.clients.producer.KafkaProducer</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"clients/producer/Partitioner/","text":"<p><code>Partitioner</code> is an abstraction of partitioners for a KafkaProducer to determine the partition of records (to be sent out).</p>","title":"Partitioner"},{"location":"clients/producer/Partitioner/#configurable","text":"","title":"Configurable <p><code>Partitioner</code> is a Configurable.</p>"},{"location":"clients/producer/Partitioner/#closeable","text":"","title":"Closeable <p><code>Partitioner</code> is a <code>Closeable</code> (Java).</p>"},{"location":"clients/producer/Partitioner/#contract","text":"","title":"Contract"},{"location":"clients/producer/Partitioner/#onnewbatch","text":"","title":"onNewBatch <pre><code>void onNewBatch(\n  String topic,\n  Cluster cluster,\n  int prevPartition)\n</code></pre> <p>Used when:</p> <ul> <li><code>KafkaProducer</code> is requested to send a record (and doSend)</li> </ul>"},{"location":"clients/producer/Partitioner/#computing-partition","text":"","title":"Computing Partition <pre><code>int partition(\n  String topic,\n  Object key,\n  byte[] keyBytes,\n  Object value,\n  byte[] valueBytes,\n  Cluster cluster)\n</code></pre> <p>Used when:</p> <ul> <li><code>KafkaProducer</code> is requested to send a record (and determines the partition)</li> </ul>"},{"location":"clients/producer/Partitioner/#implementations","text":"","title":"Implementations <ul> <li>DefaultPartitioner</li> <li>UniformStickyPartitioner</li> <li>RoundRobinPartitioner</li> </ul>"},{"location":"clients/producer/Producer/","text":"<p><code>Producer&lt;K, V&gt;</code> is an interface to KafkaProducer for Kafka developers to use to send records (with <code>K</code> keys and <code>V</code> values) to a Kafka cluster.</p>","title":"Producer"},{"location":"clients/producer/Producer/#contract-subset","text":"","title":"Contract (Subset)"},{"location":"clients/producer/Producer/#aborttransaction","text":"","title":"abortTransaction <pre><code>void abortTransaction()\n</code></pre>"},{"location":"clients/producer/Producer/#begintransaction","text":"","title":"beginTransaction <pre><code>void beginTransaction()\n</code></pre>"},{"location":"clients/producer/Producer/#committransaction","text":"","title":"commitTransaction <pre><code>void commitTransaction()\n</code></pre>"},{"location":"clients/producer/Producer/#inittransactions","text":"","title":"initTransactions <pre><code>void initTransactions()\n</code></pre>"},{"location":"clients/producer/Producer/#sendoffsetstotransaction","text":"","title":"sendOffsetsToTransaction <pre><code>void sendOffsetsToTransaction(\n  Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,\n  ConsumerGroupMetadata groupMetadata)\n</code></pre> <p>Used when the producer is also a Consumer for a consume-transform-produce pattern</p>"},{"location":"clients/producer/ProducerBatch/","text":"","title":"ProducerBatch"},{"location":"clients/producer/ProducerBatch/#creating-instance","text":"<p><code>ProducerBatch</code> takes the following to be created:</p> <ul> <li> <code>TopicPartition</code> <li> MemoryRecordsBuilder <li> createdMs <li> <code>isSplitBatch</code> flag (default: <code>false</code>)  <p><code>ProducerBatch</code> is created\u00a0when:</p> <ul> <li><code>ProducerBatch</code> is requested to createBatchOffAccumulatorForRecord</li> <li><code>RecordAccumulator</code> is requested to append a record</li> </ul>","title":"Creating Instance"},{"location":"clients/producer/ProducerBatch/#tryappend","text":"","title":"tryAppend <pre><code>FutureRecordMetadata tryAppend(\n  long timestamp,\n  byte[] key,\n  byte[] value,\n  Header[] headers,\n  Callback callback,\n  long now)\n</code></pre> <p><code>tryAppend</code>...FIXME</p> <p><code>tryAppend</code>\u00a0is used when:</p> <ul> <li><code>RecordAccumulator</code> is requested to append a record</li> </ul>"},{"location":"clients/producer/ProducerConfig/","text":"","title":"ProducerConfig"},{"location":"clients/producer/ProducerConfig/#acks","text":"","title":"acks"},{"location":"clients/producer/ProducerConfig/#batchsize","text":"","title":"batch.size <p>The buffer size allocated for a partition. When records are received (which are smaller than this size) KafkaProducer will attempt to optimistically group them together until this size is reached.</p> <p>Default: <code>16384</code></p> <p>Must be at least 0</p> <p>Related to:</p> <ul> <li>linger.ms</li> <li><code>max-partition-memory-bytes</code> (<code>ConsoleProducer</code>)</li> </ul> <p>Used when:</p> <ul> <li><code>KafkaProducer</code> is created (to create a RecordAccumulator and an accompanying BufferPool)</li> <li><code>KafkaLog4jAppender</code> is requested to <code>activateOptions</code></li> </ul>"},{"location":"clients/producer/ProducerConfig/#enableidempotence","text":"","title":"enable.idempotence <p>Default: <code>false</code></p> <p>Used when:</p> <ul> <li><code>KafkaProducer</code> is requested to configureTransactionState</li> <li><code>ProducerConfig</code> is requested to maybeOverrideEnableIdempotence and idempotenceEnabled</li> </ul>"},{"location":"clients/producer/ProducerConfig/#lingerms","text":"","title":"linger.ms"},{"location":"clients/producer/ProducerConfig/#maxblockms","text":"","title":"max.block.ms"},{"location":"clients/producer/ProducerConfig/#maxinflightrequestsperconnection","text":"","title":"max.in.flight.requests.per.connection <p>The maximum number of unacknowledged requests the client will send on a single connection before blocking. Note that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled).</p> <p>Default: <code>5</code></p> <p>Must be at least 1</p> <p>Related to:</p> <ul> <li>enable.idempotence</li> <li>retries</li> </ul> <p>Used when:</p> <ul> <li><code>KafkaProducer</code> is requested to configureInflightRequests</li> </ul>"},{"location":"clients/producer/ProducerConfig/#partitionerclass","text":"","title":"partitioner.class <p>The class of the Partitioner for a KafkaProducer</p> <p>Default: DefaultPartitioner</p>"},{"location":"clients/producer/ProducerConfig/#retries","text":"","title":"retries"},{"location":"clients/producer/ProducerConfig/#retrybackoffms","text":"","title":"retry.backoff.ms <p>retry.backoff.ms</p>"},{"location":"clients/producer/ProducerConfig/#transactionalid","text":"","title":"transactional.id <p>The ID of a KafkaProducer for transactional delivery</p> <p>Default: (undefined)</p> <p>This enables reliability semantics which span multiple producer sessions since it allows the client to guarantee that transactions using the same <code>transactional.id</code> have been completed prior to starting any new transactions.</p> <p>With no <code>transactional.id</code>, a producer is limited to idempotent delivery.</p> <p>When configured, enable.idempotence is implied (and configured when <code>KafkaProducer</code> is created).</p> <p>With <code>transactional.id</code>, <code>KafkaProducer</code> uses a modified client.id (that includes the ID).</p> <p>Note that, by default, transactions require a cluster of at least three brokers which is the recommended setting for production; for development you can change this, by adjusting broker setting transaction.state.log.replication.factor.</p> <p><code>transactional.id</code> is required for the transactional methods.</p> <p>Used when:</p> <ul> <li>KafkaProducer prints out log messages (with the transactional ID included in the log prefix)</li> <li><code>KafkaProducer</code> is created (and creates a TransactionManager)</li> </ul>"},{"location":"clients/producer/ProducerConfig/#transactionstatelogreplicationfactor","text":"","title":"transaction.state.log.replication.factor"},{"location":"clients/producer/ProducerConfig/#transactiontimeoutms","text":"","title":"transaction.timeout.ms"},{"location":"clients/producer/ProducerConfig/#idempotenceenabled","text":"","title":"idempotenceEnabled <pre><code>boolean idempotenceEnabled()\n</code></pre> <p><code>idempotenceEnabled</code> is enabled (<code>true</code>) when one of the following holds:</p> <ol> <li>transactional.id is defined</li> <li>enable.idempotence is enabled</li> </ol> <p><code>idempotenceEnabled</code> throws a <code>ConfigException</code> when enable.idempotence is disabled but transactional.id is defined:</p> <pre><code>Cannot set a transactional.id without also enabling idempotence.\n</code></pre> <p><code>idempotenceEnabled</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is created (and requested to configureTransactionState, configureInflightRequests, configureAcks)</li> <li><code>ProducerConfig</code> is requested to maybeOverrideAcksAndRetries</li> </ul>"},{"location":"clients/producer/ProducerConfig/#postprocessparsedconfig","text":"","title":"postProcessParsedConfig <pre><code>Map&lt;String, Object&gt; postProcessParsedConfig(\n  Map&lt;String, Object&gt; parsedValues)\n</code></pre> <p><code>postProcessParsedConfig</code> maybeOverrideEnableIdempotence. <code>postProcessParsedConfig</code> maybeOverrideClientId. <code>postProcessParsedConfig</code> maybeOverrideAcksAndRetries.</p> <p><code>postProcessParsedConfig</code>\u00a0is part of the AbstractConfig abstraction.</p>"},{"location":"clients/producer/ProducerConfig/#maybeoverrideclientid","text":"","title":"maybeOverrideClientId <p><code>maybeOverrideAcksAndRetries</code> overrides client.id configuration property unless already defined.</p> <p>The new value uses transactional.id (if defined) or the next available ID with the <code>producer-</code> prefix.</p>"},{"location":"clients/producer/ProducerConfig/#maybeoverrideacksandretries","text":"","title":"maybeOverrideAcksAndRetries <pre><code>void maybeOverrideAcksAndRetries(\n  Map&lt;String, Object&gt; configs)\n</code></pre> <p><code>maybeOverrideAcksAndRetries</code>...FIXME</p>"},{"location":"clients/producer/ProducerConfig/#maybeoverrideenableidempotence","text":"","title":"maybeOverrideEnableIdempotence <pre><code>void maybeOverrideEnableIdempotence(\n  Map&lt;String, Object&gt; configs)\n</code></pre> <p><code>maybeOverrideEnableIdempotence</code> sets enable.idempotence configuration property to <code>true</code> when transactional.id is defined with no enable.idempotence.</p>"},{"location":"clients/producer/ProducerInterceptors/","text":"<p><code>ProducerInterceptors</code> is...FIXME</p>","title":"ProducerInterceptors"},{"location":"clients/producer/RecordAccumulator/","text":"","title":"RecordAccumulator"},{"location":"clients/producer/RecordAccumulator/#creating-instance","text":"<p><code>RecordAccumulator</code> takes the following to be created:</p> <ul> <li> LogContext <li> batch.size <li> CompressionType <li> linger.ms <li> retry.backoff.ms <li> configureDeliveryTimeout <li> <code>Metrics</code> <li> Name of the Metrics Group <li> <code>Time</code> <li> <code>ApiVersions</code> <li>TransactionManager</li> <li> BufferPool  <p><code>RecordAccumulator</code> is created\u00a0along with KafkaProducer.</p>","title":"Creating Instance"},{"location":"clients/producer/RecordAccumulator/#transactionmanager","text":"","title":"TransactionManager <p><code>RecordAccumulator</code> is given a TransactionManager when created.</p> <p><code>RecordAccumulator</code> uses the <code>TransactionManager</code> when requested for the following:</p> <ul> <li>reenqueue</li> <li>splitAndReenqueue</li> <li>insertInSequenceOrder</li> <li>drain (drainBatchesForOneNode and shouldStopDrainBatchesForPartition)</li> <li>abortUndrainedBatches</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#appendsinprogress-counter","text":"","title":"appendsInProgress Counter <p><code>RecordAccumulator</code> creates an <code>AtomicInteger</code> (Java) for <code>appendsInProgress</code> internal counter when created.</p> <p><code>appendsInProgress</code> simply marks a single execution of append (and is incremented at the beginning and decremented right at the end).</p> <p><code>appendsInProgress</code> is used when flushInProgress.</p>"},{"location":"clients/producer/RecordAccumulator/#flushinprogress","text":"<pre><code>boolean appendsInProgress()\n</code></pre> <p><code>appendsInProgress</code> indicates if the appendsInProgress counter is above <code>0</code>.</p> <p><code>appendsInProgress</code> is used when abortIncompleteBatches.</p>","title":"flushInProgress"},{"location":"clients/producer/RecordAccumulator/#flushesinprogress-counter","text":"","title":"flushesInProgress Counter <p><code>RecordAccumulator</code> creates an <code>AtomicInteger</code> (Java) for <code>flushesInProgress</code> internal counter when created.</p> <p><code>flushesInProgress</code> is incremented when beginFlush and decremented when awaitFlushCompletion.</p> <p><code>flushesInProgress</code> is used when flushInProgress.</p>"},{"location":"clients/producer/RecordAccumulator/#flushinprogress_1","text":"","title":"flushInProgress <pre><code>boolean flushInProgress()\n</code></pre> <p><code>flushInProgress</code> indicates if the flushesInProgress counter is above <code>0</code>.</p> <p><code>flushInProgress</code>\u00a0is used when:</p> <ul> <li><code>RecordAccumulator</code> is requested to ready</li> <li><code>Sender</code> is requested to maybeSendAndPollTransactionalRequest</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#appending-record","text":"","title":"Appending Record <pre><code>RecordAppendResult append(\n  TopicPartition tp,\n  long timestamp,\n  byte[] key,\n  byte[] value,\n  Header[] headers,\n  Callback callback,\n  long maxTimeToBlock,\n  boolean abortOnNewBatch,\n  long nowMs)\n</code></pre> <p><code>append</code>...FIXME</p> <p><code>append</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to send a record (and doSend)</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#tryappend","text":"","title":"tryAppend <pre><code>RecordAppendResult tryAppend(\n  long timestamp,\n  byte[] key,\n  byte[] value,\n  Header[] headers,\n  Callback callback,\n  Deque&lt;ProducerBatch&gt; deque,\n  long nowMs)\n</code></pre> <p><code>tryAppend</code>...FIXME</p>"},{"location":"clients/producer/RecordAccumulator/#ready","text":"","title":"ready <pre><code>ReadyCheckResult ready(\n  Cluster cluster,\n  long nowMs)\n</code></pre> <p><code>ready</code> is a list of partitions with data ready to send.</p> <p><code>ready</code>...FIXME</p> <p><code>ready</code>\u00a0is used when:</p> <ul> <li><code>Sender</code> is requested to sendProducerData</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#beginflush","text":"","title":"beginFlush <pre><code>void beginFlush()\n</code></pre> <p><code>beginFlush</code> atomically increments the flushesInProgress counter.</p> <p><code>beginFlush</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to flush</li> <li><code>Sender</code> is requested to maybeSendAndPollTransactionalRequest</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#awaitflushcompletion","text":"","title":"awaitFlushCompletion <pre><code>void awaitFlushCompletion()\n</code></pre> <p><code>awaitFlushCompletion</code>...FIXME</p> <p><code>awaitFlushCompletion</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to flush</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#splitandreenqueue","text":"","title":"splitAndReenqueue <pre><code>int splitAndReenqueue(\n  ProducerBatch bigBatch)\n</code></pre> <p><code>splitAndReenqueue</code>...FIXME</p> <p><code>splitAndReenqueue</code>\u00a0is used when:</p> <ul> <li><code>Sender</code> is requested to completeBatch</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#deallocate","text":"","title":"deallocate <pre><code>void deallocate(\n  ProducerBatch batch)\n</code></pre> <p><code>deallocate</code>...FIXME</p> <p><code>deallocate</code>\u00a0is used when:</p> <ul> <li><code>RecordAccumulator</code> is requested to abortBatches and abortUndrainedBatches</li> <li><code>Sender</code> is requested to maybeRemoveAndDeallocateBatch</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#abortbatches","text":"","title":"abortBatches <pre><code>void abortBatches() // (1)\nvoid abortBatches(\n  RuntimeException reason)\n</code></pre> <ol> <li>Uses a <code>KafkaException</code></li> </ol> <p><code>abortBatches</code>...FIXME</p> <p><code>abortBatches</code>\u00a0is used when:</p> <ul> <li><code>RecordAccumulator</code> is requested to abortIncompleteBatches</li> <li><code>Sender</code> is requested to maybeAbortBatches</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#abortincompletebatches","text":"","title":"abortIncompleteBatches <pre><code>void abortIncompleteBatches()\n</code></pre> <p><code>abortIncompleteBatches</code> abortBatches as long as there are appendsInProgress. <code>abortIncompleteBatches</code> abortBatches one last time (after no thread was appending in case there was a new batch appended by the last appending thread).</p> <p>In the end, <code>abortIncompleteBatches</code> clears the batches registry.</p> <p><code>abortIncompleteBatches</code>\u00a0is used when:</p> <ul> <li><code>Sender</code> is requested to run (and forceClose)</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#abortundrainedbatches","text":"","title":"abortUndrainedBatches <pre><code>void abortUndrainedBatches(\n  RuntimeException reason)\n</code></pre> <p><code>abortUndrainedBatches</code>...FIXME</p> <p><code>abortUndrainedBatches</code>\u00a0is used when:</p> <ul> <li><code>Sender</code> is requested to maybeSendAndPollTransactionalRequest</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#incomplete-pending-batches","text":"","title":"Incomplete (Pending) Batches <p><code>RecordAccumulator</code> creates an <code>IncompleteBatches</code> for <code>incomplete</code> internal registry of pending batches when created.</p> <p><code>RecordAccumulator</code> uses the <code>IncompleteBatches</code> when:</p> <ul> <li>append (to add a new <code>ProducerBatch</code>)</li> <li>splitAndReenqueue (to add a new <code>ProducerBatch</code>)</li> <li>deallocate (to remove a <code>ProducerBatch</code>)</li> <li>awaitFlushCompletion, abortBatches and abortUndrainedBatches (to copy all <code>ProducerBatch</code>s)</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#hasincomplete","text":"","title":"hasIncomplete <pre><code>boolean hasIncomplete()\n</code></pre> <p><code>hasIncomplete</code> is <code>true</code> when the incomplete registry is not empty.</p> <p><code>hasIncomplete</code> is used when:</p> <ul> <li><code>Sender</code> is requested to maybeSendAndPollTransactionalRequest and maybeAbortBatches</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#in-progress-batches","text":"","title":"In-Progress Batches <pre><code>ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt; batches\n</code></pre> <p><code>RecordAccumulator</code> creates a <code>ConcurrentMap</code> (Java) for the <code>batches</code> internal registry of in-progress ProducerBatches (per <code>TopicPartition</code>).</p> <p><code>RecordAccumulator</code> adds a new <code>ArrayDeque</code> (Java) when getOrCreateDeque.</p> <p><code>batches</code>\u00a0is used when:</p> <ul> <li>expiredBatches</li> <li>ready</li> <li>hasUndrained</li> <li>getDeque</li> <li>batches</li> <li>abortIncompleteBatches</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#getorcreatedeque","text":"","title":"getOrCreateDeque <pre><code>Deque&lt;ProducerBatch&gt; getOrCreateDeque(\n  TopicPartition tp)\n</code></pre> <p><code>getOrCreateDeque</code>...FIXME</p> <p><code>getOrCreateDeque</code>\u00a0is used when:</p> <ul> <li><code>RecordAccumulator</code> is requested to append, reenqueue, splitAndReenqueue</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#reenqueue","text":"","title":"reenqueue <pre><code>void reenqueue(\n  ProducerBatch batch,\n  long now)\n</code></pre> <p><code>reenqueue</code>...FIXME</p> <p><code>reenqueue</code>\u00a0is used when:</p> <ul> <li><code>Sender</code> is requested to reenqueueBatch</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#insertinsequenceorder","text":"","title":"insertInSequenceOrder <pre><code>void insertInSequenceOrder(\n  Deque&lt;ProducerBatch&gt; deque,\n  ProducerBatch batch)\n</code></pre> <p><code>insertInSequenceOrder</code>...FIXME</p> <p><code>insertInSequenceOrder</code>\u00a0is used when:</p> <ul> <li><code>RecordAccumulator</code> is requested to reenqueue and splitAndReenqueue</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#drain","text":"","title":"drain <pre><code>Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; drain(\n  Cluster cluster,\n  Set&lt;Node&gt; nodes,\n  int maxSize,\n  long now)\n</code></pre> <p><code>drain</code>...FIXME</p> <p><code>drain</code>\u00a0is used when:</p> <ul> <li><code>Sender</code> is requested to sendProducerData</li> </ul>"},{"location":"clients/producer/RecordAccumulator/#drainbatchesforonenode","text":"","title":"drainBatchesForOneNode <pre><code>List&lt;ProducerBatch&gt; drainBatchesForOneNode(\n  Cluster cluster,\n  Node node,\n  int maxSize,\n  long now)\n</code></pre> <p><code>drainBatchesForOneNode</code>...FIXME</p>"},{"location":"clients/producer/RecordAccumulator/#shouldstopdrainbatchesforpartition","text":"","title":"shouldStopDrainBatchesForPartition <pre><code>boolean shouldStopDrainBatchesForPartition(\n  ProducerBatch first,\n  TopicPartition tp)\n</code></pre> <p><code>shouldStopDrainBatchesForPartition</code>...FIXME</p>"},{"location":"clients/producer/Sender/","text":"<p><code>Sender</code> is a <code>Runnable</code> (Java) that is executed as a separate thread alongside KafkaProducer to send records to a Kafka cluster.</p>","title":"Sender"},{"location":"clients/producer/Sender/#creating-instance","text":"<p><code>Sender</code> takes the following to be created:</p> <ul> <li> <code>LogContext</code> <li>KafkaClient</li> <li> <code>ProducerMetadata</code> <li> RecordAccumulator <li> <code>guaranteeMessageOrder</code> flag <li> maxRequestSize <li> acks <li> retries <li> <code>SenderMetricsRegistry</code> <li> <code>Time</code> <li> requestTimeoutMs <li> retryBackoffMs <li> TransactionManager <li> <code>ApiVersions</code>  <p><code>Sender</code> is created\u00a0along with KafkaProducer.</p>","title":"Creating Instance"},{"location":"clients/producer/Sender/#kafkaclient","text":"","title":"KafkaClient <p><code>Sender</code> is given a KafkaClient when created.</p>"},{"location":"clients/producer/Sender/#running-thread","text":"","title":"Running Thread <pre><code>void run()\n</code></pre> <p><code>run</code> prints out the following DEBUG message to the logs:</p> <pre><code>Starting Kafka producer I/O thread.\n</code></pre> <p><code>run</code> runs once and repeats until the running flag is turned off.</p> <p>Right after the running flag is off, <code>run</code> prints out the following DEBUG message to the logs:</p> <pre><code>Beginning shutdown of Kafka producer I/O thread, sending remaining records.\n</code></pre> <p><code>run</code>...FIXME</p> <p>In the end, <code>run</code>\u00a0prints out the following DEBUG message to the logs:</p> <pre><code>Shutdown of Kafka producer I/O thread has completed.\n</code></pre> <p><code>run</code>\u00a0is part of the <code>Runnable</code> (Java) abstraction.</p>"},{"location":"clients/producer/Sender/#runonce","text":"","title":"runOnce <pre><code>void runOnce()\n</code></pre> <p>If executed with a TransactionManager, <code>runOnce</code>...FIXME</p> <p><code>runOnce</code> sendProducerData.</p> <p><code>runOnce</code> requests the KafkaClient to poll.</p>"},{"location":"clients/producer/Sender/#sendproducerdata","text":"","title":"sendProducerData <pre><code>long sendProducerData(\n  long now)\n</code></pre> <p><code>sendProducerData</code> requests the ProducerMetadata for the current cluster info</p> <p><code>sendProducerData</code> requests the RecordAccumulator for the partitions with data ready to send.</p> <p><code>sendProducerData</code> requests a metadata update when there are partitions with no leaders.</p> <p><code>sendProducerData</code> removes nodes not ready to send to.</p> <p><code>sendProducerData</code> requests the RecordAccumulator to drain (and create ProducerBatchs).</p> <p><code>sendProducerData</code> registers the batches (in the inFlightBatches registry).</p> <p>With guaranteeMessageOrder, <code>sendProducerData</code> mutes all the partitions drained.</p> <p><code>sendProducerData</code> requests the RecordAccumulator to resetNextBatchExpiryTime.</p> <p><code>sendProducerData</code> requests the RecordAccumulator for the expired batches and adds all expired InflightBatches.</p> <p>If there are any expired batches, <code>sendProducerData</code>...FIXME</p> <p><code>sendProducerData</code> requests the <code>SenderMetrics</code> to <code>updateProduceRequestMetrics</code>.</p> <p>With at least one broker to send batches to, <code>sendProducerData</code> prints out the following TRACE message to the logs:</p> <pre><code>Nodes with data ready to send: [readyNodes]\n</code></pre> <p><code>sendProducerData</code> sendProduceRequests.</p>"},{"location":"clients/producer/Sender/#sendproducerequests","text":"","title":"sendProduceRequests <pre><code>void sendProduceRequests(\n  Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; collated,\n  long now)\n</code></pre> <p>For every pair of a broker node and an associated ProducerBatch (in the given <code>collated</code> collection), <code>sendProduceRequests</code> sendProduceRequest with the broker node, the acks, the requestTimeoutMs and the <code>ProducerBatch</code>.</p>"},{"location":"clients/producer/Sender/#sendproducerequest","text":"","title":"sendProduceRequest <pre><code>void sendProduceRequest(\n  long now,\n  int destination,\n  short acks,\n  int timeout,\n  List&lt;ProducerBatch&gt; batches)\n</code></pre> <p><code>sendProduceRequest</code> creates a collection of ProducerBatches by <code>TopicPartition</code> from the given <code>batches</code>.</p> <p><code>sendProduceRequest</code> requests the KafkaClient for a new ClientRequest (for the <code>destination</code> broker) and to send it.</p> <p><code>sendProduceRequest</code> registers a [handleProduceResponse] callback to invoke when a response arrives. <code>sendProduceRequest</code> expects a response for all the <code>acks</code> but <code>0</code>.</p> <p>In the end, <code>sendProduceRequest</code> prints out the following TRACE message to the logs:</p> <pre><code>Sent produce request to [nodeId]: [requestBuilder]\n</code></pre>"},{"location":"clients/producer/Sender/#handleproduceresponse","text":"","title":"handleProduceResponse <pre><code>void handleProduceResponse(\n  ClientResponse response,\n  Map&lt;TopicPartition, ProducerBatch&gt; batches,\n  long now)\n</code></pre>"},{"location":"clients/producer/Sender/#maybesendandpolltransactionalrequest","text":"","title":"maybeSendAndPollTransactionalRequest <pre><code>boolean maybeSendAndPollTransactionalRequest()\n</code></pre>"},{"location":"clients/producer/Sender/#running-flag","text":"","title":"running Flag <p><code>Sender</code> runs as long as the <code>running</code> internal flag is on.</p> <p>The <code>running</code> flag is on from when <code>Sender</code> is created until requested to initiateClose.</p>"},{"location":"clients/producer/Sender/#initiateclose","text":"","title":"initiateClose <pre><code>void initiateClose()\n</code></pre> <p><code>initiateClose</code> requests the RecordAccumulator to close and turns the running flag off.</p> <p>In the end, <code>initiateClose</code> wakes up the KafkaClient.</p> <p><code>initiateClose</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to close</li> <li><code>Sender</code> is requested to forceClose</li> </ul>"},{"location":"clients/producer/Sender/#waking-up","text":"","title":"Waking Up <pre><code>void wakeup()\n</code></pre> <p><code>wakeup</code> requests the KafkaClient to wakeup.</p> <p><code>wakeup</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to initTransactions, sendOffsetsToTransaction, commitTransaction, abortTransaction, doSend, waitOnMetadata, flush</li> <li><code>Sender</code> is requested to initiateClose</li> </ul>"},{"location":"clients/producer/Sender/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.clients.producer.internals.Sender</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.clients.producer.internals.Sender=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"clients/producer/TransactionManager/","text":"","title":"TransactionManager"},{"location":"clients/producer/TransactionManager/#creating-instance","text":"<p><code>TransactionManager</code> takes the following to be created:</p> <ul> <li> <code>LogContext</code> <li> transactional.id <li> transaction.timeout.ms <li> retry.backoff.ms <li> <code>ApiVersions</code>  <p><code>TransactionManager</code> is created\u00a0along with KafkaProducer (with idempotenceEnabled).</p>","title":"Creating Instance"},{"location":"clients/producer/TransactionManager/#states","text":"","title":"States <p><code>TransactionManager</code> can be in one of the following states:</p> <ul> <li>UNINITIALIZED</li> <li>INITIALIZING</li> <li>READY</li> <li>IN_TRANSACTION</li> <li>COMMITTING_TRANSACTION</li> <li>ABORTING_TRANSACTION</li> <li>ABORTABLE_ERROR</li> <li>FATAL_ERROR</li> </ul>"},{"location":"clients/producer/TransactionManager/#valid-transitions","text":"","title":"Valid Transitions    Source (Current) State Target States transitionTo     ABORTABLE_ERROR <ul><li>ABORTING_TRANSACTION</li><li>ABORTABLE_ERROR</li></ul>    ABORTING_TRANSACTION <ul><li>INITIALIZING</li><li>READY</li></ul> beginAbort   COMMITTING_TRANSACTION <ul><li>READY</li><li>ABORTABLE_ERROR</li></ul> beginCommit   IN_TRANSACTION <ul><li>COMMITTING_TRANSACTION</li><li>ABORTING_TRANSACTION</li><li>ABORTABLE_ERROR</li></ul> beginTransaction   INITIALIZING READY <ul><li>initializeTransactions</li><li> bumpIdempotentEpochAndResetIdIfNeeded</li><li>completeTransaction</li></ul>   READY <ul><li>UNINITIALIZED</li><li>IN_TRANSACTION</li></ul> <ul><li>completeTransaction</li><li><code>InitProducerIdHandler</code></li></ul>   UNINITIALIZED INITIALIZING resetIdempotentProducerId   any state FATAL_ERROR"},{"location":"clients/producer/TransactionManager/#beginabort","text":"","title":"beginAbort <pre><code>TransactionalRequestResult beginAbort()\n</code></pre> <p><code>beginAbort</code>...FIXME</p> <p><code>beginAbort</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to abortTransaction</li> <li><code>Sender</code> is requested to run (and is shuting down)</li> </ul>"},{"location":"clients/producer/TransactionManager/#begincommit","text":"","title":"beginCommit <pre><code>TransactionalRequestResult beginCommit()\n</code></pre> <p><code>beginCommit</code>...FIXME</p> <p><code>beginCommit</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to commitTransaction</li> </ul>"},{"location":"clients/producer/TransactionManager/#begincompletingtransaction","text":"","title":"beginCompletingTransaction <pre><code>TransactionalRequestResult beginCompletingTransaction(\n  TransactionResult transactionResult)\n</code></pre> <p><code>beginCompletingTransaction</code>...FIXME</p> <p><code>beginCompletingTransaction</code>\u00a0is used when:</p> <ul> <li><code>TransactionManager</code> is requested to beginCommit and beginAbort</li> </ul>"},{"location":"clients/producer/TransactionManager/#begintransaction","text":"","title":"beginTransaction <pre><code>void beginTransaction()\n</code></pre> <p><code>beginTransaction</code> makes sure that the producer is transactional and transition to <code>IN_TRANSACTION</code> state.</p> <p><code>beginTransaction</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to beginTransaction</li> </ul>"},{"location":"clients/producer/TransactionManager/#initializetransactions","text":"","title":"initializeTransactions <pre><code>TransactionalRequestResult initializeTransactions() // (1)\nTransactionalRequestResult initializeTransactions(\n  ProducerIdAndEpoch producerIdAndEpoch)\n</code></pre> <ol> <li>Uses <code>ProducerIdAndEpoch.NONE</code></li> </ol> <p><code>initializeTransactions</code>...FIXME</p> <p><code>initializeTransactions</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to initTransactions</li> <li><code>TransactionManager</code> is requested to beginCompletingTransaction</li> </ul>"},{"location":"clients/producer/TransactionManager/#istransactional","text":"","title":"isTransactional <pre><code>boolean isTransactional()\n</code></pre> <p><code>isTransactional</code> is enabled (<code>true</code>) when the transactional.id configuration property is defined (for the producer and the transactionalId was given when created).</p>"},{"location":"clients/producer/TransactionManager/#maybeaddpartitiontotransaction","text":"","title":"maybeAddPartitionToTransaction <pre><code>void maybeAddPartitionToTransaction(\n  TopicPartition topicPartition)\n</code></pre> <p><code>maybeAddPartitionToTransaction</code>...FIXME</p> <p><code>maybeAddPartitionToTransaction</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to doSend</li> </ul>"},{"location":"clients/producer/TransactionManager/#sendoffsetstotransaction","text":"","title":"sendOffsetsToTransaction <pre><code>TransactionalRequestResult sendOffsetsToTransaction(\n  Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,\n  ConsumerGroupMetadata groupMetadata)\n</code></pre> <p><code>sendOffsetsToTransaction</code>...FIXME</p> <p><code>sendOffsetsToTransaction</code>\u00a0is used when:</p> <ul> <li><code>KafkaProducer</code> is requested to sendOffsetsToTransaction</li> </ul>"},{"location":"demo/","text":"<p>The following demos are available:</p> <ul> <li>Kafka and kcat in Docker</li> <li>Transactional Kafka Producer</li> </ul>","title":"Demos"},{"location":"demo/kafka-and-kcat-in-docker/","text":"<p>This demo uses Docker to run Apache Kafka and kcat utility.</p>","title":"Demo: Kafka and kcat in Docker"},{"location":"demo/kafka-and-kcat-in-docker/#kafka-docker","text":"<p>Pull kafka-docker project (or create a <code>docker-compose.yml</code> file yourself).</p>","title":"kafka-docker"},{"location":"demo/kafka-and-kcat-in-docker/#running-kafka-cluster","text":"<p>Start Zookeeper and Kafka containers.</p> <pre><code>docker-compose up\n</code></pre> <pre><code>$ docker-compose ps\n          Name                        Command               State                                  Ports\n----------------------------------------------------------------------------------------------------------------------------------------\nkafka-docker_kafka_1       start-kafka.sh                   Up      0.0.0.0:62687-&gt;9092/tcp\nkafka-docker_zookeeper_1   /bin/sh -c /usr/sbin/sshd  ...   Up      0.0.0.0:2181-&gt;2181/tcp,:::2181-&gt;2181/tcp, 22/tcp, 2888/tcp, 3888/tcp\n</code></pre>","title":"Running Kafka Cluster"},{"location":"demo/kafka-and-kcat-in-docker/#docker-network","text":"<p>The above creates a Docker network <code>kafka-docker_default</code> (if ran from <code>kafka-docker</code> directory as described in the official documentation of docker-compose).</p> <pre><code>$ docker network ls\nNETWORK ID     NAME                   DRIVER    SCOPE\nb8b255710858   bridge                 bridge    local\n3c9c3a969ef2   cda                    bridge    local\n398f9f3196aa   host                   host      local\n68611503fde8   kafka-docker_default   bridge    local\ndb43a5e50281   none                   null      local\n</code></pre>","title":"Docker Network"},{"location":"demo/kafka-and-kcat-in-docker/#kcat","text":"<p>Connect <code>kcat</code> container to the network (using <code>--network</code> option as described in the official documentation of docker-compose).</p>","title":"kcat"},{"location":"demo/kafka-and-kcat-in-docker/#metadata-listing","text":"<pre><code>docker run -it --rm \\\n  --network kafka-docker_default \\\n  edenhill/kcat:1.7.0 \\\n  -b kafka-docker_kafka_1:9092 -L\n</code></pre> <pre><code>Metadata for all topics (from broker -1: kafka-docker_kafka_1:9092/bootstrap):\n 1 brokers:\n  broker 1001 at 09cc8de4d067:9092 (controller)\n 0 topics:\n</code></pre>","title":"Metadata Listing"},{"location":"demo/kafka-and-kcat-in-docker/#producer","text":"<pre><code>docker run -it --rm \\\n  --network kafka-docker_default \\\n  --name producer \\\n  edenhill/kcat:1.7.0 \\\n  -b kafka-docker_kafka_1:9092 -P -t t1\n</code></pre>  <p>Caution</p> <p>For some reason the above command couldn't send messages whenever I pressed ENTER but expected <code>Ctrl+D</code> instead (that terminates the shell and the container). Switching to confluentinc/cp-kafkacat made things working fine.</p>  <pre><code>docker run -it --rm \\\n  --network kafka-docker_default \\\n  --name producer \\\n  confluentinc/cp-kafkacat \\\n  kafkacat \\\n  -b kafka-docker_kafka_1:9092 -P -t t1\n</code></pre>","title":"Producer"},{"location":"demo/kafka-and-kcat-in-docker/#consumer","text":"<pre><code>docker run -it --rm \\\n  --network kafka-docker_default \\\n  --name consumer \\\n  edenhill/kcat:1.7.0 \\\n  -b kafka-docker_kafka_1:9092 -C -t t1\n</code></pre>","title":"Consumer"},{"location":"demo/kafka-and-kcat-in-docker/#clean-up","text":"<pre><code>docker-compose down\n</code></pre>","title":"Clean Up"},{"location":"demo/transactional-kafka-producer/","text":"<p>This demo shows the internals of transactional KafkaProducer that is a Kafka producer with transaction.id defined.</p>","title":"Demo: Transactional Kafka Producer"},{"location":"demo/transactional-kafka-producer/#kafkaproducer","text":"","title":"KafkaProducer"},{"location":"demo/transactional-kafka-producer/#start-up","text":"<p>Use <code>sbt console</code> for interactive environment (or IntelliJ IDEA).</p> <pre><code>import org.apache.kafka.clients.producer.KafkaProducer\nimport org.apache.kafka.clients.producer.ProducerConfig\nimport org.apache.kafka.common.serialization.StringSerializer\n\nimport java.util.Properties\nval props = new Properties()\nprops.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName)\nprops.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName)\nprops.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \":9092\")\nprops.put(ProducerConfig.CLIENT_ID_CONFIG, \"txn-demo\")\n// Define transaction.id\nval transactionalId = \"my-custom-txnId\"\nprops.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId)\nval producer = new KafkaProducer[String, String](props)\n</code></pre>","title":"Start Up"},{"location":"demo/transactional-kafka-producer/#initialize-transactions","text":"<pre><code>producer.initTransactions\n</code></pre> <p>Once initialized, the transactional producer must not be initialized again.</p> <pre><code>scala&gt; producer.initTransactions\norg.apache.kafka.common.KafkaException: TransactionalId my-custom-txnId: Invalid transition attempted from state READY to state INITIALIZING\n  at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:1078)\n  at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:1071)\n  at org.apache.kafka.clients.producer.internals.TransactionManager.lambda$initializeTransactions$1(TransactionManager.java:337)\n  at org.apache.kafka.clients.producer.internals.TransactionManager.handleCachedTransactionRequestResult(TransactionManager.java:1200)\n  at org.apache.kafka.clients.producer.internals.TransactionManager.initializeTransactions(TransactionManager.java:334)\n  at org.apache.kafka.clients.producer.internals.TransactionManager.initializeTransactions(TransactionManager.java:329)\n  at org.apache.kafka.clients.producer.KafkaProducer.initTransactions(KafkaProducer.java:596)\n  ... 31 elided\n</code></pre>","title":"Initialize Transactions"},{"location":"demo/transactional-kafka-producer/#start-transaction","text":"<p>Next up is starting a transaction using KafkaProducer.beginTransaction</p> <pre><code>producer.beginTransaction\n</code></pre>","title":"Start Transaction"},{"location":"demo/transactional-kafka-producer/#transactional-sends","text":"<pre><code>import org.apache.kafka.clients.producer.ProducerRecord\nval topic = \"txn-demo\"\nval record = new ProducerRecord[String, String](topic, \"Hello from transactional producer\")\nproducer.send(record)\n</code></pre> <pre><code>producer.send(\n  new ProducerRecord[String, String](topic, \"Another hello from txn producer\"))\n</code></pre>","title":"Transactional sends"},{"location":"demo/transactional-kafka-producer/#logs","text":"","title":"Logs"},{"location":"demo/transactional-kafka-producer/#kafka-producer","text":"<p>You should see the following INFO messages in the logs of the Kafka producer:</p> <pre><code>INFO [Producer clientId=producer-my-custom-txnId, transactionalId=my-custom-txnId] Invoking InitProducerId for the first time in order to acquire a producer ID (org.apache.kafka.clients.producer.internals.TransactionManager)\nINFO [Producer clientId=producer-my-custom-txnId, transactionalId=my-custom-txnId] Discovered transaction coordinator localhost:9092 (id: 0 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)\nINFO [Producer clientId=producer-my-custom-txnId, transactionalId=my-custom-txnId] ProducerId set to 0 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)\n</code></pre>","title":"Kafka Producer"},{"location":"demo/transactional-kafka-producer/#kafka-cluster","text":"<p>You should see the following INFO message in the logs of a Kafka cluster:</p> <pre><code>INFO [TransactionCoordinator id=0] Initialized transactionalId my-custom-txnId with producerId 0 and producer epoch 0 on partition __transaction_state-20 (kafka.coordinator.transaction.TransactionCoordinator)\n</code></pre> <p>The calculation to determine the transactional partition (<code>__transaction_state-20</code>) is as follows:</p> <pre><code>Math.abs(transactionalId.hashCode) % 50\n</code></pre>","title":"Kafka Cluster"},{"location":"demo/transactional-kafka-producer/#start-up-consumer","text":"","title":"Start Up Consumer"},{"location":"demo/transactional-kafka-producer/#kcat","text":"<pre><code>kcat -C -b localhost -t txn-demo\n</code></pre> <p>You should see no records produced yet (since the transaction has not been committed yet).</p>","title":"kcat"},{"location":"demo/transactional-kafka-producer/#kafka-console-consumer","text":"<pre><code>./bin/kafka-console-consumer.sh \\\n  --bootstrap-server :9092 \\\n  --topic txn-demo \\\n  --from-beginning\n</code></pre> <p>Unlike <code>kcat</code>, <code>kafka-console-consumer</code> uses read_uncommitted isolation level and so there should be records printed out to the console.</p> <p>Use <code>--isolation-level</code> option to set isolation.level configuration property.</p>","title":"kafka-console-consumer"},{"location":"demo/transactional-kafka-producer/#commit-transaction","text":"<p>Let's commit the transaction using KafkaProducer.commitTransaction.</p> <pre><code>producer.commitTransaction\n</code></pre> <p>Immediately after committing the transaction you should see the record printed out by the Kafka consumer.</p>","title":"Commit Transaction"},{"location":"metrics/","text":"","title":"Metrics"},{"location":"metrics/MetricConfig/","text":"","title":"MetricConfig"},{"location":"metrics/MetricConfig/#creating-instance","text":"<p><code>MetricConfig</code> takes no arguments to be created.</p> <p><code>MetricConfig</code> is created when:</p> <ul> <li>many places (FIXME)</li> </ul>","title":"Creating Instance"},{"location":"metrics/MetricConfig/#recordinglevel","text":"","title":"RecordingLevel <p><code>MetricConfig</code> uses <code>INFO</code> recording level by default (when created) that can be changed using recordLevel.</p>"},{"location":"metrics/MetricConfig/#recordlevel","text":"","title":"recordLevel <pre><code>MetricConfig recordLevel(\n  Sensor.RecordingLevel recordingLevel)\n</code></pre> <p><code>recordLevel</code> sets the recordingLevel to the given <code>RecordingLevel</code>.</p> <p><code>recordLevel</code> is used when:</p> <ul> <li><code>KafkaAdminClient</code> is requested to <code>createInternal</code></li> <li><code>KafkaConsumer</code> is requested to buildMetrics</li> <li><code>KafkaProducer</code> is created</li> <li><code>KafkaStreams</code> (Kafka Streams) is requested to <code>getMetrics</code></li> <li><code>StreamsMetricsImpl</code> (Kafka Streams) is requested to <code>addClientLevelImmutableMetric</code>, <code>addClientLevelMutableMetric</code>, <code>addStoreLevelMutableMetric</code></li> <li><code>Server</code> utility is used to buildMetricsConfig</li> </ul>"},{"location":"metrics/Metrics/","text":"<p><code>Metrics</code> is a registry of sensors and performance metrics (of Kafka brokers and clients).</p>","title":"Metrics"},{"location":"metrics/Metrics/#creating-instance","text":"<p><code>Metrics</code> takes the following to be created:</p> <ul> <li> MetricConfig <li> <code>MetricsReporter</code>s <li> <code>Time</code> <li> <code>enableExpiration</code> flag <li> <code>MetricsContext</code>  <p><code>Metrics</code> is created when:</p> <ul> <li><code>Server</code> utility is used to buildMetrics</li> <li><code>KafkaAdminClient</code> utility is used to <code>createInternal</code></li> <li><code>KafkaConsumer</code> utility is used to buildMetrics</li> <li><code>KafkaProducer</code> is created</li> <li><code>KafkaStreams</code> (Kafka Streams) utility is used to <code>getMetrics</code></li> <li>Kafka Connect clients</li> </ul>","title":"Creating Instance"},{"location":"metrics/Metrics/#addreporter","text":"","title":"addReporter <pre><code>void addReporter(\n  MetricsReporter reporter)\n</code></pre> <p><code>addReporter</code>...FIXME</p> <p><code>addReporter</code> is used when:</p> <ul> <li><code>DynamicMetricsReporters</code> is requested to <code>createReporters</code></li> </ul>"},{"location":"metrics/Metrics/#sensors","text":"","title":"sensors <p><code>Metrics</code> defines <code>sensors</code> collection of metric <code>Sensor</code>s by name (<code>ConcurrentMap&lt;String, Sensor&gt;</code>).</p> <p><code>sensors</code> is empty when <code>Metrics</code> is created.</p> <p>A new <code>Sensor</code> is added in sensor.</p>"},{"location":"metrics/Metrics/#sensor","text":"","title":"sensor <pre><code>Sensor sensor(\n  String name,\n  MetricConfig config,\n  long inactiveSensorExpirationTimeSeconds,\n  Sensor.RecordingLevel recordingLevel,\n  Sensor... parents)\nSensor sensor(...) // (1)\n</code></pre> <ol> <li>There are others</li> </ol> <p><code>sensor</code> looks up the sensor (by name) and returns it immediately if available.</p> <p>Otherwise, <code>sensor</code> creates a new <code>Sensor</code> and adds it to the sensors registry.</p> <p>In the end, <code>sensor</code> prints out the following TRACE message to the logs:</p> <pre><code>Added sensor with name [name]\n</code></pre>"},{"location":"metrics/Metrics/#getsensor","text":"","title":"getSensor <pre><code>Sensor getSensor(\n  String name)\n</code></pre> <p><code>getSensor</code> looks up the given <code>name</code> in the sensors registry.</p>"},{"location":"metrics/Metrics/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.common.metrics.Metrics</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.common.metrics.Metrics=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"metrics/MetricsReporter/","text":"<p><code>MetricsReporter</code> is an extension of the <code>Reconfigurable</code> abstraction for metrics reporters.</p>","title":"MetricsReporter"},{"location":"metrics/MetricsReporter/#contract-subset","text":"","title":"Contract (Subset)"},{"location":"metrics/MetricsReporter/#init","text":"","title":"init <pre><code>void init(\n  List&lt;KafkaMetric&gt; metrics)\n</code></pre> <p>Used when:</p> <ul> <li><code>Metrics</code> is created and requested to addReporter</li> </ul>"},{"location":"metrics/MetricsReporter/#implementations","text":"<ul> <li><code>JmxReporter</code></li> <li><code>PushHttpMetricsReporter</code></li> </ul>","title":"Implementations"},{"location":"metrics/Sensor/","text":"","title":"Sensor"},{"location":"metrics/Sensor/#creating-instance","text":"<p><code>Sensor</code> takes the following to be created:</p> <ul> <li> Metrics <li> Name <li> Parent <code>Sensor</code>s <li> MetricConfig <li> <code>Time</code> <li> <code>inactiveSensorExpirationTimeSeconds</code> <li> <code>RecordingLevel</code>  <p><code>Sensor</code> is created when:</p> <ul> <li><code>Metrics</code> is requested for a sensor</li> </ul>","title":"Creating Instance"},{"location":"metrics/Sensor/#shouldrecord","text":"","title":"shouldRecord <pre><code>boolean shouldRecord()\n</code></pre> <p><code>shouldRecord</code> requests the RecordingLevel to <code>shouldRecord</code> based on the configured RecordingLevel (in the MetricConfig).</p>"},{"location":"raft/","text":"<p>KIP-500 allows running a Kafka cluster without Apache ZooKeeper in so-called Kafka Raft metadata mode (KRaft mode).</p>  <p>Preview and Not for Production Deployments</p> <p>KRaft mode is currently PREVIEW AND SHOULD NOT BE USED IN PRODUCTION.</p>","title":"Kafka Raft (KRaft)"},{"location":"raft/BrokerServer/","text":"<p><code>BrokerServer</code> is a KafkaBroker that runs in KRaft mode.</p>","title":"BrokerServer"},{"location":"raft/BrokerServer/#creating-instance","text":"<p><code>BrokerServer</code> takes the following to be created:</p> <ul> <li> KafkaConfig <li> MetaProperties <li> RaftManager <li> <code>Time</code> <li> Metrics <li> Optional <code>threadNamePrefix</code> <li>Initial Offline Log Directories</li> <li> <code>controllerQuorumVotersFuture</code> (<code>CompletableFuture[util.Map[Integer, AddressSpec]]</code>) <li> Supported Features  <p><code>BrokerServer</code> is created when:</p> <ul> <li><code>KafkaRaftServer</code> is created (with <code>BrokerRole</code> among the processRoles)</li> </ul>","title":"Creating Instance"},{"location":"raft/BrokerServer/#initial-offline-log-directories","text":"","title":"Initial Offline Log Directories <pre><code>initialOfflineDirs: Seq[String]\n</code></pre> <p><code>BrokerServer</code> is given <code>initialOfflineDirs</code> when created (that is offlineDirs).</p> <p><code>initialOfflineDirs</code> is used to create a LogManager when <code>BrokerServer</code> is requested to startup.</p>"},{"location":"raft/BrokerServer/#startup","text":"","title":"startup <pre><code>startup(): Unit\n</code></pre> <p><code>startup</code>...FIXME</p> <p><code>startup</code>\u00a0is used when:</p> <ul> <li><code>KafkaRaftServer</code> is requested to startup</li> </ul>"},{"location":"raft/ControllerServer/","text":"<p><code>ControllerServer</code> is...FIXME</p>","title":"ControllerServer"},{"location":"raft/KafkaRaftManager/","text":"<p><code>KafkaRaftManager</code> is...FIXME</p>","title":"KafkaRaftManager"},{"location":"raft/KafkaRaftServer/","text":"<p><code>KafkaRaftServer</code> is a Kafka broker that runs without Zookeeper.</p>","title":"KafkaRaftServer"},{"location":"raft/KafkaRaftServer/#creating-instance","text":"<p><code>KafkaRaftServer</code> takes the following to be created:</p> <ul> <li> KafkaConfig <li> <code>Time</code> <li> Optional <code>threadNamePrefix</code>  <p><code>KafkaRaftServer</code> is created when:</p> <ul> <li><code>Kafka</code> utility is used to build a Server (with no processRoles and hence no Zookeeper)</li> </ul>","title":"Creating Instance"},{"location":"raft/KafkaRaftServer/#startup","text":"","title":"startup <pre><code>startup(): Unit\n</code></pre> <p><code>startup</code>...FIXME</p> <p><code>startup</code>\u00a0is part of the Server abstraction.</p>"},{"location":"raft/KafkaRaftServer/#initializelogdirs","text":"","title":"initializeLogDirs <pre><code>initializeLogDirs(\n  config: KafkaConfig): (MetaProperties, Seq[String])\n</code></pre> <p><code>initializeLogDirs</code> uses the log.dirs (if defined) or log.dir and metadata.log.dir for the log directories (<code>logDirs</code>).</p> <p><code>initializeLogDirs</code> getBrokerMetadataAndOfflineDirs the log directories (into <code>rawMetaProperties</code> and <code>offlineDirs</code>).</p>  <p>Note</p> <p>metadata.log.dir is not allowed to be among the offline directories (<code>offlineDirs</code>) or a <code>KafkaException</code> is thrown:</p> <pre><code>Cannot start server since `meta.properties` could not be loaded from [metadataLogDir]\n</code></pre>  <p><code>initializeLogDirs</code>...FIXME</p>  <p><code>initializeLogDirs</code> is used when:</p> <ul> <li><code>KafkaRaftServer</code> is created (and initializes metaProps and offlineDirs)</li> </ul>"},{"location":"raft/KafkaRaftServer/#offlinedirs","text":"","title":"offlineDirs <pre><code>offlineDirs: Seq[String]\n</code></pre> <p><code>KafkaRaftServer</code> initializes an <code>offlineDirs</code> registry when created.</p> <p><code>offlineDirs</code> is used when:</p> <ul> <li><code>KafkaRaftServer</code> is requested for the broker (to create a BrokerServer)</li> </ul>"},{"location":"raft/KafkaRaftServer/#metaprops","text":"","title":"metaProps <pre><code>metaProps: MetaProperties\n</code></pre> <p><code>KafkaRaftServer</code> creates a MetaProperties when created.</p> <p><code>metaProps</code> is used to initialize the other <code>KafkaRaftServer</code> services:</p> <ul> <li>BrokerServer</li> <li>ControllerServer</li> <li>KafkaRaftManager</li> <li>Metrics</li> </ul>"},{"location":"raft/KafkaRaftServer/#brokerserver","text":"","title":"BrokerServer <pre><code>broker: Option[BrokerServer]\n</code></pre> <p><code>KafkaRaftServer</code> creates a BrokerServer when created with BrokerRole.</p> <p>The lifecycle of <code>BrokerServer</code> is tied up to <code>KafkaRaftServer</code>:</p> <ul> <li>startup when <code>KafkaRaftServer</code> is requested to startup</li> <li>shutdown when <code>KafkaRaftServer</code> is requested to shutdown</li> <li>awaitShutdown when <code>KafkaRaftServer</code> is requested to awaitShutdown</li> </ul>"},{"location":"raft/MetaProperties/","text":"<p><code>MetaProperties</code> is...FIXME</p>","title":"MetaProperties"},{"location":"raft/RaftConfig/","text":"<p><code>RaftConfig</code> is...FIXME</p>","title":"RaftConfig"},{"location":"raft/RaftManager/","text":"<p><code>RaftManager</code> is...FIXME</p>","title":"RaftManager"},{"location":"tools/","text":"","title":"Tools"},{"location":"tools/ConsoleConsumer/","text":"<p><code>ConsoleConsumer</code> is...FIXME</p>","title":"ConsoleConsumer"},{"location":"tools/ReplicaVerificationTool/","text":"<p><code>ReplicaVerificationTool</code> is...FIXME</p>","title":"ReplicaVerificationTool"},{"location":"transactions/","text":"<p>Apache Kafka supports transactional record delivery (and consumption if in consumer-process-produce processing mode).</p> <p>Every Kafka broker runs a TransactionCoordinator to manage (coordinate) transactions.</p>","title":"Transactions"},{"location":"transactions/#transactional-producer","text":"<p>A KafkaProducer is transactional when transactional.id configuration property is specified.</p> <p>Any record sending has to be after KafkaProducer.initTransactions followed by KafkaProducer.beginTransaction. Otherwise, the underlying TransactionManager is going to be in a wrong state (that will inevitably lead to exceptions).</p>","title":"Transactional Producer"},{"location":"transactions/#transaction-aware-consumer","text":"<p>A KafkaConsumer supports transactions using isolation.level configuration property.</p>","title":"Transaction-Aware Consumer"},{"location":"transactions/#kafka-console-consumer","text":"<p><code>kafka-console-consumer</code> supports <code>--isolation-level</code> option for isolation.level configuration property.</p>","title":"kafka-console-consumer"},{"location":"transactions/#demo","text":"<p>Demo: Transactional Kafka Producer</p>","title":"Demo"},{"location":"transactions/#transactional-configuration-properties","text":"","title":"Transactional Configuration Properties <p>TransactionConfig</p>"},{"location":"transactions/#transaction-topic","text":"","title":"Transaction Topic <p>Kafka brokers use <code>__transaction_state</code> internal topic for managing transactions (as records).</p> <p><code>__transaction_state</code> is auto-created at the first transaction.</p> <p>The number of partitions is configured using transaction.state.log.num.partitions configuration property.</p> <p>A transaction (record) is assigned a partition (txn topic partition) based on the absolute hash code of the transactional.id.</p>"},{"location":"transactions/#learning-resources","text":"","title":"Learning Resources <ul> <li>Transactions in Apache Kafka by Confluent</li> </ul>"},{"location":"transactions/TransactionConfig/","text":"<p><code>TransactionConfig</code> holds the values of the transactional configuration properties.</p>","title":"TransactionConfig"},{"location":"transactions/TransactionConfig/#transactionalidexpirationms","text":"","title":"transactional.id.expiration.ms <p>transactional.id.expiration.ms</p> <p>Default: 7 days</p>"},{"location":"transactions/TransactionConfig/#transactionmaxtimeoutms","text":"","title":"transaction.max.timeout.ms <p>transaction.max.timeout.ms</p> <p>Default: 15 minutes</p>"},{"location":"transactions/TransactionConfig/#transactionstatelognumpartitions","text":"","title":"transaction.state.log.num.partitions <p>transaction.state.log.num.partitions</p> <p>Default: 50</p>"},{"location":"transactions/TransactionConfig/#transactionstatelogreplicationfactor","text":"","title":"transaction.state.log.replication.factor <p>transaction.state.log.replication.factor</p> <p>Default: 3</p>"},{"location":"transactions/TransactionConfig/#transactionstatelogsegmentbytes","text":"","title":"transaction.state.log.segment.bytes <p>transaction.state.log.segment.bytes</p> <p>Default: 100 * 1024 * 1024</p>"},{"location":"transactions/TransactionConfig/#transactionstatelogloadbuffersize","text":"","title":"transaction.state.log.load.buffer.size <p>transaction.state.log.load.buffer.size</p> <p>Default: 5 * 1024 * 1024</p>"},{"location":"transactions/TransactionConfig/#transactionstatelogminisr","text":"","title":"transaction.state.log.min.isr <p>transaction.state.log.min.isr</p> <p>Default: 2</p>"},{"location":"transactions/TransactionConfig/#transactionaborttimedouttransactioncleanupintervalms","text":"","title":"transaction.abort.timed.out.transaction.cleanup.interval.ms <p>transaction.abort.timed.out.transaction.cleanup.interval.ms</p> <p>Default: 10 seconds</p>"},{"location":"transactions/TransactionConfig/#transactionremoveexpiredtransactioncleanupintervalms","text":"","title":"transaction.remove.expired.transaction.cleanup.interval.ms <p>transaction.remove.expired.transaction.cleanup.interval.ms</p> <p>Default: 1 hour</p>"},{"location":"transactions/TransactionConfig/#requesttimeoutms","text":"","title":"request.timeout.ms <p>request.timeout.ms</p> <p>Default: 30000</p>"},{"location":"transactions/TransactionCoordinator/","text":"<p><code>TransactionCoordinator</code> runs on every Kafka broker (BrokerServer or KafkaServer).</p>","title":"TransactionCoordinator"},{"location":"transactions/TransactionCoordinator/#creating-instance","text":"<p><code>TransactionCoordinator</code> takes the following to be created:</p> <ul> <li> Broker Id <li> TransactionConfig <li> <code>Scheduler</code> <li> <code>createProducerIdGenerator</code> function (<code>() =&gt; ProducerIdGenerator</code>) <li> TransactionStateManager <li> <code>TransactionMarkerChannelManager</code> <li> <code>Time</code> <li> <code>LogContext</code>  <p><code>TransactionCoordinator</code> is created using apply factory.</p>","title":"Creating Instance"},{"location":"transactions/TransactionCoordinator/#creating-transactioncoordinator","text":"","title":"Creating TransactionCoordinator <pre><code>apply(\n  config: KafkaConfig,\n  replicaManager: ReplicaManager,\n  scheduler: Scheduler,\n  createProducerIdGenerator: () =&gt; ProducerIdGenerator,\n  metrics: Metrics,\n  metadataCache: MetadataCache,\n  time: Time): TransactionCoordinator\n</code></pre> <p><code>apply</code> creates a TransactionConfig.</p> <p><code>apply</code> creates a TransactionStateManager (with the brokerId and the other Kafka services).</p> <p><code>apply</code> creates a <code>LogContext</code> that uses the following log prefix (with the brokerId):</p> <pre><code>[TransactionCoordinator id=[brokerId]]\n</code></pre> <p><code>apply</code> creates a <code>TransactionMarkerChannelManager</code>.</p> <p>In the end, <code>apply</code> creates a TransactionCoordinator.</p> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>BrokerServer</code> is requested to start up</li> <li><code>KafkaServer</code> is requested to start up</li> </ul>"},{"location":"transactions/TransactionCoordinator/#starting-up","text":"","title":"Starting Up <pre><code>startup(\n  retrieveTransactionTopicPartitionCount: () =&gt; Int,\n  enableTransactionalIdExpiration: Boolean = true): Unit\n</code></pre> <p><code>startup</code>...FIXME</p> <p><code>startup</code>\u00a0is used when:</p> <ul> <li><code>BrokerServer</code> is requested to start up</li> <li><code>KafkaServer</code> is requested to start up</li> </ul>"},{"location":"transactions/TransactionCoordinator/#onelection","text":"","title":"onElection <pre><code>onElection(\n  txnTopicPartitionId: Int,\n  coordinatorEpoch: Int): Unit\n</code></pre> <p><code>onElection</code> prints out the following INFO message to the logs:</p> <pre><code>Elected as the txn coordinator for partition [txnTopicPartitionId] at epoch [coordinatorEpoch]\n</code></pre> <p><code>onElection</code> requests the TransactionMarkerChannelManager to removeMarkersForTxnTopicPartition for the given <code>txnTopicPartitionId</code> partition.</p> <p>In the end, <code>onElection</code> requests the TransactionStateManager to loadTransactionsForTxnTopicPartition.</p> <p><code>onElection</code>\u00a0is used when:</p> <ul> <li><code>RequestHandlerHelper</code> is requested to onLeadershipChange</li> </ul>"},{"location":"transactions/TransactionCoordinator/#onresignation","text":"","title":"onResignation <pre><code>onResignation(\n  txnTopicPartitionId: Int,\n  coordinatorEpoch: Option[Int]): Unit\n</code></pre> <p><code>onResignation</code>...FIXME</p> <p><code>onResignation</code>\u00a0is used when:</p> <ul> <li><code>KafkaApis</code> is requested to handleStopReplicaRequest</li> <li><code>RequestHandlerHelper</code> is requested to onLeadershipChange</li> </ul>"},{"location":"transactions/TransactionCoordinator/#handleinitproducerid","text":"","title":"handleInitProducerId <pre><code>handleInitProducerId(\n  transactionalId: String,\n  transactionTimeoutMs: Int,\n  expectedProducerIdAndEpoch: Option[ProducerIdAndEpoch],\n  responseCallback: InitProducerIdCallback): Unit\n</code></pre> <p>For <code>transactionalId</code> undefined (<code>null</code>), <code>handleInitProducerId</code> requests the ProducerIdGenerator to <code>generateProducerId</code> and sends it back (using the given <code>InitProducerIdCallback</code>).</p> <p><code>handleInitProducerId</code> requests the TransactionStateManager to getTransactionState for the given <code>transactionalId</code>.</p> <p><code>handleInitProducerId</code> prints out the following INFO message to the logs:</p> <pre><code>Initialized transactionalId [transactionalId] with producerId [producerId] and producer epoch [producerEpoch]\non partition __transaction_state-[partition]\n</code></pre> <p>In the end, <code>handleInitProducerId</code> requests the TransactionStateManager to appendTransactionToLog.</p> <p><code>handleInitProducerId</code>\u00a0is used when:</p> <ul> <li><code>KafkaApis</code> is requested to handleInitProducerIdRequest</li> </ul>"},{"location":"transactions/TransactionCoordinator/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>kafka.coordinator.transaction.TransactionCoordinator</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.kafka.coordinator.transaction.TransactionCoordinator=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"transactions/TransactionMarkerChannelManager/","text":"<p><code>TransactionMarkerChannelManager</code> is...FIXME</p>","title":"TransactionMarkerChannelManager"},{"location":"transactions/TransactionStateManager/","text":"","title":"TransactionStateManager"},{"location":"transactions/TransactionStateManager/#creating-instance","text":"<p><code>TransactionStateManager</code> takes the following to be created:</p> <ul> <li> Broker ID <li> <code>Scheduler</code> <li> ReplicaManager <li> TransactionConfig <li> <code>Time</code> <li> <code>Metrics</code>  <p><code>TransactionStateManager</code> is created\u00a0when:</p> <ul> <li><code>TransactionCoordinator</code> utility is used to create a TransactionCoordinator</li> </ul>","title":"Creating Instance"},{"location":"transactions/TransactionStateManager/#starting-up","text":"","title":"Starting Up <pre><code>startup(\n  retrieveTransactionTopicPartitionCount: () =&gt; Int,\n  enableTransactionalIdExpiration: Boolean = true): Unit\n</code></pre> <p><code>startup</code>...FIXME</p> <p><code>startup</code>\u00a0is used when:</p> <ul> <li><code>TransactionCoordinator</code> is requested to start up</li> </ul>"},{"location":"transactions/TransactionStateManager/#enabletransactionalidexpiration","text":"","title":"enableTransactionalIdExpiration <pre><code>enableTransactionalIdExpiration(): Unit\n</code></pre> <p><code>enableTransactionalIdExpiration</code>...FIXME</p>"},{"location":"transactions/TransactionStateManager/#appendtransactiontolog","text":"","title":"appendTransactionToLog <pre><code>appendTransactionToLog(\n  transactionalId: String,\n  coordinatorEpoch: Int,\n  newMetadata: TxnTransitMetadata,\n  responseCallback: Errors =&gt; Unit,\n  retryOnError: Errors =&gt; Boolean = _ =&gt; false): Unit\n</code></pre> <p><code>appendTransactionToLog</code> generates the key and the value (of the record to represent the transaction in the topic) based on the given <code>transactionalId</code> and the <code>TxnTransitMetadata</code>, respectively.</p> <p><code>appendTransactionToLog</code>...FIXME</p> <p><code>appendTransactionToLog</code> requests the ReplicaManager to appendRecords (with <code>-1</code> acks, <code>internalTopicsAllowed</code> enabled annd <code>Coordinator</code> origin) and prints out the following TRACE message to the logs:</p> <pre><code>Appending new metadata [newMetadata] for transaction id [transactionalId] with coordinator epoch [coordinatorEpoch] to the local transaction log\n</code></pre> <p><code>appendTransactionToLog</code>\u00a0is used when:</p> <ul> <li><code>TransactionCoordinator</code> is requested to handleInitProducerId, handleAddPartitionsToTransaction, endTransaction</li> <li><code>TransactionMarkerChannelManager</code> is requested to <code>tryAppendToLog</code></li> </ul>"},{"location":"transactions/TransactionStateManager/#partitionfor","text":"","title":"partitionFor <pre><code>partitionFor(\n  transactionalId: String): Int\n</code></pre> <p><code>partitionFor</code> calculates the partition for the given <code>transactionalId</code>.</p> <p><code>partitionFor</code> gets the absolute value of the <code>hashCode</code> of the <code>transactionalId</code> string modulo the number of partitions of the <code>__transaction_state</code> topic.</p> <p><code>partitionFor</code>\u00a0is used when:</p> <ul> <li><code>TransactionStateManager</code> is requested to appendTransactionToLog, enableTransactionalIdExpiration, getAndMaybeAddTransactionState</li> <li><code>TransactionCoordinator</code> is requested to handleInitProducerId</li> <li><code>TransactionMarkerChannelManager</code> is requested to <code>addTxnMarkersToBrokerQueue</code></li> </ul>"},{"location":"transactions/TransactionStateManager/#loadtransactionsfortxntopicpartition","text":"","title":"loadTransactionsForTxnTopicPartition <pre><code>loadTransactionsForTxnTopicPartition(\n  partitionId: Int,\n  coordinatorEpoch: Int,\n  sendTxnMarkers: SendTxnMarkersCallback): Unit\n</code></pre> <p><code>loadTransactionsForTxnTopicPartition</code>...FIXME</p> <p><code>loadTransactionsForTxnTopicPartition</code>\u00a0is used when:</p> <ul> <li><code>TransactionCoordinator</code> is requested to onElection</li> </ul>"},{"location":"transactions/TransactionStateManager/#removetransactionsfortxntopicpartition","text":"","title":"removeTransactionsForTxnTopicPartition <pre><code>removeTransactionsForTxnTopicPartition(\n  partitionId: Int): Unit\nremoveTransactionsForTxnTopicPartition(\n  partitionId: Int,\n  coordinatorEpoch: Int): Unit\n</code></pre> <p><code>removeTransactionsForTxnTopicPartition</code>...FIXME</p> <p><code>removeTransactionsForTxnTopicPartition</code>\u00a0is used when:</p> <ul> <li><code>TransactionCoordinator</code> is requested to onResignation</li> </ul>"}]}