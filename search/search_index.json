{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Apache Kafka 2.8.0 \u00b6 Welcome to The Internals of Apache Kafka online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Kafka as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Apache Kafka \ud83d\udd25 Last update: 2021-09-20","title":"Home"},{"location":"#the-internals-of-apache-kafka-280","text":"Welcome to The Internals of Apache Kafka online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Kafka as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Apache Kafka \ud83d\udd25 Last update: 2021-09-20","title":"The Internals of Apache Kafka 2.8.0"},{"location":"AbstractConfig/","text":"AbstractConfig \u00b6 AbstractConfig is...FIXME","title":"AbstractConfig"},{"location":"AbstractConfig/#abstractconfig","text":"AbstractConfig is...FIXME","title":"AbstractConfig"},{"location":"Configurable/","text":"Configurable \u00b6 Configurable is...FIXME","title":"Configurable"},{"location":"Configurable/#configurable","text":"Configurable is...FIXME","title":"Configurable"},{"location":"InterBrokerSendThread/","text":"InterBrokerSendThread \u00b6","title":"InterBrokerSendThread"},{"location":"InterBrokerSendThread/#interbrokersendthread","text":"","title":"InterBrokerSendThread"},{"location":"Utils/","text":"Utils \u00b6 murmur2 \u00b6 int murmur2 ( byte [] data ) murmur2 generates a 32-bit murmur2 hash for the given byte array. murmur2 is used when: DefaultPartitioner is requested to compute a partition for a record Demo \u00b6 import org.apache.kafka.common.utils.Utils val keyBytes = \"hello\".getBytes val hash = Utils.murmur2(keyBytes) println(hash) toPositive \u00b6 int toPositive ( int number ) toPositive converts a number to a positive value.","title":"Utils"},{"location":"Utils/#utils","text":"","title":"Utils"},{"location":"Utils/#murmur2","text":"int murmur2 ( byte [] data ) murmur2 generates a 32-bit murmur2 hash for the given byte array. murmur2 is used when: DefaultPartitioner is requested to compute a partition for a record","title":" murmur2"},{"location":"Utils/#demo","text":"import org.apache.kafka.common.utils.Utils val keyBytes = \"hello\".getBytes val hash = Utils.murmur2(keyBytes) println(hash)","title":" Demo"},{"location":"Utils/#topositive","text":"int toPositive ( int number ) toPositive converts a number to a positive value.","title":" toPositive"},{"location":"building-from-sources/","text":"Building from Sources \u00b6 Based on README.md : KAFKA_VERSION=2.8.0 ./gradlew clean releaseTarGz install && \\ tar -zxvf core/build/distributions/kafka_2.13-$KAFKA_VERSION.tgz) cd kafka_2.13-$KAFKA_VERSION $ ./bin/kafka-server-start.sh --version [2021-09-12 19:00:12,467] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$) 2.8.0 (Commit:ebb1d6e21cc92130)","title":"Building from Sources"},{"location":"building-from-sources/#building-from-sources","text":"Based on README.md : KAFKA_VERSION=2.8.0 ./gradlew clean releaseTarGz install && \\ tar -zxvf core/build/distributions/kafka_2.13-$KAFKA_VERSION.tgz) cd kafka_2.13-$KAFKA_VERSION $ ./bin/kafka-server-start.sh --version [2021-09-12 19:00:12,467] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$) 2.8.0 (Commit:ebb1d6e21cc92130)","title":"Building from Sources"},{"location":"logging/","text":"Logging \u00b6 Brokers \u00b6 Kafka brokers use Apache Log4j 2 for logging and use config/log4j.properties by default. The default logging level is INFO with stdout appender. log4j.rootLogger=INFO, stdout, kafkaAppender log4j.logger.kafka=INFO log4j.logger.org.apache.kafka=INFO Tools \u00b6 Kafka tools (e.g. kafka-console-producer ) use config/tools-log4j.properties as the logging configuration file. Clients \u00b6 build.sbt \u00b6 libraryDependencies += \"org.apache.kafka\" % \"kafka-clients\" % \"2.8.0\" val slf4j = \"1.7.32\" libraryDependencies += \"org.slf4j\" % \"slf4j-api\" % slf4j libraryDependencies += \"org.slf4j\" % \"slf4j-log4j12\" % slf4j log4j.properties \u00b6 In src/main/resources/log4j.properties use the following: log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL","title":"Logging"},{"location":"logging/#logging","text":"","title":"Logging"},{"location":"logging/#brokers","text":"Kafka brokers use Apache Log4j 2 for logging and use config/log4j.properties by default. The default logging level is INFO with stdout appender. log4j.rootLogger=INFO, stdout, kafkaAppender log4j.logger.kafka=INFO log4j.logger.org.apache.kafka=INFO","title":"Brokers"},{"location":"logging/#tools","text":"Kafka tools (e.g. kafka-console-producer ) use config/tools-log4j.properties as the logging configuration file.","title":"Tools"},{"location":"logging/#clients","text":"","title":"Clients"},{"location":"logging/#buildsbt","text":"libraryDependencies += \"org.apache.kafka\" % \"kafka-clients\" % \"2.8.0\" val slf4j = \"1.7.32\" libraryDependencies += \"org.slf4j\" % \"slf4j-api\" % slf4j libraryDependencies += \"org.slf4j\" % \"slf4j-log4j12\" % slf4j","title":"build.sbt"},{"location":"logging/#log4jproperties","text":"In src/main/resources/log4j.properties use the following: log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL","title":"log4j.properties"},{"location":"overview/","text":"Apache Kafka \u00b6 Apache Kafka is an open source project for a distributed publish-subscribe messaging system rethought as a distributed commit log . Messages \u00b6 Messages ( records , events ) are byte arrays (String, JSON, and Avro are among the most common formats). If a message has a key, Kafka (uses Partitioner ) to make sure that all messages of the same key are in the same partition. Topics \u00b6 Kafka stores messages in topics that are partitioned and replicated across multiple brokers in a cluster. Kafka Clients \u00b6 Producers send messages to topics from which consumers read. Language Agnostic \u00b6 Kafka clients use binary protocol to talk to a Kafka cluster. Consumer Groups \u00b6 Consumers may be grouped in a consumer group with multiple consumers. Each consumer in a consumer group will read messages from a unique subset of partitions in each topic they subscribe to. Each message is delivered to one consumer in the group, and all messages with the same key arrive to the same consumer. Durability \u00b6 Kafka does not track which messages were read by consumers. Kafka keeps all messages for a finite amount of time, and it is consumers' responsibility to track their location per topic (offsets).","title":"Overview"},{"location":"overview/#apache-kafka","text":"Apache Kafka is an open source project for a distributed publish-subscribe messaging system rethought as a distributed commit log .","title":"Apache Kafka"},{"location":"overview/#messages","text":"Messages ( records , events ) are byte arrays (String, JSON, and Avro are among the most common formats). If a message has a key, Kafka (uses Partitioner ) to make sure that all messages of the same key are in the same partition.","title":"Messages"},{"location":"overview/#topics","text":"Kafka stores messages in topics that are partitioned and replicated across multiple brokers in a cluster.","title":"Topics"},{"location":"overview/#kafka-clients","text":"Producers send messages to topics from which consumers read.","title":"Kafka Clients"},{"location":"overview/#language-agnostic","text":"Kafka clients use binary protocol to talk to a Kafka cluster.","title":"Language Agnostic"},{"location":"overview/#consumer-groups","text":"Consumers may be grouped in a consumer group with multiple consumers. Each consumer in a consumer group will read messages from a unique subset of partitions in each topic they subscribe to. Each message is delivered to one consumer in the group, and all messages with the same key arrive to the same consumer.","title":"Consumer Groups"},{"location":"overview/#durability","text":"Kafka does not track which messages were read by consumers. Kafka keeps all messages for a finite amount of time, and it is consumers' responsibility to track their location per topic (offsets).","title":"Durability"},{"location":"clients/","text":"Kafka Clients \u00b6","title":"Kafka Clients"},{"location":"clients/#kafka-clients","text":"","title":"Kafka Clients"},{"location":"clients/CommonClientConfigs/","text":"CommonClientConfigs \u00b6 client.id \u00b6 retries \u00b6 retry.backoff.ms \u00b6","title":"CommonClientConfigs"},{"location":"clients/CommonClientConfigs/#commonclientconfigs","text":"","title":"CommonClientConfigs"},{"location":"clients/CommonClientConfigs/#clientid","text":"","title":" client.id"},{"location":"clients/CommonClientConfigs/#retries","text":"","title":" retries"},{"location":"clients/CommonClientConfigs/#retrybackoffms","text":"","title":" retry.backoff.ms"},{"location":"clients/KafkaClient/","text":"KafkaClient \u00b6 KafkaClient is an interface to NetworkClient . Contract \u00b6 inFlightRequestCount \u00b6 int inFlightRequestCount () int inFlightRequestCount ( String nodeId ) Used when: ConsumerNetworkClient is requested to pendingRequestCount and poll Sender is requested to run SenderMetrics is requested for requests-in-flight performance metric leastLoadedNode \u00b6 Node leastLoadedNode ( long now ) Used when: ConsumerNetworkClient is requested for the leastLoadedNode DefaultMetadataUpdater is requested to maybeUpdate KafkaAdminClient is used Sender is requested for the maybeSendAndPollTransactionalRequest newClientRequest \u00b6 ClientRequest newClientRequest ( String nodeId , AbstractRequest . Builder <?> requestBuilder , long createdTimeMs , boolean expectResponse ) ClientRequest newClientRequest ( String nodeId , AbstractRequest . Builder <?> requestBuilder , long createdTimeMs , boolean expectResponse , int requestTimeoutMs , RequestCompletionHandler callback ) Used when: AdminClientRunnable is requested to sendEligibleCalls ConsumerNetworkClient is requested to send NetworkClient is requested to newClientRequest , sendInternalMetadataRequest and handleInitiateApiVersionRequests RequestSendThread is requested to doWork Sender is requested to run KafkaServer is requested to controlledShutdown ReplicaFetcherBlockingSend is requested to sendRequest ReplicaVerificationTool is used poll \u00b6 List < ClientResponse > poll ( long timeout , long now ) Used when: FIXME pollDelayMs \u00b6 long pollDelayMs ( Node node , long now ) Used when: FIXME Is Node Ready and Connected \u00b6 boolean ready ( Node node , long now ); Used when: AdminClientRunnable is requested to sendEligibleCalls ConsumerNetworkClient is requested to tryConnect and trySend InterBrokerSendThread is requested to sendRequests NetworkClientUtils is requested to awaitReady Sender is requested to sendProducerData send \u00b6 void send ( ClientRequest request , long now ) Used when: FIXME wakeup \u00b6 void wakeup () Used when: FIXME Implementations \u00b6 NetworkClient Closeable \u00b6 KafkaClient is a Closeable ( Java ).","title":"KafkaClient"},{"location":"clients/KafkaClient/#kafkaclient","text":"KafkaClient is an interface to NetworkClient .","title":"KafkaClient"},{"location":"clients/KafkaClient/#contract","text":"","title":"Contract"},{"location":"clients/KafkaClient/#inflightrequestcount","text":"int inFlightRequestCount () int inFlightRequestCount ( String nodeId ) Used when: ConsumerNetworkClient is requested to pendingRequestCount and poll Sender is requested to run SenderMetrics is requested for requests-in-flight performance metric","title":" inFlightRequestCount"},{"location":"clients/KafkaClient/#leastloadednode","text":"Node leastLoadedNode ( long now ) Used when: ConsumerNetworkClient is requested for the leastLoadedNode DefaultMetadataUpdater is requested to maybeUpdate KafkaAdminClient is used Sender is requested for the maybeSendAndPollTransactionalRequest","title":" leastLoadedNode"},{"location":"clients/KafkaClient/#newclientrequest","text":"ClientRequest newClientRequest ( String nodeId , AbstractRequest . Builder <?> requestBuilder , long createdTimeMs , boolean expectResponse ) ClientRequest newClientRequest ( String nodeId , AbstractRequest . Builder <?> requestBuilder , long createdTimeMs , boolean expectResponse , int requestTimeoutMs , RequestCompletionHandler callback ) Used when: AdminClientRunnable is requested to sendEligibleCalls ConsumerNetworkClient is requested to send NetworkClient is requested to newClientRequest , sendInternalMetadataRequest and handleInitiateApiVersionRequests RequestSendThread is requested to doWork Sender is requested to run KafkaServer is requested to controlledShutdown ReplicaFetcherBlockingSend is requested to sendRequest ReplicaVerificationTool is used","title":" newClientRequest"},{"location":"clients/KafkaClient/#poll","text":"List < ClientResponse > poll ( long timeout , long now ) Used when: FIXME","title":" poll"},{"location":"clients/KafkaClient/#polldelayms","text":"long pollDelayMs ( Node node , long now ) Used when: FIXME","title":" pollDelayMs"},{"location":"clients/KafkaClient/#is-node-ready-and-connected","text":"boolean ready ( Node node , long now ); Used when: AdminClientRunnable is requested to sendEligibleCalls ConsumerNetworkClient is requested to tryConnect and trySend InterBrokerSendThread is requested to sendRequests NetworkClientUtils is requested to awaitReady Sender is requested to sendProducerData","title":" Is Node Ready and Connected"},{"location":"clients/KafkaClient/#send","text":"void send ( ClientRequest request , long now ) Used when: FIXME","title":" send"},{"location":"clients/KafkaClient/#wakeup","text":"void wakeup () Used when: FIXME","title":" wakeup"},{"location":"clients/KafkaClient/#implementations","text":"NetworkClient","title":"Implementations"},{"location":"clients/KafkaClient/#closeable","text":"KafkaClient is a Closeable ( Java ).","title":" Closeable"},{"location":"clients/Metadata/","text":"Metadata \u00b6 update \u00b6 void update ( int requestVersion , MetadataResponse response , boolean isPartialUpdate , long nowMs ) update ...FIXME update is used when: ProducerMetadata is requested to update Metadata is requested to updateWithCurrentRequestVersion DefaultMetadataUpdater is requested to handleSuccessfulResponse","title":"Metadata"},{"location":"clients/Metadata/#metadata","text":"","title":"Metadata"},{"location":"clients/Metadata/#update","text":"void update ( int requestVersion , MetadataResponse response , boolean isPartialUpdate , long nowMs ) update ...FIXME update is used when: ProducerMetadata is requested to update Metadata is requested to updateWithCurrentRequestVersion DefaultMetadataUpdater is requested to handleSuccessfulResponse","title":" update"},{"location":"clients/MetadataUpdater/","text":"MetadataUpdater \u00b6 MetadataUpdater is...FIXME","title":"MetadataUpdater"},{"location":"clients/MetadataUpdater/#metadataupdater","text":"MetadataUpdater is...FIXME","title":"MetadataUpdater"},{"location":"clients/NetworkClient/","text":"NetworkClient \u00b6 NetworkClient is a KafkaClient . leastLoadedNode \u00b6 Node leastLoadedNode ( long now ) leastLoadedNode requests the MetadataUpdater for the nodes (in a non-blocking fashion). leastLoadedNode generates a random number to offset the first node to start checking node candidates from. leastLoadedNode finds three nodes: foundReady that is ready (perhaps with some in-flight requests ) foundConnecting with a connection already being established (using the ClusterConnectionStates registry) foundCanConnect that can be connected When a node is found that is ready and has no in-flight requests , leastLoadedNode prints out the following TRACE message to the logs and returns the node immediately: Found least loaded node [node] connected with no in-flight requests When a node candidate does not meet any of the above requirements, leastLoadedNode prints out the following TRACE message to the logs: Removing node [node] from least loaded node selection since it is neither ready for sending nor connecting leastLoadedNode prefers the foundReady node over the foundConnecting with the foundCanConnect as the last resort. When no node could be found, leastLoadedNode prints out the following TRACE message to the logs (and returns null ): Least loaded node selection failed to find an available node leastLoadedNode is part of the KafkaClient abstraction.","title":"NetworkClient"},{"location":"clients/NetworkClient/#networkclient","text":"NetworkClient is a KafkaClient .","title":"NetworkClient"},{"location":"clients/NetworkClient/#leastloadednode","text":"Node leastLoadedNode ( long now ) leastLoadedNode requests the MetadataUpdater for the nodes (in a non-blocking fashion). leastLoadedNode generates a random number to offset the first node to start checking node candidates from. leastLoadedNode finds three nodes: foundReady that is ready (perhaps with some in-flight requests ) foundConnecting with a connection already being established (using the ClusterConnectionStates registry) foundCanConnect that can be connected When a node is found that is ready and has no in-flight requests , leastLoadedNode prints out the following TRACE message to the logs and returns the node immediately: Found least loaded node [node] connected with no in-flight requests When a node candidate does not meet any of the above requirements, leastLoadedNode prints out the following TRACE message to the logs: Removing node [node] from least loaded node selection since it is neither ready for sending nor connecting leastLoadedNode prefers the foundReady node over the foundConnecting with the foundCanConnect as the last resort. When no node could be found, leastLoadedNode prints out the following TRACE message to the logs (and returns null ): Least loaded node selection failed to find an available node leastLoadedNode is part of the KafkaClient abstraction.","title":" leastLoadedNode"},{"location":"clients/NetworkClientUtils/","text":"NetworkClientUtils \u00b6","title":"NetworkClientUtils"},{"location":"clients/NetworkClientUtils/#networkclientutils","text":"","title":"NetworkClientUtils"},{"location":"clients/admin/","text":"Kafka Admin \u00b6","title":"Kafka Admin"},{"location":"clients/admin/#kafka-admin","text":"","title":"Kafka Admin"},{"location":"clients/admin/AdminClientRunnable/","text":"AdminClientRunnable \u00b6","title":"AdminClientRunnable"},{"location":"clients/admin/AdminClientRunnable/#adminclientrunnable","text":"","title":"AdminClientRunnable"},{"location":"clients/consumer/","text":"Kafka Consumers \u00b6","title":"Kafka Consumers"},{"location":"clients/consumer/#kafka-consumers","text":"","title":"Kafka Consumers"},{"location":"clients/consumer/ConsumerNetworkClient/","text":"ConsumerNetworkClient \u00b6","title":"ConsumerNetworkClient"},{"location":"clients/consumer/ConsumerNetworkClient/#consumernetworkclient","text":"","title":"ConsumerNetworkClient"},{"location":"clients/producer/","text":"Kafka Producers \u00b6 KafkaProducer uses Sender to send records to a Kafka cluster. KafkaProducer can be transactional or idempotent (and associated with a TransactionManager ).","title":"Kafka Producers"},{"location":"clients/producer/#kafka-producers","text":"KafkaProducer uses Sender to send records to a Kafka cluster. KafkaProducer can be transactional or idempotent (and associated with a TransactionManager ).","title":"Kafka Producers"},{"location":"clients/producer/BufferPool/","text":"BufferPool \u00b6 BufferPool is...FIXME","title":"BufferPool"},{"location":"clients/producer/BufferPool/#bufferpool","text":"BufferPool is...FIXME","title":"BufferPool"},{"location":"clients/producer/DefaultPartitioner/","text":"DefaultPartitioner \u00b6 DefaultPartitioner is a Partitioner . Demo \u00b6 import org.apache.kafka.clients.producer.internals.DefaultPartitioner val partitioner = new DefaultPartitioner val keyBytes = \"hello\".getBytes val numPartitions = 3 val p = partitioner.partition(null, null, keyBytes, null, null, null, numPartitions) println(p) The following snippet should generate the same partition value (since it is exactly how DefaultPartitioner does it). import org.apache.kafka.common.utils.Utils val keyBytes = \"hello\".getBytes val numPartitions = 3 val p = Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions println(p)","title":"DefaultPartitioner"},{"location":"clients/producer/DefaultPartitioner/#defaultpartitioner","text":"DefaultPartitioner is a Partitioner .","title":"DefaultPartitioner"},{"location":"clients/producer/DefaultPartitioner/#demo","text":"import org.apache.kafka.clients.producer.internals.DefaultPartitioner val partitioner = new DefaultPartitioner val keyBytes = \"hello\".getBytes val numPartitions = 3 val p = partitioner.partition(null, null, keyBytes, null, null, null, numPartitions) println(p) The following snippet should generate the same partition value (since it is exactly how DefaultPartitioner does it). import org.apache.kafka.common.utils.Utils val keyBytes = \"hello\".getBytes val numPartitions = 3 val p = Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions println(p)","title":"Demo"},{"location":"clients/producer/KafkaProducer/","text":"KafkaProducer \u00b6 KafkaProducer<K, V> is a concrete Producer . Creating Instance \u00b6 KafkaProducer takes the following to be created: ProducerConfig Key Serializer<K> Value Serializer<V> ProducerMetadata KafkaClient ProducerInterceptor<K, V> s Time configureTransactionState \u00b6 TransactionManager configureTransactionState ( ProducerConfig config , LogContext logContext ) configureTransactionState creates a new TransactionManager or returns null . configureTransactionState checks whether the following configuration properties are specified in the given ProducerConfig : enable.idempotence transactional.id With transactional.id specified, configureTransactionState turns the enable.idempotence on and prints out the following INFO message to the logs: Overriding the default [enable.idempotence] to true since transactional.id is specified. With idempotence enabled , configureTransactionState creates a TransactionManager with the values of the following configuration properties: transactional.id transaction.timeout.ms retry.backoff.ms internal.auto.downgrade.txn.commit When the TransactionManager is transactional , configureTransactionState prints out the following INFO message to the logs: Instantiated a transactional producer. Otherwise, configureTransactionState prints out the following INFO message to the logs: Instantiated an idempotent producer. In the end, configureTransactionState returns the TransactionManager or null . newSender \u00b6 Sender newSender ( LogContext logContext , KafkaClient kafkaClient , ProducerMetadata metadata ) newSender ...FIXME configureInflightRequests \u00b6 int configureInflightRequests ( ProducerConfig config ) configureInflightRequests gives the value of the max.in.flight.requests.per.connection (in the given ProducerConfig ). configureInflightRequests throws a ConfigException when the idempotence is enabled and the value of the max.in.flight.requests.per.connection is above 5: Must set max.in.flight.requests.per.connection to at most 5 to use the idempotent producer. configureAcks \u00b6 short configureAcks ( ProducerConfig config , Logger log ) configureAcks returns the value of acks configuration property (in the given ProducerConfig ). With idempotenceEnabled , configureAcks prints out the following INFO message to the logs when there is no acks configuration property defined: Overriding the default [acks] to all since idempotence is enabled. With idempotenceEnabled and the acks not -1 , configureAcks throws a ConfigException : Must set acks to all in order to use the idempotent producer. Otherwise we cannot guarantee idempotence. TransactionManager \u00b6 KafkaProducer may create a TransactionManager when created . TransactionManager is used to create the following: RecordAccumulator Sender KafkaProducer uses the TransactionManager for the following transactional methods: abortTransaction beginTransaction commitTransaction initTransactions sendOffsetsToTransaction doSend throwIfNoTransactionManager \u00b6 KafkaProducer throws an IllegalStateException for the transactional methods but TransactionManager is not configured. Cannot use transactional methods without enabling transactions by setting the transactional.id configuration property Sender Thread \u00b6 KafkaProducer creates a Sender when created . Sender is immediately started as a daemon thread with the following name (using the clientId ): kafka-producer-network-thread | [clientId] KafkaProducer is actually considered open (and usable) as long as the Sender is running . KafkaProducer simply requests the Sender to wake up for the following: initTransactions sendOffsetsToTransaction commitTransaction abortTransaction doSend waitOnMetadata flush RecordAccumulator \u00b6 KafkaProducer creates a RecordAccumulator when created . This RecordAccumulator is used for the following: Create a Sender append when doSend beginFlush when flush Aborting Incomplete Transaction \u00b6 void abortTransaction () abortTransaction prints out the following INFO message to the logs: Aborting incomplete transaction abortTransaction ...FIXME abortTransaction is part of the Producer abstraction. Sending Record \u00b6 Future < RecordMetadata > send ( ProducerRecord < K , V > record , Callback callback ) send ...FIXME send is part of the Producer abstraction. doSend \u00b6 Future < RecordMetadata > doSend ( ProducerRecord < K , V > record , Callback callback ) doSend ...FIXME partition \u00b6 int partition ( ProducerRecord < K , V > record , byte [] serializedKey , byte [] serializedValue , Cluster cluster ) partition is the partition (of the given ProducerRecord ) if defined or requests the Partitioner for the partition . Flushing \u00b6 void flush () flush requests the RecordAccumulator to beginFlush . flush requests the Sender to wakeup . flush requests the RecordAccumulator to awaitFlushCompletion . flush is part of the Producer abstraction. waitOnMetadata \u00b6 ClusterAndWaitTime waitOnMetadata ( String topic , Integer partition , long nowMs , long maxWaitMs ) waitOnMetadata requests the ProducerMetadata for the current cluster info . waitOnMetadata ...FIXME waitOnMetadata is used when: KafkaProducer is requested to doSend and partitionsFor Demo \u00b6 // Necessary imports import org.apache.kafka.clients.producer.KafkaProducer import org.apache.kafka.clients.producer.ProducerConfig import org.apache.kafka.common.serialization.StringSerializer // Creating a KafkaProducer import java.util.Properties val props = new Properties() props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \":9092\") val producer = new KafkaProducer[String, String](props) // Creating a record to be sent import org.apache.kafka.clients.producer.ProducerRecord val r = new ProducerRecord[String, String](\"0\", \"this is a message\") // Sending the record (with no Callback) import java.util.concurrent.Future import org.apache.kafka.clients.producer.RecordMetadata val metadataF: Future[RecordMetadata] = producer.send(r) Logging \u00b6 Enable ALL logging level for org.apache.kafka.clients.producer.KafkaProducer logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL Refer to Logging .","title":"KafkaProducer"},{"location":"clients/producer/KafkaProducer/#kafkaproducer","text":"KafkaProducer<K, V> is a concrete Producer .","title":"KafkaProducer"},{"location":"clients/producer/KafkaProducer/#creating-instance","text":"KafkaProducer takes the following to be created: ProducerConfig Key Serializer<K> Value Serializer<V> ProducerMetadata KafkaClient ProducerInterceptor<K, V> s Time","title":"Creating Instance"},{"location":"clients/producer/KafkaProducer/#configuretransactionstate","text":"TransactionManager configureTransactionState ( ProducerConfig config , LogContext logContext ) configureTransactionState creates a new TransactionManager or returns null . configureTransactionState checks whether the following configuration properties are specified in the given ProducerConfig : enable.idempotence transactional.id With transactional.id specified, configureTransactionState turns the enable.idempotence on and prints out the following INFO message to the logs: Overriding the default [enable.idempotence] to true since transactional.id is specified. With idempotence enabled , configureTransactionState creates a TransactionManager with the values of the following configuration properties: transactional.id transaction.timeout.ms retry.backoff.ms internal.auto.downgrade.txn.commit When the TransactionManager is transactional , configureTransactionState prints out the following INFO message to the logs: Instantiated a transactional producer. Otherwise, configureTransactionState prints out the following INFO message to the logs: Instantiated an idempotent producer. In the end, configureTransactionState returns the TransactionManager or null .","title":" configureTransactionState"},{"location":"clients/producer/KafkaProducer/#newsender","text":"Sender newSender ( LogContext logContext , KafkaClient kafkaClient , ProducerMetadata metadata ) newSender ...FIXME","title":" newSender"},{"location":"clients/producer/KafkaProducer/#configureinflightrequests","text":"int configureInflightRequests ( ProducerConfig config ) configureInflightRequests gives the value of the max.in.flight.requests.per.connection (in the given ProducerConfig ). configureInflightRequests throws a ConfigException when the idempotence is enabled and the value of the max.in.flight.requests.per.connection is above 5: Must set max.in.flight.requests.per.connection to at most 5 to use the idempotent producer.","title":" configureInflightRequests"},{"location":"clients/producer/KafkaProducer/#configureacks","text":"short configureAcks ( ProducerConfig config , Logger log ) configureAcks returns the value of acks configuration property (in the given ProducerConfig ). With idempotenceEnabled , configureAcks prints out the following INFO message to the logs when there is no acks configuration property defined: Overriding the default [acks] to all since idempotence is enabled. With idempotenceEnabled and the acks not -1 , configureAcks throws a ConfigException : Must set acks to all in order to use the idempotent producer. Otherwise we cannot guarantee idempotence.","title":" configureAcks"},{"location":"clients/producer/KafkaProducer/#transactionmanager","text":"KafkaProducer may create a TransactionManager when created . TransactionManager is used to create the following: RecordAccumulator Sender KafkaProducer uses the TransactionManager for the following transactional methods: abortTransaction beginTransaction commitTransaction initTransactions sendOffsetsToTransaction doSend","title":" TransactionManager"},{"location":"clients/producer/KafkaProducer/#throwifnotransactionmanager","text":"KafkaProducer throws an IllegalStateException for the transactional methods but TransactionManager is not configured. Cannot use transactional methods without enabling transactions by setting the transactional.id configuration property","title":" throwIfNoTransactionManager"},{"location":"clients/producer/KafkaProducer/#sender-thread","text":"KafkaProducer creates a Sender when created . Sender is immediately started as a daemon thread with the following name (using the clientId ): kafka-producer-network-thread | [clientId] KafkaProducer is actually considered open (and usable) as long as the Sender is running . KafkaProducer simply requests the Sender to wake up for the following: initTransactions sendOffsetsToTransaction commitTransaction abortTransaction doSend waitOnMetadata flush","title":" Sender Thread"},{"location":"clients/producer/KafkaProducer/#recordaccumulator","text":"KafkaProducer creates a RecordAccumulator when created . This RecordAccumulator is used for the following: Create a Sender append when doSend beginFlush when flush","title":" RecordAccumulator"},{"location":"clients/producer/KafkaProducer/#aborting-incomplete-transaction","text":"void abortTransaction () abortTransaction prints out the following INFO message to the logs: Aborting incomplete transaction abortTransaction ...FIXME abortTransaction is part of the Producer abstraction.","title":" Aborting Incomplete Transaction"},{"location":"clients/producer/KafkaProducer/#sending-record","text":"Future < RecordMetadata > send ( ProducerRecord < K , V > record , Callback callback ) send ...FIXME send is part of the Producer abstraction.","title":" Sending Record"},{"location":"clients/producer/KafkaProducer/#dosend","text":"Future < RecordMetadata > doSend ( ProducerRecord < K , V > record , Callback callback ) doSend ...FIXME","title":" doSend"},{"location":"clients/producer/KafkaProducer/#partition","text":"int partition ( ProducerRecord < K , V > record , byte [] serializedKey , byte [] serializedValue , Cluster cluster ) partition is the partition (of the given ProducerRecord ) if defined or requests the Partitioner for the partition .","title":" partition"},{"location":"clients/producer/KafkaProducer/#flushing","text":"void flush () flush requests the RecordAccumulator to beginFlush . flush requests the Sender to wakeup . flush requests the RecordAccumulator to awaitFlushCompletion . flush is part of the Producer abstraction.","title":" Flushing"},{"location":"clients/producer/KafkaProducer/#waitonmetadata","text":"ClusterAndWaitTime waitOnMetadata ( String topic , Integer partition , long nowMs , long maxWaitMs ) waitOnMetadata requests the ProducerMetadata for the current cluster info . waitOnMetadata ...FIXME waitOnMetadata is used when: KafkaProducer is requested to doSend and partitionsFor","title":" waitOnMetadata"},{"location":"clients/producer/KafkaProducer/#demo","text":"// Necessary imports import org.apache.kafka.clients.producer.KafkaProducer import org.apache.kafka.clients.producer.ProducerConfig import org.apache.kafka.common.serialization.StringSerializer // Creating a KafkaProducer import java.util.Properties val props = new Properties() props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \":9092\") val producer = new KafkaProducer[String, String](props) // Creating a record to be sent import org.apache.kafka.clients.producer.ProducerRecord val r = new ProducerRecord[String, String](\"0\", \"this is a message\") // Sending the record (with no Callback) import java.util.concurrent.Future import org.apache.kafka.clients.producer.RecordMetadata val metadataF: Future[RecordMetadata] = producer.send(r)","title":"Demo"},{"location":"clients/producer/KafkaProducer/#logging","text":"Enable ALL logging level for org.apache.kafka.clients.producer.KafkaProducer logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL Refer to Logging .","title":"Logging"},{"location":"clients/producer/Partitioner/","text":"Partitioner \u00b6 Partitioner is an abstraction of partitioners for a KafkaProducer to determine the partition of records (to be sent out ). Configurable \u00b6 Partitioner is a Configurable . Closeable \u00b6 Partitioner is a Closeable ( Java ). Contract \u00b6 onNewBatch \u00b6 void onNewBatch ( String topic , Cluster cluster , int prevPartition ) Used when: KafkaProducer is requested to send a record (and doSend ) Computing Partition \u00b6 int partition ( String topic , Object key , byte [] keyBytes , Object value , byte [] valueBytes , Cluster cluster ) Used when: KafkaProducer is requested to send a record (and determines the partition ) Implementations \u00b6 DefaultPartitioner UniformStickyPartitioner RoundRobinPartitioner","title":"Partitioner"},{"location":"clients/producer/Partitioner/#partitioner","text":"Partitioner is an abstraction of partitioners for a KafkaProducer to determine the partition of records (to be sent out ).","title":"Partitioner"},{"location":"clients/producer/Partitioner/#configurable","text":"Partitioner is a Configurable .","title":" Configurable"},{"location":"clients/producer/Partitioner/#closeable","text":"Partitioner is a Closeable ( Java ).","title":" Closeable"},{"location":"clients/producer/Partitioner/#contract","text":"","title":"Contract"},{"location":"clients/producer/Partitioner/#onnewbatch","text":"void onNewBatch ( String topic , Cluster cluster , int prevPartition ) Used when: KafkaProducer is requested to send a record (and doSend )","title":" onNewBatch"},{"location":"clients/producer/Partitioner/#computing-partition","text":"int partition ( String topic , Object key , byte [] keyBytes , Object value , byte [] valueBytes , Cluster cluster ) Used when: KafkaProducer is requested to send a record (and determines the partition )","title":" Computing Partition"},{"location":"clients/producer/Partitioner/#implementations","text":"DefaultPartitioner UniformStickyPartitioner RoundRobinPartitioner","title":"Implementations"},{"location":"clients/producer/Producer/","text":"Producer \u00b6 Producer<K, V> is an interface to KafkaProducer for Kafka developers to use to send messages (with K keys and V values) to a Kafka cluster. Contract (Subset) \u00b6 abortTransaction \u00b6 void abortTransaction () beginTransaction \u00b6 void beginTransaction () Used when: FIXME commitTransaction \u00b6 void commitTransaction () Used when: FIXME initTransactions \u00b6 void initTransactions () Used when: FIXME sendOffsetsToTransaction \u00b6 void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , String consumerGroupId ) void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , ConsumerGroupMetadata groupMetadata ) Used when: FIXME","title":"Producer"},{"location":"clients/producer/Producer/#producer","text":"Producer<K, V> is an interface to KafkaProducer for Kafka developers to use to send messages (with K keys and V values) to a Kafka cluster.","title":"Producer"},{"location":"clients/producer/Producer/#contract-subset","text":"","title":"Contract (Subset)"},{"location":"clients/producer/Producer/#aborttransaction","text":"void abortTransaction ()","title":" abortTransaction"},{"location":"clients/producer/Producer/#begintransaction","text":"void beginTransaction () Used when: FIXME","title":" beginTransaction"},{"location":"clients/producer/Producer/#committransaction","text":"void commitTransaction () Used when: FIXME","title":" commitTransaction"},{"location":"clients/producer/Producer/#inittransactions","text":"void initTransactions () Used when: FIXME","title":" initTransactions"},{"location":"clients/producer/Producer/#sendoffsetstotransaction","text":"void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , String consumerGroupId ) void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , ConsumerGroupMetadata groupMetadata ) Used when: FIXME","title":" sendOffsetsToTransaction"},{"location":"clients/producer/ProducerBatch/","text":"ProducerBatch \u00b6 Creating Instance \u00b6 ProducerBatch takes the following to be created: TopicPartition MemoryRecordsBuilder createdMs isSplitBatch flag (default: false ) ProducerBatch is created when: ProducerBatch is requested to createBatchOffAccumulatorForRecord RecordAccumulator is requested to append a record tryAppend \u00b6 FutureRecordMetadata tryAppend ( long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long now ) tryAppend ...FIXME tryAppend is used when: RecordAccumulator is requested to append a record","title":"ProducerBatch"},{"location":"clients/producer/ProducerBatch/#producerbatch","text":"","title":"ProducerBatch"},{"location":"clients/producer/ProducerBatch/#creating-instance","text":"ProducerBatch takes the following to be created: TopicPartition MemoryRecordsBuilder createdMs isSplitBatch flag (default: false ) ProducerBatch is created when: ProducerBatch is requested to createBatchOffAccumulatorForRecord RecordAccumulator is requested to append a record","title":"Creating Instance"},{"location":"clients/producer/ProducerBatch/#tryappend","text":"FutureRecordMetadata tryAppend ( long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long now ) tryAppend ...FIXME tryAppend is used when: RecordAccumulator is requested to append a record","title":" tryAppend"},{"location":"clients/producer/ProducerConfig/","text":"ProducerConfig \u00b6 acks \u00b6 batch.size \u00b6 The buffer size allocated for a partition. When records are received (which are smaller than this size) KafkaProducer will attempt to optimistically group them together until this size is reached. Default: 16384 Must be at least 0 Related to: linger.ms max-partition-memory-bytes ( ConsoleProducer ) Used when: KafkaProducer is created (to create a RecordAccumulator and an accompanying BufferPool ) KafkaLog4jAppender is requested to activateOptions enable.idempotence \u00b6 Default: false Used when: KafkaProducer is requested to configureTransactionState ProducerConfig is requested to maybeOverrideEnableIdempotence and idempotenceEnabled internal.auto.downgrade.txn.commit \u00b6 linger.ms \u00b6 max.in.flight.requests.per.connection \u00b6 The maximum number of unacknowledged requests the client will send on a single connection before blocking. Note that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled). Default: 5 Must be at least 1 Related to: enable.idempotence retries Used when: KafkaProducer is requested to configureInflightRequests partitioner.class \u00b6 The class of the Partitioner for a KafkaProducer Default: DefaultPartitioner retries \u00b6 retry.backoff.ms \u00b6 retry.backoff.ms transactional.id \u00b6 The TransactionalId to use for transactional delivery. This enables reliability semantics which span multiple producer sessions since it allows the client to guarantee that transactions using the same TransactionalId have been completed prior to starting any new transactions. If no TransactionalId is provided, then the producer is limited to idempotent delivery. Default: (empty) If a TransactionalId is configured, enable.idempotence is implied (and configured when KafkaProducer is created ). Note that, by default, transactions require a cluster of at least three brokers which is the recommended setting for production; for development you can change this, by adjusting broker setting transaction.state.log.replication.factor . transaction.state.log.replication.factor \u00b6 transaction.timeout.ms \u00b6 idempotenceEnabled \u00b6 boolean idempotenceEnabled () idempotenceEnabled is enabled ( true ) when one of the following holds: transactional.id is defined enable.idempotence is enabled idempotenceEnabled throws a ConfigException when enable.idempotence is disabled but transactional.id is defined: Cannot set a transactional.id without also enabling idempotence. idempotenceEnabled is used when: KafkaProducer is created (and requested to configureTransactionState , configureInflightRequests , configureAcks ) ProducerConfig is requested to maybeOverrideAcksAndRetries postProcessParsedConfig \u00b6 Map < String , Object > postProcessParsedConfig ( Map < String , Object > parsedValues ) postProcessParsedConfig maybeOverrideEnableIdempotence . postProcessParsedConfig maybeOverrideClientId . postProcessParsedConfig maybeOverrideAcksAndRetries . postProcessParsedConfig is part of the AbstractConfig abstraction. maybeOverrideClientId \u00b6 maybeOverrideAcksAndRetries uses transactional.id (if defined) or the next available ID for an ID with producer- prefix for client.id unless already defined. maybeOverrideAcksAndRetries \u00b6 void maybeOverrideAcksAndRetries ( Map < String , Object > configs ) maybeOverrideAcksAndRetries ...FIXME maybeOverrideEnableIdempotence \u00b6 void maybeOverrideEnableIdempotence ( Map < String , Object > configs ) maybeOverrideEnableIdempotence ...FIXME","title":"ProducerConfig"},{"location":"clients/producer/ProducerConfig/#producerconfig","text":"","title":"ProducerConfig"},{"location":"clients/producer/ProducerConfig/#acks","text":"","title":" acks"},{"location":"clients/producer/ProducerConfig/#batchsize","text":"The buffer size allocated for a partition. When records are received (which are smaller than this size) KafkaProducer will attempt to optimistically group them together until this size is reached. Default: 16384 Must be at least 0 Related to: linger.ms max-partition-memory-bytes ( ConsoleProducer ) Used when: KafkaProducer is created (to create a RecordAccumulator and an accompanying BufferPool ) KafkaLog4jAppender is requested to activateOptions","title":" batch.size"},{"location":"clients/producer/ProducerConfig/#enableidempotence","text":"Default: false Used when: KafkaProducer is requested to configureTransactionState ProducerConfig is requested to maybeOverrideEnableIdempotence and idempotenceEnabled","title":" enable.idempotence"},{"location":"clients/producer/ProducerConfig/#internalautodowngradetxncommit","text":"","title":" internal.auto.downgrade.txn.commit"},{"location":"clients/producer/ProducerConfig/#lingerms","text":"","title":" linger.ms"},{"location":"clients/producer/ProducerConfig/#maxinflightrequestsperconnection","text":"The maximum number of unacknowledged requests the client will send on a single connection before blocking. Note that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled). Default: 5 Must be at least 1 Related to: enable.idempotence retries Used when: KafkaProducer is requested to configureInflightRequests","title":" max.in.flight.requests.per.connection"},{"location":"clients/producer/ProducerConfig/#partitionerclass","text":"The class of the Partitioner for a KafkaProducer Default: DefaultPartitioner","title":" partitioner.class"},{"location":"clients/producer/ProducerConfig/#retries","text":"","title":" retries"},{"location":"clients/producer/ProducerConfig/#retrybackoffms","text":"retry.backoff.ms","title":" retry.backoff.ms"},{"location":"clients/producer/ProducerConfig/#transactionalid","text":"The TransactionalId to use for transactional delivery. This enables reliability semantics which span multiple producer sessions since it allows the client to guarantee that transactions using the same TransactionalId have been completed prior to starting any new transactions. If no TransactionalId is provided, then the producer is limited to idempotent delivery. Default: (empty) If a TransactionalId is configured, enable.idempotence is implied (and configured when KafkaProducer is created ). Note that, by default, transactions require a cluster of at least three brokers which is the recommended setting for production; for development you can change this, by adjusting broker setting transaction.state.log.replication.factor .","title":" transactional.id"},{"location":"clients/producer/ProducerConfig/#transactionstatelogreplicationfactor","text":"","title":" transaction.state.log.replication.factor"},{"location":"clients/producer/ProducerConfig/#transactiontimeoutms","text":"","title":" transaction.timeout.ms"},{"location":"clients/producer/ProducerConfig/#idempotenceenabled","text":"boolean idempotenceEnabled () idempotenceEnabled is enabled ( true ) when one of the following holds: transactional.id is defined enable.idempotence is enabled idempotenceEnabled throws a ConfigException when enable.idempotence is disabled but transactional.id is defined: Cannot set a transactional.id without also enabling idempotence. idempotenceEnabled is used when: KafkaProducer is created (and requested to configureTransactionState , configureInflightRequests , configureAcks ) ProducerConfig is requested to maybeOverrideAcksAndRetries","title":" idempotenceEnabled"},{"location":"clients/producer/ProducerConfig/#postprocessparsedconfig","text":"Map < String , Object > postProcessParsedConfig ( Map < String , Object > parsedValues ) postProcessParsedConfig maybeOverrideEnableIdempotence . postProcessParsedConfig maybeOverrideClientId . postProcessParsedConfig maybeOverrideAcksAndRetries . postProcessParsedConfig is part of the AbstractConfig abstraction.","title":" postProcessParsedConfig"},{"location":"clients/producer/ProducerConfig/#maybeoverrideclientid","text":"maybeOverrideAcksAndRetries uses transactional.id (if defined) or the next available ID for an ID with producer- prefix for client.id unless already defined.","title":" maybeOverrideClientId"},{"location":"clients/producer/ProducerConfig/#maybeoverrideacksandretries","text":"void maybeOverrideAcksAndRetries ( Map < String , Object > configs ) maybeOverrideAcksAndRetries ...FIXME","title":" maybeOverrideAcksAndRetries"},{"location":"clients/producer/ProducerConfig/#maybeoverrideenableidempotence","text":"void maybeOverrideEnableIdempotence ( Map < String , Object > configs ) maybeOverrideEnableIdempotence ...FIXME","title":" maybeOverrideEnableIdempotence"},{"location":"clients/producer/RecordAccumulator/","text":"RecordAccumulator \u00b6 Creating Instance \u00b6 RecordAccumulator takes the following to be created: LogContext Batch size CompressionType Linger retryBackoffMs deliveryTimeoutMs Metrics Name of the Metrics Group Time ApiVersions TransactionManager BufferPool RecordAccumulator is created together with KafkaProducer . TransactionManager \u00b6 RecordAccumulator is given a TransactionManager when created . RecordAccumulator uses the TransactionManager when requested for the following: reenqueue splitAndReenqueue insertInSequenceOrder drain ( drainBatchesForOneNode and shouldStopDrainBatchesForPartition ) abortUndrainedBatches appendsInProgress Counter \u00b6 RecordAccumulator creates an AtomicInteger ( Java ) for appendsInProgress internal counter when created . appendsInProgress simply marks a single execution of append (and is incremented at the beginning and decremented right at the end). appendsInProgress is used when flushInProgress . flushInProgress \u00b6 boolean appendsInProgress () appendsInProgress indicates if the appendsInProgress counter is above 0 . appendsInProgress is used when abortIncompleteBatches . flushesInProgress Counter \u00b6 RecordAccumulator creates an AtomicInteger ( Java ) for flushesInProgress internal counter when created . flushesInProgress is incremented when beginFlush and decremented when awaitFlushCompletion . flushesInProgress is used when flushInProgress . flushInProgress \u00b6 boolean flushInProgress () flushInProgress indicates if the flushesInProgress counter is above 0 . flushInProgress is used when: RecordAccumulator is requested to ready Sender is requested to maybeSendAndPollTransactionalRequest Appending Record \u00b6 RecordAppendResult append ( TopicPartition tp , long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long maxTimeToBlock , boolean abortOnNewBatch , long nowMs ) append ...FIXME append is used when: KafkaProducer is requested to send a record (and doSend ) tryAppend \u00b6 RecordAppendResult tryAppend ( long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , Deque < ProducerBatch > deque , long nowMs ) tryAppend ...FIXME ready \u00b6 ReadyCheckResult ready ( Cluster cluster , long nowMs ) ready is a list of partitions with data ready to send. ready ...FIXME ready is used when: Sender is requested to sendProducerData beginFlush \u00b6 void beginFlush () beginFlush atomically increments the flushesInProgress counter. beginFlush is used when: KafkaProducer is requested to flush Sender is requested to maybeSendAndPollTransactionalRequest awaitFlushCompletion \u00b6 void awaitFlushCompletion () awaitFlushCompletion ...FIXME awaitFlushCompletion is used when: KafkaProducer is requested to flush splitAndReenqueue \u00b6 int splitAndReenqueue ( ProducerBatch bigBatch ) splitAndReenqueue ...FIXME splitAndReenqueue is used when: Sender is requested to completeBatch deallocate \u00b6 void deallocate ( ProducerBatch batch ) deallocate ...FIXME deallocate is used when: RecordAccumulator is requested to abortBatches and abortUndrainedBatches Sender is requested to maybeRemoveAndDeallocateBatch abortBatches \u00b6 void abortBatches () // (1) void abortBatches ( RuntimeException reason ) Uses a KafkaException abortBatches ...FIXME abortBatches is used when: RecordAccumulator is requested to abortIncompleteBatches Sender is requested to maybeAbortBatches abortIncompleteBatches \u00b6 void abortIncompleteBatches () abortIncompleteBatches abortBatches as long as there are appendsInProgress . abortIncompleteBatches abortBatches one last time (after no thread was appending in case there was a new batch appended by the last appending thread). In the end, abortIncompleteBatches clears the batches registry. abortIncompleteBatches is used when: Sender is requested to run (and forceClose ) abortUndrainedBatches \u00b6 void abortUndrainedBatches ( RuntimeException reason ) abortUndrainedBatches ...FIXME abortUndrainedBatches is used when: Sender is requested to maybeSendAndPollTransactionalRequest Incomplete (Pending) Batches \u00b6 RecordAccumulator creates an IncompleteBatches for incomplete internal registry of pending batches when created . RecordAccumulator uses the IncompleteBatches when: append (to add a new ProducerBatch ) splitAndReenqueue (to add a new ProducerBatch ) deallocate (to remove a ProducerBatch ) awaitFlushCompletion , abortBatches and abortUndrainedBatches (to copy all ProducerBatch s) hasIncomplete \u00b6 boolean hasIncomplete () hasIncomplete is true when the incomplete registry is not empty. hasIncomplete is used when: Sender is requested to maybeSendAndPollTransactionalRequest and maybeAbortBatches In-Progress Batches \u00b6 ConcurrentMap < TopicPartition , Deque < ProducerBatch >> batches RecordAccumulator creates a ConcurrentMap ( Java ) for the batches internal registry of in-progress ProducerBatch es (per TopicPartition ). RecordAccumulator adds a new ArrayDeque ( Java ) when getOrCreateDeque . batches is used when: expiredBatches ready hasUndrained getDeque batches abortIncompleteBatches getOrCreateDeque \u00b6 Deque < ProducerBatch > getOrCreateDeque ( TopicPartition tp ) getOrCreateDeque ...FIXME getOrCreateDeque is used when: RecordAccumulator is requested to append , reenqueue , splitAndReenqueue reenqueue \u00b6 void reenqueue ( ProducerBatch batch , long now ) reenqueue ...FIXME reenqueue is used when: Sender is requested to reenqueueBatch insertInSequenceOrder \u00b6 void insertInSequenceOrder ( Deque < ProducerBatch > deque , ProducerBatch batch ) insertInSequenceOrder ...FIXME insertInSequenceOrder is used when: RecordAccumulator is requested to reenqueue and splitAndReenqueue drain \u00b6 Map < Integer , List < ProducerBatch >> drain ( Cluster cluster , Set < Node > nodes , int maxSize , long now ) drain ...FIXME drain is used when: Sender is requested to sendProducerData drainBatchesForOneNode \u00b6 List < ProducerBatch > drainBatchesForOneNode ( Cluster cluster , Node node , int maxSize , long now ) drainBatchesForOneNode ...FIXME shouldStopDrainBatchesForPartition \u00b6 boolean shouldStopDrainBatchesForPartition ( ProducerBatch first , TopicPartition tp ) shouldStopDrainBatchesForPartition ...FIXME","title":"RecordAccumulator"},{"location":"clients/producer/RecordAccumulator/#recordaccumulator","text":"","title":"RecordAccumulator"},{"location":"clients/producer/RecordAccumulator/#creating-instance","text":"RecordAccumulator takes the following to be created: LogContext Batch size CompressionType Linger retryBackoffMs deliveryTimeoutMs Metrics Name of the Metrics Group Time ApiVersions TransactionManager BufferPool RecordAccumulator is created together with KafkaProducer .","title":"Creating Instance"},{"location":"clients/producer/RecordAccumulator/#transactionmanager","text":"RecordAccumulator is given a TransactionManager when created . RecordAccumulator uses the TransactionManager when requested for the following: reenqueue splitAndReenqueue insertInSequenceOrder drain ( drainBatchesForOneNode and shouldStopDrainBatchesForPartition ) abortUndrainedBatches","title":" TransactionManager"},{"location":"clients/producer/RecordAccumulator/#appendsinprogress-counter","text":"RecordAccumulator creates an AtomicInteger ( Java ) for appendsInProgress internal counter when created . appendsInProgress simply marks a single execution of append (and is incremented at the beginning and decremented right at the end). appendsInProgress is used when flushInProgress .","title":" appendsInProgress Counter"},{"location":"clients/producer/RecordAccumulator/#flushinprogress","text":"boolean appendsInProgress () appendsInProgress indicates if the appendsInProgress counter is above 0 . appendsInProgress is used when abortIncompleteBatches .","title":"flushInProgress"},{"location":"clients/producer/RecordAccumulator/#flushesinprogress-counter","text":"RecordAccumulator creates an AtomicInteger ( Java ) for flushesInProgress internal counter when created . flushesInProgress is incremented when beginFlush and decremented when awaitFlushCompletion . flushesInProgress is used when flushInProgress .","title":" flushesInProgress Counter"},{"location":"clients/producer/RecordAccumulator/#flushinprogress_1","text":"boolean flushInProgress () flushInProgress indicates if the flushesInProgress counter is above 0 . flushInProgress is used when: RecordAccumulator is requested to ready Sender is requested to maybeSendAndPollTransactionalRequest","title":" flushInProgress"},{"location":"clients/producer/RecordAccumulator/#appending-record","text":"RecordAppendResult append ( TopicPartition tp , long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long maxTimeToBlock , boolean abortOnNewBatch , long nowMs ) append ...FIXME append is used when: KafkaProducer is requested to send a record (and doSend )","title":" Appending Record"},{"location":"clients/producer/RecordAccumulator/#tryappend","text":"RecordAppendResult tryAppend ( long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , Deque < ProducerBatch > deque , long nowMs ) tryAppend ...FIXME","title":" tryAppend"},{"location":"clients/producer/RecordAccumulator/#ready","text":"ReadyCheckResult ready ( Cluster cluster , long nowMs ) ready is a list of partitions with data ready to send. ready ...FIXME ready is used when: Sender is requested to sendProducerData","title":" ready"},{"location":"clients/producer/RecordAccumulator/#beginflush","text":"void beginFlush () beginFlush atomically increments the flushesInProgress counter. beginFlush is used when: KafkaProducer is requested to flush Sender is requested to maybeSendAndPollTransactionalRequest","title":" beginFlush"},{"location":"clients/producer/RecordAccumulator/#awaitflushcompletion","text":"void awaitFlushCompletion () awaitFlushCompletion ...FIXME awaitFlushCompletion is used when: KafkaProducer is requested to flush","title":" awaitFlushCompletion"},{"location":"clients/producer/RecordAccumulator/#splitandreenqueue","text":"int splitAndReenqueue ( ProducerBatch bigBatch ) splitAndReenqueue ...FIXME splitAndReenqueue is used when: Sender is requested to completeBatch","title":" splitAndReenqueue"},{"location":"clients/producer/RecordAccumulator/#deallocate","text":"void deallocate ( ProducerBatch batch ) deallocate ...FIXME deallocate is used when: RecordAccumulator is requested to abortBatches and abortUndrainedBatches Sender is requested to maybeRemoveAndDeallocateBatch","title":" deallocate"},{"location":"clients/producer/RecordAccumulator/#abortbatches","text":"void abortBatches () // (1) void abortBatches ( RuntimeException reason ) Uses a KafkaException abortBatches ...FIXME abortBatches is used when: RecordAccumulator is requested to abortIncompleteBatches Sender is requested to maybeAbortBatches","title":" abortBatches"},{"location":"clients/producer/RecordAccumulator/#abortincompletebatches","text":"void abortIncompleteBatches () abortIncompleteBatches abortBatches as long as there are appendsInProgress . abortIncompleteBatches abortBatches one last time (after no thread was appending in case there was a new batch appended by the last appending thread). In the end, abortIncompleteBatches clears the batches registry. abortIncompleteBatches is used when: Sender is requested to run (and forceClose )","title":" abortIncompleteBatches"},{"location":"clients/producer/RecordAccumulator/#abortundrainedbatches","text":"void abortUndrainedBatches ( RuntimeException reason ) abortUndrainedBatches ...FIXME abortUndrainedBatches is used when: Sender is requested to maybeSendAndPollTransactionalRequest","title":" abortUndrainedBatches"},{"location":"clients/producer/RecordAccumulator/#incomplete-pending-batches","text":"RecordAccumulator creates an IncompleteBatches for incomplete internal registry of pending batches when created . RecordAccumulator uses the IncompleteBatches when: append (to add a new ProducerBatch ) splitAndReenqueue (to add a new ProducerBatch ) deallocate (to remove a ProducerBatch ) awaitFlushCompletion , abortBatches and abortUndrainedBatches (to copy all ProducerBatch s)","title":" Incomplete (Pending) Batches"},{"location":"clients/producer/RecordAccumulator/#hasincomplete","text":"boolean hasIncomplete () hasIncomplete is true when the incomplete registry is not empty. hasIncomplete is used when: Sender is requested to maybeSendAndPollTransactionalRequest and maybeAbortBatches","title":" hasIncomplete"},{"location":"clients/producer/RecordAccumulator/#in-progress-batches","text":"ConcurrentMap < TopicPartition , Deque < ProducerBatch >> batches RecordAccumulator creates a ConcurrentMap ( Java ) for the batches internal registry of in-progress ProducerBatch es (per TopicPartition ). RecordAccumulator adds a new ArrayDeque ( Java ) when getOrCreateDeque . batches is used when: expiredBatches ready hasUndrained getDeque batches abortIncompleteBatches","title":" In-Progress Batches"},{"location":"clients/producer/RecordAccumulator/#getorcreatedeque","text":"Deque < ProducerBatch > getOrCreateDeque ( TopicPartition tp ) getOrCreateDeque ...FIXME getOrCreateDeque is used when: RecordAccumulator is requested to append , reenqueue , splitAndReenqueue","title":" getOrCreateDeque"},{"location":"clients/producer/RecordAccumulator/#reenqueue","text":"void reenqueue ( ProducerBatch batch , long now ) reenqueue ...FIXME reenqueue is used when: Sender is requested to reenqueueBatch","title":" reenqueue"},{"location":"clients/producer/RecordAccumulator/#insertinsequenceorder","text":"void insertInSequenceOrder ( Deque < ProducerBatch > deque , ProducerBatch batch ) insertInSequenceOrder ...FIXME insertInSequenceOrder is used when: RecordAccumulator is requested to reenqueue and splitAndReenqueue","title":" insertInSequenceOrder"},{"location":"clients/producer/RecordAccumulator/#drain","text":"Map < Integer , List < ProducerBatch >> drain ( Cluster cluster , Set < Node > nodes , int maxSize , long now ) drain ...FIXME drain is used when: Sender is requested to sendProducerData","title":" drain"},{"location":"clients/producer/RecordAccumulator/#drainbatchesforonenode","text":"List < ProducerBatch > drainBatchesForOneNode ( Cluster cluster , Node node , int maxSize , long now ) drainBatchesForOneNode ...FIXME","title":" drainBatchesForOneNode"},{"location":"clients/producer/RecordAccumulator/#shouldstopdrainbatchesforpartition","text":"boolean shouldStopDrainBatchesForPartition ( ProducerBatch first , TopicPartition tp ) shouldStopDrainBatchesForPartition ...FIXME","title":" shouldStopDrainBatchesForPartition"},{"location":"clients/producer/Sender/","text":"Sender \u00b6 Sender is a Runnable ( Java ) that is executed as a separate thread alongside KafkaProducer to send records to a Kafka cluster. Creating Instance \u00b6 Sender takes the following to be created: LogContext KafkaClient ProducerMetadata RecordAccumulator guaranteeMessageOrder flag maxRequestSize acks retries SenderMetricsRegistry Time requestTimeoutMs retryBackoffMs TransactionManager ApiVersions Sender is created together with KafkaProducer . KafkaClient \u00b6 Sender is given a KafkaClient when created . Running Thread \u00b6 void run () run prints out the following DEBUG message to the logs: Starting Kafka producer I/O thread. run runs once and repeats until the running flag is turned off. Right after the running flag is off, run prints out the following DEBUG message to the logs: Beginning shutdown of Kafka producer I/O thread, sending remaining records. run ...FIXME In the end, run prints out the following DEBUG message to the logs: Shutdown of Kafka producer I/O thread has completed. run is part of the Runnable ( Java ) abstraction. runOnce \u00b6 void runOnce () If executed with a TransactionManager , runOnce ...FIXME runOnce sendProducerData . runOnce requests the KafkaClient to poll . sendProducerData \u00b6 long sendProducerData ( long now ) sendProducerData requests the ProducerMetadata for the current cluster info sendProducerData requests the RecordAccumulator for the partitions with data ready to send . sendProducerData requests a metadata update when there are partitions with no leaders. sendProducerData removes nodes not ready to send to. sendProducerData requests the RecordAccumulator to drain (and create ProducerBatch s). sendProducerData registers the batches (in the inFlightBatches registry). With guaranteeMessageOrder , sendProducerData mutes all the partitions drained. sendProducerData requests the RecordAccumulator to resetNextBatchExpiryTime . sendProducerData requests the RecordAccumulator for the expired batches and adds all expired InflightBatches . If there are any expired batches, sendProducerData ...FIXME sendProducerData requests the SenderMetrics to updateProduceRequestMetrics . With at least one broker to send batches to, sendProducerData prints out the following TRACE message to the logs: Nodes with data ready to send: [readyNodes] sendProducerData sendProduceRequests . sendProduceRequests \u00b6 void sendProduceRequests ( Map < Integer , List < ProducerBatch >> collated , long now ) For every pair of a broker node and an associated ProducerBatch (in the given collated collection), sendProduceRequests sendProduceRequest with the broker node, the acks , the requestTimeoutMs and the ProducerBatch . sendProduceRequest \u00b6 void sendProduceRequest ( long now , int destination , short acks , int timeout , List < ProducerBatch > batches ) sendProduceRequest creates a collection of ProducerBatch es by TopicPartition from the given batches . sendProduceRequest requests the KafkaClient for a new ClientRequest (for the destination broker) and to send it . sendProduceRequest registers a [handleProduceResponse] callback to invoke when a response arrives. sendProduceRequest expects a response for all the acks but 0 . In the end, sendProduceRequest prints out the following TRACE message to the logs: Sent produce request to [nodeId]: [requestBuilder] handleProduceResponse \u00b6 void handleProduceResponse ( ClientResponse response , Map < TopicPartition , ProducerBatch > batches , long now ) maybeSendAndPollTransactionalRequest \u00b6 boolean maybeSendAndPollTransactionalRequest () running Flag \u00b6 Sender runs as long as the running internal flag is on. The running flag is on from when Sender is created until requested to initiateClose . initiateClose \u00b6 void initiateClose () initiateClose requests the RecordAccumulator to close and turns the running flag off. In the end, initiateClose wakes up the KafkaClient . initiateClose is used when: KafkaProducer is requested to close Sender is requested to forceClose Waking Up \u00b6 void wakeup () wakeup requests the KafkaClient to wakeup . wakeup is used when: KafkaProducer is requested to initTransactions , sendOffsetsToTransaction , commitTransaction , abortTransaction , doSend , waitOnMetadata , flush Sender is requested to initiateClose Logging \u00b6 Enable ALL logging level for org.apache.kafka.clients.producer.internals.Sender logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.org.apache.kafka.clients.producer.internals.Sender=ALL Refer to Logging .","title":"Sender"},{"location":"clients/producer/Sender/#sender","text":"Sender is a Runnable ( Java ) that is executed as a separate thread alongside KafkaProducer to send records to a Kafka cluster.","title":"Sender"},{"location":"clients/producer/Sender/#creating-instance","text":"Sender takes the following to be created: LogContext KafkaClient ProducerMetadata RecordAccumulator guaranteeMessageOrder flag maxRequestSize acks retries SenderMetricsRegistry Time requestTimeoutMs retryBackoffMs TransactionManager ApiVersions Sender is created together with KafkaProducer .","title":"Creating Instance"},{"location":"clients/producer/Sender/#kafkaclient","text":"Sender is given a KafkaClient when created .","title":" KafkaClient"},{"location":"clients/producer/Sender/#running-thread","text":"void run () run prints out the following DEBUG message to the logs: Starting Kafka producer I/O thread. run runs once and repeats until the running flag is turned off. Right after the running flag is off, run prints out the following DEBUG message to the logs: Beginning shutdown of Kafka producer I/O thread, sending remaining records. run ...FIXME In the end, run prints out the following DEBUG message to the logs: Shutdown of Kafka producer I/O thread has completed. run is part of the Runnable ( Java ) abstraction.","title":" Running Thread"},{"location":"clients/producer/Sender/#runonce","text":"void runOnce () If executed with a TransactionManager , runOnce ...FIXME runOnce sendProducerData . runOnce requests the KafkaClient to poll .","title":" runOnce"},{"location":"clients/producer/Sender/#sendproducerdata","text":"long sendProducerData ( long now ) sendProducerData requests the ProducerMetadata for the current cluster info sendProducerData requests the RecordAccumulator for the partitions with data ready to send . sendProducerData requests a metadata update when there are partitions with no leaders. sendProducerData removes nodes not ready to send to. sendProducerData requests the RecordAccumulator to drain (and create ProducerBatch s). sendProducerData registers the batches (in the inFlightBatches registry). With guaranteeMessageOrder , sendProducerData mutes all the partitions drained. sendProducerData requests the RecordAccumulator to resetNextBatchExpiryTime . sendProducerData requests the RecordAccumulator for the expired batches and adds all expired InflightBatches . If there are any expired batches, sendProducerData ...FIXME sendProducerData requests the SenderMetrics to updateProduceRequestMetrics . With at least one broker to send batches to, sendProducerData prints out the following TRACE message to the logs: Nodes with data ready to send: [readyNodes] sendProducerData sendProduceRequests .","title":" sendProducerData"},{"location":"clients/producer/Sender/#sendproducerequests","text":"void sendProduceRequests ( Map < Integer , List < ProducerBatch >> collated , long now ) For every pair of a broker node and an associated ProducerBatch (in the given collated collection), sendProduceRequests sendProduceRequest with the broker node, the acks , the requestTimeoutMs and the ProducerBatch .","title":" sendProduceRequests"},{"location":"clients/producer/Sender/#sendproducerequest","text":"void sendProduceRequest ( long now , int destination , short acks , int timeout , List < ProducerBatch > batches ) sendProduceRequest creates a collection of ProducerBatch es by TopicPartition from the given batches . sendProduceRequest requests the KafkaClient for a new ClientRequest (for the destination broker) and to send it . sendProduceRequest registers a [handleProduceResponse] callback to invoke when a response arrives. sendProduceRequest expects a response for all the acks but 0 . In the end, sendProduceRequest prints out the following TRACE message to the logs: Sent produce request to [nodeId]: [requestBuilder]","title":" sendProduceRequest"},{"location":"clients/producer/Sender/#handleproduceresponse","text":"void handleProduceResponse ( ClientResponse response , Map < TopicPartition , ProducerBatch > batches , long now )","title":" handleProduceResponse"},{"location":"clients/producer/Sender/#maybesendandpolltransactionalrequest","text":"boolean maybeSendAndPollTransactionalRequest ()","title":" maybeSendAndPollTransactionalRequest"},{"location":"clients/producer/Sender/#running-flag","text":"Sender runs as long as the running internal flag is on. The running flag is on from when Sender is created until requested to initiateClose .","title":" running Flag"},{"location":"clients/producer/Sender/#initiateclose","text":"void initiateClose () initiateClose requests the RecordAccumulator to close and turns the running flag off. In the end, initiateClose wakes up the KafkaClient . initiateClose is used when: KafkaProducer is requested to close Sender is requested to forceClose","title":" initiateClose"},{"location":"clients/producer/Sender/#waking-up","text":"void wakeup () wakeup requests the KafkaClient to wakeup . wakeup is used when: KafkaProducer is requested to initTransactions , sendOffsetsToTransaction , commitTransaction , abortTransaction , doSend , waitOnMetadata , flush Sender is requested to initiateClose","title":" Waking Up"},{"location":"clients/producer/Sender/#logging","text":"Enable ALL logging level for org.apache.kafka.clients.producer.internals.Sender logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.org.apache.kafka.clients.producer.internals.Sender=ALL Refer to Logging .","title":"Logging"},{"location":"clients/producer/TransactionManager/","text":"TransactionManager \u00b6 TransactionManager is...FIXME","title":"TransactionManager"},{"location":"clients/producer/TransactionManager/#transactionmanager","text":"TransactionManager is...FIXME","title":"TransactionManager"},{"location":"demo/","text":"Demos \u00b6 The following demos are available: Kafka and kcat in Docker","title":"Demos"},{"location":"demo/#demos","text":"The following demos are available: Kafka and kcat in Docker","title":"Demos"},{"location":"demo/kafka-and-kcat-in-docker/","text":"Demo: Kafka and kcat in Docker \u00b6 This demo uses Docker to run Apache Kafka and kcat utility. kafka-docker \u00b6 Pull kafka-docker project (or create a docker-compose.yml file yourself). Running Kafka Cluster \u00b6 Start Zookeeper and Kafka containers. docker-compose up $ docker-compose ps Name Command State Ports ---------------------------------------------------------------------------------------------------------------------------------------- kafka-docker_kafka_1 start-kafka.sh Up 0.0.0.0:62687->9092/tcp kafka-docker_zookeeper_1 /bin/sh -c /usr/sbin/sshd ... Up 0.0.0.0:2181->2181/tcp,:::2181->2181/tcp, 22/tcp, 2888/tcp, 3888/tcp Docker Network \u00b6 The above creates a Docker network kafka-docker_default (if ran from kafka-docker directory as described in the official documentation of docker-compose ). $ docker network ls NETWORK ID NAME DRIVER SCOPE b8b255710858 bridge bridge local 3c9c3a969ef2 cda bridge local 398f9f3196aa host host local 68611503fde8 kafka-docker_default bridge local db43a5e50281 none null local kcat \u00b6 Connect kcat container to the network (using --network option as described in the official documentation of docker-compose ). Metadata Listing \u00b6 docker run -it --rm \\ --network kafka-docker_default \\ edenhill/kcat:1.7.0 \\ -b kafka-docker_kafka_1:9092 -L Metadata for all topics (from broker -1: kafka-docker_kafka_1:9092/bootstrap): 1 brokers: broker 1001 at 09cc8de4d067:9092 (controller) 0 topics: Producer \u00b6 docker run -it --rm \\ --network kafka-docker_default \\ --name producer \\ edenhill/kcat:1.7.0 \\ -b kafka-docker_kafka_1:9092 -P -t t1 Caution For some reason the above command couldn't send messages whenever I pressed ENTER but expected Ctrl+D instead (that terminates the shell and the container). Switching to confluentinc/cp-kafkacat made things working fine. docker run -it --rm \\ --network kafka-docker_default \\ --name producer \\ confluentinc/cp-kafkacat \\ kafkacat \\ -b kafka-docker_kafka_1:9092 -P -t t1 Consumer \u00b6 docker run -it --rm \\ --network kafka-docker_default \\ --name consumer \\ edenhill/kcat:1.7.0 \\ -b kafka-docker_kafka_1:9092 -C -t t1 Clean Up \u00b6 docker-compose down","title":"Kafka and kcat in Docker"},{"location":"demo/kafka-and-kcat-in-docker/#demo-kafka-and-kcat-in-docker","text":"This demo uses Docker to run Apache Kafka and kcat utility.","title":"Demo: Kafka and kcat in Docker"},{"location":"demo/kafka-and-kcat-in-docker/#kafka-docker","text":"Pull kafka-docker project (or create a docker-compose.yml file yourself).","title":"kafka-docker"},{"location":"demo/kafka-and-kcat-in-docker/#running-kafka-cluster","text":"Start Zookeeper and Kafka containers. docker-compose up $ docker-compose ps Name Command State Ports ---------------------------------------------------------------------------------------------------------------------------------------- kafka-docker_kafka_1 start-kafka.sh Up 0.0.0.0:62687->9092/tcp kafka-docker_zookeeper_1 /bin/sh -c /usr/sbin/sshd ... Up 0.0.0.0:2181->2181/tcp,:::2181->2181/tcp, 22/tcp, 2888/tcp, 3888/tcp","title":"Running Kafka Cluster"},{"location":"demo/kafka-and-kcat-in-docker/#docker-network","text":"The above creates a Docker network kafka-docker_default (if ran from kafka-docker directory as described in the official documentation of docker-compose ). $ docker network ls NETWORK ID NAME DRIVER SCOPE b8b255710858 bridge bridge local 3c9c3a969ef2 cda bridge local 398f9f3196aa host host local 68611503fde8 kafka-docker_default bridge local db43a5e50281 none null local","title":"Docker Network"},{"location":"demo/kafka-and-kcat-in-docker/#kcat","text":"Connect kcat container to the network (using --network option as described in the official documentation of docker-compose ).","title":"kcat"},{"location":"demo/kafka-and-kcat-in-docker/#metadata-listing","text":"docker run -it --rm \\ --network kafka-docker_default \\ edenhill/kcat:1.7.0 \\ -b kafka-docker_kafka_1:9092 -L Metadata for all topics (from broker -1: kafka-docker_kafka_1:9092/bootstrap): 1 brokers: broker 1001 at 09cc8de4d067:9092 (controller) 0 topics:","title":"Metadata Listing"},{"location":"demo/kafka-and-kcat-in-docker/#producer","text":"docker run -it --rm \\ --network kafka-docker_default \\ --name producer \\ edenhill/kcat:1.7.0 \\ -b kafka-docker_kafka_1:9092 -P -t t1 Caution For some reason the above command couldn't send messages whenever I pressed ENTER but expected Ctrl+D instead (that terminates the shell and the container). Switching to confluentinc/cp-kafkacat made things working fine. docker run -it --rm \\ --network kafka-docker_default \\ --name producer \\ confluentinc/cp-kafkacat \\ kafkacat \\ -b kafka-docker_kafka_1:9092 -P -t t1","title":"Producer"},{"location":"demo/kafka-and-kcat-in-docker/#consumer","text":"docker run -it --rm \\ --network kafka-docker_default \\ --name consumer \\ edenhill/kcat:1.7.0 \\ -b kafka-docker_kafka_1:9092 -C -t t1","title":"Consumer"},{"location":"demo/kafka-and-kcat-in-docker/#clean-up","text":"docker-compose down","title":"Clean Up"},{"location":"tools/","text":"Tools \u00b6","title":"Tools"},{"location":"tools/#tools","text":"","title":"Tools"},{"location":"tools/ReplicaVerificationTool/","text":"ReplicaVerificationTool \u00b6 ReplicaVerificationTool is...FIXME","title":"ReplicaVerificationTool"},{"location":"tools/ReplicaVerificationTool/#replicaverificationtool","text":"ReplicaVerificationTool is...FIXME","title":"ReplicaVerificationTool"}]}