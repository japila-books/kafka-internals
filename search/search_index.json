{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Apache Kafka 2.8.0 \u00b6 Welcome to The Internals of Apache Kafka online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Kafka as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Apache Kafka \ud83d\udd25 Last update: 2021-09-13","title":"Home"},{"location":"#the-internals-of-apache-kafka-280","text":"Welcome to The Internals of Apache Kafka online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Kafka as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Apache Kafka \ud83d\udd25 Last update: 2021-09-13","title":"The Internals of Apache Kafka 2.8.0"},{"location":"AbstractConfig/","text":"AbstractConfig \u00b6 AbstractConfig is...FIXME","title":"AbstractConfig"},{"location":"AbstractConfig/#abstractconfig","text":"AbstractConfig is...FIXME","title":"AbstractConfig"},{"location":"CommonClientConfigs/","text":"CommonClientConfigs \u00b6 client.id \u00b6","title":"CommonClientConfigs"},{"location":"CommonClientConfigs/#commonclientconfigs","text":"","title":"CommonClientConfigs"},{"location":"CommonClientConfigs/#clientid","text":"","title":" client.id"},{"location":"building-from-sources/","text":"Building from Sources \u00b6 Based on README.md : KAFKA_VERSION=2.8.0 ./gradlew clean releaseTarGz install && \\ tar -zxvf core/build/distributions/kafka_2.13-$KAFKA_VERSION.tgz) cd kafka_2.13-$KAFKA_VERSION $ ./bin/kafka-server-start.sh --version [2021-09-12 19:00:12,467] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$) 2.8.0 (Commit:ebb1d6e21cc92130)","title":"Building from Sources"},{"location":"building-from-sources/#building-from-sources","text":"Based on README.md : KAFKA_VERSION=2.8.0 ./gradlew clean releaseTarGz install && \\ tar -zxvf core/build/distributions/kafka_2.13-$KAFKA_VERSION.tgz) cd kafka_2.13-$KAFKA_VERSION $ ./bin/kafka-server-start.sh --version [2021-09-12 19:00:12,467] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$) 2.8.0 (Commit:ebb1d6e21cc92130)","title":"Building from Sources"},{"location":"logging/","text":"Logging \u00b6 Brokers \u00b6 Kafka brokers use Apache Log4j 2 for logging and use config/log4j.properties by default. The default logging level is INFO with stdout appender. log4j.rootLogger=INFO, stdout, kafkaAppender log4j.logger.kafka=INFO log4j.logger.org.apache.kafka=INFO Clients \u00b6 build.sbt \u00b6 libraryDependencies += \"org.apache.kafka\" % \"kafka-clients\" % \"2.8.0\" val slf4j = \"1.7.32\" libraryDependencies += \"org.slf4j\" % \"slf4j-api\" % slf4j libraryDependencies += \"org.slf4j\" % \"slf4j-log4j12\" % slf4j log4j.properties \u00b6 In src/main/resources/log4j.properties use the following: log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL","title":"Logging"},{"location":"logging/#logging","text":"","title":"Logging"},{"location":"logging/#brokers","text":"Kafka brokers use Apache Log4j 2 for logging and use config/log4j.properties by default. The default logging level is INFO with stdout appender. log4j.rootLogger=INFO, stdout, kafkaAppender log4j.logger.kafka=INFO log4j.logger.org.apache.kafka=INFO","title":"Brokers"},{"location":"logging/#clients","text":"","title":"Clients"},{"location":"logging/#buildsbt","text":"libraryDependencies += \"org.apache.kafka\" % \"kafka-clients\" % \"2.8.0\" val slf4j = \"1.7.32\" libraryDependencies += \"org.slf4j\" % \"slf4j-api\" % slf4j libraryDependencies += \"org.slf4j\" % \"slf4j-log4j12\" % slf4j","title":"build.sbt"},{"location":"logging/#log4jproperties","text":"In src/main/resources/log4j.properties use the following: log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL","title":"log4j.properties"},{"location":"overview/","text":"Apache Kafka \u00b6 Apache Kafka is an open source project for a distributed publish-subscribe messaging system rethought as a distributed commit log. Kafka stores messages in topics that are partitioned and replicated across multiple brokers in a cluster. Producers send messages to topics from which consumers read. Language Agnostic \u00b6 Kafka clients (producers and consumers) use binary protocol to talk to a Kafka cluster. Messages are byte arrays (with String, JSON, and Avro being the most common formats). If a message has a key, Kafka makes sure that all messages of the same key are in the same partition. Consumers may be grouped in a consumer group with multiple consumers. Each consumer in a consumer group will read messages from a unique subset of partitions in each topic they subscribe to. Each message is delivered to one consumer in the group, and all messages with the same key arrive at the same consumer. Durability \u00b6 Kafka does not track which messages were read by each consumer. Kafka keeps all messages for a finite amount of time, and it is consumers' responsibility to track their location per topic (offsets).","title":"Overview"},{"location":"overview/#apache-kafka","text":"Apache Kafka is an open source project for a distributed publish-subscribe messaging system rethought as a distributed commit log. Kafka stores messages in topics that are partitioned and replicated across multiple brokers in a cluster. Producers send messages to topics from which consumers read.","title":"Apache Kafka"},{"location":"overview/#language-agnostic","text":"Kafka clients (producers and consumers) use binary protocol to talk to a Kafka cluster. Messages are byte arrays (with String, JSON, and Avro being the most common formats). If a message has a key, Kafka makes sure that all messages of the same key are in the same partition. Consumers may be grouped in a consumer group with multiple consumers. Each consumer in a consumer group will read messages from a unique subset of partitions in each topic they subscribe to. Each message is delivered to one consumer in the group, and all messages with the same key arrive at the same consumer.","title":"Language Agnostic"},{"location":"overview/#durability","text":"Kafka does not track which messages were read by each consumer. Kafka keeps all messages for a finite amount of time, and it is consumers' responsibility to track their location per topic (offsets).","title":"Durability"},{"location":"producer/","text":"Kafka Producers \u00b6","title":"Kafka Producers"},{"location":"producer/#kafka-producers","text":"","title":"Kafka Producers"},{"location":"producer/KafkaProducer/","text":"KafkaProducer \u00b6 KafkaProducer<K, V> is a Producer . Creating Instance \u00b6 KafkaProducer takes the following to be created: ProducerConfig Key Serializer<K> Value Serializer<V> ProducerMetadata KafkaClient ProducerInterceptor<K, V> s Time KafkaProducer is created when: FIXME abortTransaction \u00b6 void abortTransaction () abortTransaction prints out the following INFO message to the logs: Aborting incomplete transaction abortTransaction ...FIXME abortTransaction is part of the Producer abstraction. Demo \u00b6 // Necessary imports import org.apache.kafka.clients.producer.KafkaProducer import org.apache.kafka.clients.producer.ProducerConfig import org.apache.kafka.common.serialization.StringSerializer // Creating a KafkaProducer import java.util.Properties val props = new Properties() props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \":9092\") val producer = new KafkaProducer[String, String](props) // Creating a record to be sent import org.apache.kafka.clients.producer.ProducerRecord val r = new ProducerRecord[String, String](\"0\", \"this is a message\") // Sending the record (with no Callback) import java.util.concurrent.Future import org.apache.kafka.clients.producer.RecordMetadata val metadataF: Future[RecordMetadata] = producer.send(r) Logging \u00b6 Enable ALL logging level for org.apache.kafka.clients.producer.KafkaProducer logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL Refer to Logging .","title":"KafkaProducer"},{"location":"producer/KafkaProducer/#kafkaproducer","text":"KafkaProducer<K, V> is a Producer .","title":"KafkaProducer"},{"location":"producer/KafkaProducer/#creating-instance","text":"KafkaProducer takes the following to be created: ProducerConfig Key Serializer<K> Value Serializer<V> ProducerMetadata KafkaClient ProducerInterceptor<K, V> s Time KafkaProducer is created when: FIXME","title":"Creating Instance"},{"location":"producer/KafkaProducer/#aborttransaction","text":"void abortTransaction () abortTransaction prints out the following INFO message to the logs: Aborting incomplete transaction abortTransaction ...FIXME abortTransaction is part of the Producer abstraction.","title":" abortTransaction"},{"location":"producer/KafkaProducer/#demo","text":"// Necessary imports import org.apache.kafka.clients.producer.KafkaProducer import org.apache.kafka.clients.producer.ProducerConfig import org.apache.kafka.common.serialization.StringSerializer // Creating a KafkaProducer import java.util.Properties val props = new Properties() props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \":9092\") val producer = new KafkaProducer[String, String](props) // Creating a record to be sent import org.apache.kafka.clients.producer.ProducerRecord val r = new ProducerRecord[String, String](\"0\", \"this is a message\") // Sending the record (with no Callback) import java.util.concurrent.Future import org.apache.kafka.clients.producer.RecordMetadata val metadataF: Future[RecordMetadata] = producer.send(r)","title":"Demo"},{"location":"producer/KafkaProducer/#logging","text":"Enable ALL logging level for org.apache.kafka.clients.producer.KafkaProducer logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL Refer to Logging .","title":"Logging"},{"location":"producer/Producer/","text":"Producer \u00b6 Producer<K, V> is an interface to KafkaProducer for Kafka developers to use to send messages (with K keys and V values) to a Kafka cluster. Contract (Subset) \u00b6 abortTransaction \u00b6 void abortTransaction () beginTransaction \u00b6 void beginTransaction () Used when: FIXME commitTransaction \u00b6 void commitTransaction () Used when: FIXME initTransactions \u00b6 void initTransactions () Used when: FIXME sendOffsetsToTransaction \u00b6 void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , String consumerGroupId ) void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , ConsumerGroupMetadata groupMetadata ) Used when: FIXME","title":"Producer"},{"location":"producer/Producer/#producer","text":"Producer<K, V> is an interface to KafkaProducer for Kafka developers to use to send messages (with K keys and V values) to a Kafka cluster.","title":"Producer"},{"location":"producer/Producer/#contract-subset","text":"","title":"Contract (Subset)"},{"location":"producer/Producer/#aborttransaction","text":"void abortTransaction ()","title":" abortTransaction"},{"location":"producer/Producer/#begintransaction","text":"void beginTransaction () Used when: FIXME","title":" beginTransaction"},{"location":"producer/Producer/#committransaction","text":"void commitTransaction () Used when: FIXME","title":" commitTransaction"},{"location":"producer/Producer/#inittransactions","text":"void initTransactions () Used when: FIXME","title":" initTransactions"},{"location":"producer/Producer/#sendoffsetstotransaction","text":"void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , String consumerGroupId ) void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , ConsumerGroupMetadata groupMetadata ) Used when: FIXME","title":" sendOffsetsToTransaction"},{"location":"producer/ProducerConfig/","text":"ProducerConfig \u00b6 enable.idempotence \u00b6 Default: false Used when: KafkaProducer is requested to configureTransactionState ProducerConfig is requested to maybeOverrideEnableIdempotence and idempotenceEnabled transactional.id \u00b6 The TransactionalId to use for transactional delivery. This enables reliability semantics which span multiple producer sessions since it allows the client to guarantee that transactions using the same TransactionalId have been completed prior to starting any new transactions. If no TransactionalId is provided, then the producer is limited to idempotent delivery. If a TransactionalId is configured, enable.idempotence is implied. Default: (empty) (transactions cannot be used) Note that, by default, transactions require a cluster of at least three brokers which is the recommended setting for production; for development you can change this, by adjusting broker setting transaction.state.log.replication.factor . transaction.state.log.replication.factor \u00b6 idempotenceEnabled \u00b6 boolean idempotenceEnabled () idempotenceEnabled ...FIXME idempotenceEnabled is used when: KafkaProducer is requested to configureTransactionState , configureInflightRequests , configureAcks ProducerConfig is requested to maybeOverrideAcksAndRetries postProcessParsedConfig \u00b6 Map < String , Object > postProcessParsedConfig ( Map < String , Object > parsedValues ) postProcessParsedConfig maybeOverrideEnableIdempotence . postProcessParsedConfig maybeOverrideClientId . postProcessParsedConfig maybeOverrideAcksAndRetries . postProcessParsedConfig is part of the AbstractConfig abstraction. maybeOverrideClientId \u00b6 maybeOverrideAcksAndRetries uses transactional.id (if defined) or the next available ID for an ID with producer- prefix for client.id unless already defined. maybeOverrideAcksAndRetries \u00b6 void maybeOverrideAcksAndRetries ( Map < String , Object > configs ) maybeOverrideAcksAndRetries ...FIXME maybeOverrideEnableIdempotence \u00b6 void maybeOverrideEnableIdempotence ( Map < String , Object > configs ) maybeOverrideEnableIdempotence ...FIXME","title":"ProducerConfig"},{"location":"producer/ProducerConfig/#producerconfig","text":"","title":"ProducerConfig"},{"location":"producer/ProducerConfig/#enableidempotence","text":"Default: false Used when: KafkaProducer is requested to configureTransactionState ProducerConfig is requested to maybeOverrideEnableIdempotence and idempotenceEnabled","title":" enable.idempotence"},{"location":"producer/ProducerConfig/#transactionalid","text":"The TransactionalId to use for transactional delivery. This enables reliability semantics which span multiple producer sessions since it allows the client to guarantee that transactions using the same TransactionalId have been completed prior to starting any new transactions. If no TransactionalId is provided, then the producer is limited to idempotent delivery. If a TransactionalId is configured, enable.idempotence is implied. Default: (empty) (transactions cannot be used) Note that, by default, transactions require a cluster of at least three brokers which is the recommended setting for production; for development you can change this, by adjusting broker setting transaction.state.log.replication.factor .","title":" transactional.id"},{"location":"producer/ProducerConfig/#transactionstatelogreplicationfactor","text":"","title":" transaction.state.log.replication.factor"},{"location":"producer/ProducerConfig/#idempotenceenabled","text":"boolean idempotenceEnabled () idempotenceEnabled ...FIXME idempotenceEnabled is used when: KafkaProducer is requested to configureTransactionState , configureInflightRequests , configureAcks ProducerConfig is requested to maybeOverrideAcksAndRetries","title":" idempotenceEnabled"},{"location":"producer/ProducerConfig/#postprocessparsedconfig","text":"Map < String , Object > postProcessParsedConfig ( Map < String , Object > parsedValues ) postProcessParsedConfig maybeOverrideEnableIdempotence . postProcessParsedConfig maybeOverrideClientId . postProcessParsedConfig maybeOverrideAcksAndRetries . postProcessParsedConfig is part of the AbstractConfig abstraction.","title":" postProcessParsedConfig"},{"location":"producer/ProducerConfig/#maybeoverrideclientid","text":"maybeOverrideAcksAndRetries uses transactional.id (if defined) or the next available ID for an ID with producer- prefix for client.id unless already defined.","title":" maybeOverrideClientId"},{"location":"producer/ProducerConfig/#maybeoverrideacksandretries","text":"void maybeOverrideAcksAndRetries ( Map < String , Object > configs ) maybeOverrideAcksAndRetries ...FIXME","title":" maybeOverrideAcksAndRetries"},{"location":"producer/ProducerConfig/#maybeoverrideenableidempotence","text":"void maybeOverrideEnableIdempotence ( Map < String , Object > configs ) maybeOverrideEnableIdempotence ...FIXME","title":" maybeOverrideEnableIdempotence"}]}