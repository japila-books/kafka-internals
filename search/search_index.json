{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Apache Kafka 2.8.0 \u00b6 Welcome to The Internals of Apache Kafka online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Kafka as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Apache Kafka \ud83d\udd25 Last update: 2021-09-15","title":"Home"},{"location":"#the-internals-of-apache-kafka-280","text":"Welcome to The Internals of Apache Kafka online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Kafka as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Apache Kafka \ud83d\udd25 Last update: 2021-09-15","title":"The Internals of Apache Kafka 2.8.0"},{"location":"AbstractConfig/","text":"AbstractConfig \u00b6 AbstractConfig is...FIXME","title":"AbstractConfig"},{"location":"AbstractConfig/#abstractconfig","text":"AbstractConfig is...FIXME","title":"AbstractConfig"},{"location":"CommonClientConfigs/","text":"CommonClientConfigs \u00b6 client.id \u00b6 retries \u00b6","title":"CommonClientConfigs"},{"location":"CommonClientConfigs/#commonclientconfigs","text":"","title":"CommonClientConfigs"},{"location":"CommonClientConfigs/#clientid","text":"","title":" client.id"},{"location":"CommonClientConfigs/#retries","text":"","title":" retries"},{"location":"building-from-sources/","text":"Building from Sources \u00b6 Based on README.md : KAFKA_VERSION=2.8.0 ./gradlew clean releaseTarGz install && \\ tar -zxvf core/build/distributions/kafka_2.13-$KAFKA_VERSION.tgz) cd kafka_2.13-$KAFKA_VERSION $ ./bin/kafka-server-start.sh --version [2021-09-12 19:00:12,467] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$) 2.8.0 (Commit:ebb1d6e21cc92130)","title":"Building from Sources"},{"location":"building-from-sources/#building-from-sources","text":"Based on README.md : KAFKA_VERSION=2.8.0 ./gradlew clean releaseTarGz install && \\ tar -zxvf core/build/distributions/kafka_2.13-$KAFKA_VERSION.tgz) cd kafka_2.13-$KAFKA_VERSION $ ./bin/kafka-server-start.sh --version [2021-09-12 19:00:12,467] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$) 2.8.0 (Commit:ebb1d6e21cc92130)","title":"Building from Sources"},{"location":"logging/","text":"Logging \u00b6 Brokers \u00b6 Kafka brokers use Apache Log4j 2 for logging and use config/log4j.properties by default. The default logging level is INFO with stdout appender. log4j.rootLogger=INFO, stdout, kafkaAppender log4j.logger.kafka=INFO log4j.logger.org.apache.kafka=INFO Clients \u00b6 build.sbt \u00b6 libraryDependencies += \"org.apache.kafka\" % \"kafka-clients\" % \"2.8.0\" val slf4j = \"1.7.32\" libraryDependencies += \"org.slf4j\" % \"slf4j-api\" % slf4j libraryDependencies += \"org.slf4j\" % \"slf4j-log4j12\" % slf4j log4j.properties \u00b6 In src/main/resources/log4j.properties use the following: log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL","title":"Logging"},{"location":"logging/#logging","text":"","title":"Logging"},{"location":"logging/#brokers","text":"Kafka brokers use Apache Log4j 2 for logging and use config/log4j.properties by default. The default logging level is INFO with stdout appender. log4j.rootLogger=INFO, stdout, kafkaAppender log4j.logger.kafka=INFO log4j.logger.org.apache.kafka=INFO","title":"Brokers"},{"location":"logging/#clients","text":"","title":"Clients"},{"location":"logging/#buildsbt","text":"libraryDependencies += \"org.apache.kafka\" % \"kafka-clients\" % \"2.8.0\" val slf4j = \"1.7.32\" libraryDependencies += \"org.slf4j\" % \"slf4j-api\" % slf4j libraryDependencies += \"org.slf4j\" % \"slf4j-log4j12\" % slf4j","title":"build.sbt"},{"location":"logging/#log4jproperties","text":"In src/main/resources/log4j.properties use the following: log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL","title":"log4j.properties"},{"location":"overview/","text":"Apache Kafka \u00b6 Apache Kafka is an open source project for a distributed publish-subscribe messaging system rethought as a distributed commit log. Kafka stores messages in topics that are partitioned and replicated across multiple brokers in a cluster. Producers send messages to topics from which consumers read. Language Agnostic \u00b6 Kafka clients (producers and consumers) use binary protocol to talk to a Kafka cluster. Messages are byte arrays (with String, JSON, and Avro being the most common formats). If a message has a key, Kafka makes sure that all messages of the same key are in the same partition. Consumers may be grouped in a consumer group with multiple consumers. Each consumer in a consumer group will read messages from a unique subset of partitions in each topic they subscribe to. Each message is delivered to one consumer in the group, and all messages with the same key arrive at the same consumer. Durability \u00b6 Kafka does not track which messages were read by each consumer. Kafka keeps all messages for a finite amount of time, and it is consumers' responsibility to track their location per topic (offsets).","title":"Overview"},{"location":"overview/#apache-kafka","text":"Apache Kafka is an open source project for a distributed publish-subscribe messaging system rethought as a distributed commit log. Kafka stores messages in topics that are partitioned and replicated across multiple brokers in a cluster. Producers send messages to topics from which consumers read.","title":"Apache Kafka"},{"location":"overview/#language-agnostic","text":"Kafka clients (producers and consumers) use binary protocol to talk to a Kafka cluster. Messages are byte arrays (with String, JSON, and Avro being the most common formats). If a message has a key, Kafka makes sure that all messages of the same key are in the same partition. Consumers may be grouped in a consumer group with multiple consumers. Each consumer in a consumer group will read messages from a unique subset of partitions in each topic they subscribe to. Each message is delivered to one consumer in the group, and all messages with the same key arrive at the same consumer.","title":"Language Agnostic"},{"location":"overview/#durability","text":"Kafka does not track which messages were read by each consumer. Kafka keeps all messages for a finite amount of time, and it is consumers' responsibility to track their location per topic (offsets).","title":"Durability"},{"location":"producer/","text":"Kafka Producers \u00b6","title":"Kafka Producers"},{"location":"producer/#kafka-producers","text":"","title":"Kafka Producers"},{"location":"producer/BufferPool/","text":"BufferPool \u00b6 BufferPool is...FIXME","title":"BufferPool"},{"location":"producer/BufferPool/#bufferpool","text":"BufferPool is...FIXME","title":"BufferPool"},{"location":"producer/KafkaProducer/","text":"KafkaProducer \u00b6 KafkaProducer<K, V> is a concrete Producer . Creating Instance \u00b6 KafkaProducer takes the following to be created: ProducerConfig Key Serializer<K> Value Serializer<V> ProducerMetadata KafkaClient ProducerInterceptor<K, V> s Time configureTransactionState \u00b6 TransactionManager configureTransactionState ( ProducerConfig config , LogContext logContext ) configureTransactionState ...FIXME newSender \u00b6 Sender newSender ( LogContext logContext , KafkaClient kafkaClient , ProducerMetadata metadata ) newSender ...FIXME configureInflightRequests \u00b6 int configureInflightRequests ( ProducerConfig config ) configureInflightRequests gives the value of the max.in.flight.requests.per.connection (in the given ProducerConfig ). configureInflightRequests throws a ConfigException when the idempotence is enabled and the value of the max.in.flight.requests.per.connection is above 5: Must set max.in.flight.requests.per.connection to at most 5 to use the idempotent producer. configureAcks \u00b6 short configureAcks ( ProducerConfig config , Logger log ) configureAcks ...FIXME RecordAccumulator \u00b6 KafkaProducer creates a RecordAccumulator when created . This RecordAccumulator is used for the following: Create a Sender append when doSend beginFlush when flush Aborting Incomplete Transaction \u00b6 void abortTransaction () abortTransaction prints out the following INFO message to the logs: Aborting incomplete transaction abortTransaction ...FIXME abortTransaction is part of the Producer abstraction. Sending Record \u00b6 Future < RecordMetadata > send ( ProducerRecord < K , V > record , Callback callback ) send ...FIXME send is part of the Producer abstraction. doSend \u00b6 Future < RecordMetadata > doSend ( ProducerRecord < K , V > record , Callback callback ) doSend ...FIXME partition \u00b6 int partition ( ProducerRecord < K , V > record , byte [] serializedKey , byte [] serializedValue , Cluster cluster ) partition is the partition (of the given ProducerRecord ) if defined or requests the Partitioner for the partition . Flushing \u00b6 void flush () flush requests the RecordAccumulator to beginFlush . flush requests the Sender to wakeup . flush requests the RecordAccumulator to awaitFlushCompletion . flush is part of the Producer abstraction. Demo \u00b6 // Necessary imports import org.apache.kafka.clients.producer.KafkaProducer import org.apache.kafka.clients.producer.ProducerConfig import org.apache.kafka.common.serialization.StringSerializer // Creating a KafkaProducer import java.util.Properties val props = new Properties() props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \":9092\") val producer = new KafkaProducer[String, String](props) // Creating a record to be sent import org.apache.kafka.clients.producer.ProducerRecord val r = new ProducerRecord[String, String](\"0\", \"this is a message\") // Sending the record (with no Callback) import java.util.concurrent.Future import org.apache.kafka.clients.producer.RecordMetadata val metadataF: Future[RecordMetadata] = producer.send(r) Logging \u00b6 Enable ALL logging level for org.apache.kafka.clients.producer.KafkaProducer logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL Refer to Logging .","title":"KafkaProducer"},{"location":"producer/KafkaProducer/#kafkaproducer","text":"KafkaProducer<K, V> is a concrete Producer .","title":"KafkaProducer"},{"location":"producer/KafkaProducer/#creating-instance","text":"KafkaProducer takes the following to be created: ProducerConfig Key Serializer<K> Value Serializer<V> ProducerMetadata KafkaClient ProducerInterceptor<K, V> s Time","title":"Creating Instance"},{"location":"producer/KafkaProducer/#configuretransactionstate","text":"TransactionManager configureTransactionState ( ProducerConfig config , LogContext logContext ) configureTransactionState ...FIXME","title":" configureTransactionState"},{"location":"producer/KafkaProducer/#newsender","text":"Sender newSender ( LogContext logContext , KafkaClient kafkaClient , ProducerMetadata metadata ) newSender ...FIXME","title":" newSender"},{"location":"producer/KafkaProducer/#configureinflightrequests","text":"int configureInflightRequests ( ProducerConfig config ) configureInflightRequests gives the value of the max.in.flight.requests.per.connection (in the given ProducerConfig ). configureInflightRequests throws a ConfigException when the idempotence is enabled and the value of the max.in.flight.requests.per.connection is above 5: Must set max.in.flight.requests.per.connection to at most 5 to use the idempotent producer.","title":" configureInflightRequests"},{"location":"producer/KafkaProducer/#configureacks","text":"short configureAcks ( ProducerConfig config , Logger log ) configureAcks ...FIXME","title":" configureAcks"},{"location":"producer/KafkaProducer/#recordaccumulator","text":"KafkaProducer creates a RecordAccumulator when created . This RecordAccumulator is used for the following: Create a Sender append when doSend beginFlush when flush","title":" RecordAccumulator"},{"location":"producer/KafkaProducer/#aborting-incomplete-transaction","text":"void abortTransaction () abortTransaction prints out the following INFO message to the logs: Aborting incomplete transaction abortTransaction ...FIXME abortTransaction is part of the Producer abstraction.","title":" Aborting Incomplete Transaction"},{"location":"producer/KafkaProducer/#sending-record","text":"Future < RecordMetadata > send ( ProducerRecord < K , V > record , Callback callback ) send ...FIXME send is part of the Producer abstraction.","title":" Sending Record"},{"location":"producer/KafkaProducer/#dosend","text":"Future < RecordMetadata > doSend ( ProducerRecord < K , V > record , Callback callback ) doSend ...FIXME","title":" doSend"},{"location":"producer/KafkaProducer/#partition","text":"int partition ( ProducerRecord < K , V > record , byte [] serializedKey , byte [] serializedValue , Cluster cluster ) partition is the partition (of the given ProducerRecord ) if defined or requests the Partitioner for the partition .","title":" partition"},{"location":"producer/KafkaProducer/#flushing","text":"void flush () flush requests the RecordAccumulator to beginFlush . flush requests the Sender to wakeup . flush requests the RecordAccumulator to awaitFlushCompletion . flush is part of the Producer abstraction.","title":" Flushing"},{"location":"producer/KafkaProducer/#demo","text":"// Necessary imports import org.apache.kafka.clients.producer.KafkaProducer import org.apache.kafka.clients.producer.ProducerConfig import org.apache.kafka.common.serialization.StringSerializer // Creating a KafkaProducer import java.util.Properties val props = new Properties() props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName) props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \":9092\") val producer = new KafkaProducer[String, String](props) // Creating a record to be sent import org.apache.kafka.clients.producer.ProducerRecord val r = new ProducerRecord[String, String](\"0\", \"this is a message\") // Sending the record (with no Callback) import java.util.concurrent.Future import org.apache.kafka.clients.producer.RecordMetadata val metadataF: Future[RecordMetadata] = producer.send(r)","title":"Demo"},{"location":"producer/KafkaProducer/#logging","text":"Enable ALL logging level for org.apache.kafka.clients.producer.KafkaProducer logger to see what happens inside. Add the following line to log4j.properties : log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=ALL Refer to Logging .","title":"Logging"},{"location":"producer/Partitioner/","text":"Partitioner \u00b6 Partitioner is...FIXME","title":"Partitioner"},{"location":"producer/Partitioner/#partitioner","text":"Partitioner is...FIXME","title":"Partitioner"},{"location":"producer/Producer/","text":"Producer \u00b6 Producer<K, V> is an interface to KafkaProducer for Kafka developers to use to send messages (with K keys and V values) to a Kafka cluster. Contract (Subset) \u00b6 abortTransaction \u00b6 void abortTransaction () beginTransaction \u00b6 void beginTransaction () Used when: FIXME commitTransaction \u00b6 void commitTransaction () Used when: FIXME initTransactions \u00b6 void initTransactions () Used when: FIXME sendOffsetsToTransaction \u00b6 void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , String consumerGroupId ) void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , ConsumerGroupMetadata groupMetadata ) Used when: FIXME","title":"Producer"},{"location":"producer/Producer/#producer","text":"Producer<K, V> is an interface to KafkaProducer for Kafka developers to use to send messages (with K keys and V values) to a Kafka cluster.","title":"Producer"},{"location":"producer/Producer/#contract-subset","text":"","title":"Contract (Subset)"},{"location":"producer/Producer/#aborttransaction","text":"void abortTransaction ()","title":" abortTransaction"},{"location":"producer/Producer/#begintransaction","text":"void beginTransaction () Used when: FIXME","title":" beginTransaction"},{"location":"producer/Producer/#committransaction","text":"void commitTransaction () Used when: FIXME","title":" commitTransaction"},{"location":"producer/Producer/#inittransactions","text":"void initTransactions () Used when: FIXME","title":" initTransactions"},{"location":"producer/Producer/#sendoffsetstotransaction","text":"void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , String consumerGroupId ) void sendOffsetsToTransaction ( Map < TopicPartition , OffsetAndMetadata > offsets , ConsumerGroupMetadata groupMetadata ) Used when: FIXME","title":" sendOffsetsToTransaction"},{"location":"producer/ProducerBatch/","text":"ProducerBatch \u00b6 Creating Instance \u00b6 ProducerBatch takes the following to be created: TopicPartition MemoryRecordsBuilder createdMs isSplitBatch flag (default: false ) ProducerBatch is created when: ProducerBatch is requested to createBatchOffAccumulatorForRecord RecordAccumulator is requested to append a record tryAppend \u00b6 FutureRecordMetadata tryAppend ( long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long now ) tryAppend ...FIXME tryAppend is used when: RecordAccumulator is requested to append a record","title":"ProducerBatch"},{"location":"producer/ProducerBatch/#producerbatch","text":"","title":"ProducerBatch"},{"location":"producer/ProducerBatch/#creating-instance","text":"ProducerBatch takes the following to be created: TopicPartition MemoryRecordsBuilder createdMs isSplitBatch flag (default: false ) ProducerBatch is created when: ProducerBatch is requested to createBatchOffAccumulatorForRecord RecordAccumulator is requested to append a record","title":"Creating Instance"},{"location":"producer/ProducerBatch/#tryappend","text":"FutureRecordMetadata tryAppend ( long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long now ) tryAppend ...FIXME tryAppend is used when: RecordAccumulator is requested to append a record","title":" tryAppend"},{"location":"producer/ProducerConfig/","text":"ProducerConfig \u00b6 batch.size \u00b6 The buffer size allocated for a partition. When records are received (which are smaller than this size) KafkaProducer will attempt to optimistically group them together until this size is reached. Default: 16384 Must be at least 0 Related to: linger.ms max-partition-memory-bytes ( ConsoleProducer ) Used when: KafkaProducer is created (to create a RecordAccumulator and an accompanying BufferPool ) KafkaLog4jAppender is requested to activateOptions enable.idempotence \u00b6 Default: false Used when: KafkaProducer is requested to configureTransactionState ProducerConfig is requested to maybeOverrideEnableIdempotence and idempotenceEnabled linger.ms \u00b6 max.in.flight.requests.per.connection \u00b6 The maximum number of unacknowledged requests the client will send on a single connection before blocking. Note that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled). Default: 5 Must be at least 1 Related to: enable.idempotence retries Used when: KafkaProducer is requested to configureInflightRequests retries \u00b6 transactional.id \u00b6 The TransactionalId to use for transactional delivery. This enables reliability semantics which span multiple producer sessions since it allows the client to guarantee that transactions using the same TransactionalId have been completed prior to starting any new transactions. If no TransactionalId is provided, then the producer is limited to idempotent delivery. If a TransactionalId is configured, enable.idempotence is implied. Default: (empty) (transactions cannot be used) Note that, by default, transactions require a cluster of at least three brokers which is the recommended setting for production; for development you can change this, by adjusting broker setting transaction.state.log.replication.factor . transaction.state.log.replication.factor \u00b6 idempotenceEnabled \u00b6 boolean idempotenceEnabled () idempotenceEnabled ...FIXME idempotenceEnabled is used when: KafkaProducer is created (and requested to configureTransactionState , configureInflightRequests , configureAcks ) ProducerConfig is requested to maybeOverrideAcksAndRetries postProcessParsedConfig \u00b6 Map < String , Object > postProcessParsedConfig ( Map < String , Object > parsedValues ) postProcessParsedConfig maybeOverrideEnableIdempotence . postProcessParsedConfig maybeOverrideClientId . postProcessParsedConfig maybeOverrideAcksAndRetries . postProcessParsedConfig is part of the AbstractConfig abstraction. maybeOverrideClientId \u00b6 maybeOverrideAcksAndRetries uses transactional.id (if defined) or the next available ID for an ID with producer- prefix for client.id unless already defined. maybeOverrideAcksAndRetries \u00b6 void maybeOverrideAcksAndRetries ( Map < String , Object > configs ) maybeOverrideAcksAndRetries ...FIXME maybeOverrideEnableIdempotence \u00b6 void maybeOverrideEnableIdempotence ( Map < String , Object > configs ) maybeOverrideEnableIdempotence ...FIXME","title":"ProducerConfig"},{"location":"producer/ProducerConfig/#producerconfig","text":"","title":"ProducerConfig"},{"location":"producer/ProducerConfig/#batchsize","text":"The buffer size allocated for a partition. When records are received (which are smaller than this size) KafkaProducer will attempt to optimistically group them together until this size is reached. Default: 16384 Must be at least 0 Related to: linger.ms max-partition-memory-bytes ( ConsoleProducer ) Used when: KafkaProducer is created (to create a RecordAccumulator and an accompanying BufferPool ) KafkaLog4jAppender is requested to activateOptions","title":" batch.size"},{"location":"producer/ProducerConfig/#enableidempotence","text":"Default: false Used when: KafkaProducer is requested to configureTransactionState ProducerConfig is requested to maybeOverrideEnableIdempotence and idempotenceEnabled","title":" enable.idempotence"},{"location":"producer/ProducerConfig/#lingerms","text":"","title":" linger.ms"},{"location":"producer/ProducerConfig/#maxinflightrequestsperconnection","text":"The maximum number of unacknowledged requests the client will send on a single connection before blocking. Note that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled). Default: 5 Must be at least 1 Related to: enable.idempotence retries Used when: KafkaProducer is requested to configureInflightRequests","title":" max.in.flight.requests.per.connection"},{"location":"producer/ProducerConfig/#retries","text":"","title":" retries"},{"location":"producer/ProducerConfig/#transactionalid","text":"The TransactionalId to use for transactional delivery. This enables reliability semantics which span multiple producer sessions since it allows the client to guarantee that transactions using the same TransactionalId have been completed prior to starting any new transactions. If no TransactionalId is provided, then the producer is limited to idempotent delivery. If a TransactionalId is configured, enable.idempotence is implied. Default: (empty) (transactions cannot be used) Note that, by default, transactions require a cluster of at least three brokers which is the recommended setting for production; for development you can change this, by adjusting broker setting transaction.state.log.replication.factor .","title":" transactional.id"},{"location":"producer/ProducerConfig/#transactionstatelogreplicationfactor","text":"","title":" transaction.state.log.replication.factor"},{"location":"producer/ProducerConfig/#idempotenceenabled","text":"boolean idempotenceEnabled () idempotenceEnabled ...FIXME idempotenceEnabled is used when: KafkaProducer is created (and requested to configureTransactionState , configureInflightRequests , configureAcks ) ProducerConfig is requested to maybeOverrideAcksAndRetries","title":" idempotenceEnabled"},{"location":"producer/ProducerConfig/#postprocessparsedconfig","text":"Map < String , Object > postProcessParsedConfig ( Map < String , Object > parsedValues ) postProcessParsedConfig maybeOverrideEnableIdempotence . postProcessParsedConfig maybeOverrideClientId . postProcessParsedConfig maybeOverrideAcksAndRetries . postProcessParsedConfig is part of the AbstractConfig abstraction.","title":" postProcessParsedConfig"},{"location":"producer/ProducerConfig/#maybeoverrideclientid","text":"maybeOverrideAcksAndRetries uses transactional.id (if defined) or the next available ID for an ID with producer- prefix for client.id unless already defined.","title":" maybeOverrideClientId"},{"location":"producer/ProducerConfig/#maybeoverrideacksandretries","text":"void maybeOverrideAcksAndRetries ( Map < String , Object > configs ) maybeOverrideAcksAndRetries ...FIXME","title":" maybeOverrideAcksAndRetries"},{"location":"producer/ProducerConfig/#maybeoverrideenableidempotence","text":"void maybeOverrideEnableIdempotence ( Map < String , Object > configs ) maybeOverrideEnableIdempotence ...FIXME","title":" maybeOverrideEnableIdempotence"},{"location":"producer/RecordAccumulator/","text":"RecordAccumulator \u00b6 Creating Instance \u00b6 RecordAccumulator takes the following to be created: LogContext Batch size CompressionType Linger retryBackoffMs deliveryTimeoutMs Metrics Name of the Metrics Group Time ApiVersions TransactionManager BufferPool RecordAccumulator is created for KafkaProducer . appendsInProgress Counter \u00b6 RecordAccumulator creates an AtomicInteger ( Java ) for appendsInProgress internal counter when created . appendsInProgress simply marks a single execution of append (and is incremented at the beginning and decremented right at the end). appendsInProgress is used when flushInProgress . flushInProgress \u00b6 boolean appendsInProgress () appendsInProgress indicates if the appendsInProgress counter is above 0 . appendsInProgress is used when abortIncompleteBatches . flushesInProgress Counter \u00b6 RecordAccumulator creates an AtomicInteger ( Java ) for flushesInProgress internal counter when created . flushesInProgress is incremented when beginFlush and decremented when awaitFlushCompletion . flushesInProgress is used when flushInProgress . flushInProgress \u00b6 boolean flushInProgress () flushInProgress indicates if the flushesInProgress counter is above 0 . flushInProgress is used when: RecordAccumulator is requested to ready Sender is requested to maybeSendAndPollTransactionalRequest Appending Record \u00b6 RecordAppendResult append ( TopicPartition tp , long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long maxTimeToBlock , boolean abortOnNewBatch , long nowMs ) append ...FIXME append is used when: KafkaProducer is requested to send a record (and doSend ) tryAppend \u00b6 RecordAppendResult tryAppend ( long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , Deque < ProducerBatch > deque , long nowMs ) tryAppend ...FIXME ready \u00b6 ReadyCheckResult ready ( Cluster cluster , long nowMs ) ready is a list of partitions with data ready to send. ready ...FIXME ready is used when: Sender is requested to sendProducerData beginFlush \u00b6 void beginFlush () beginFlush atomically increments the flushesInProgress counter. beginFlush is used when: KafkaProducer is requested to flush Sender is requested to maybeSendAndPollTransactionalRequest awaitFlushCompletion \u00b6 void awaitFlushCompletion () awaitFlushCompletion ...FIXME awaitFlushCompletion is used when: KafkaProducer is requested to flush splitAndReenqueue \u00b6 int splitAndReenqueue ( ProducerBatch bigBatch ) splitAndReenqueue ...FIXME splitAndReenqueue is used when: Sender is requested to completeBatch deallocate \u00b6 void deallocate ( ProducerBatch batch ) deallocate ...FIXME deallocate is used when: RecordAccumulator is requested to abortBatches and abortUndrainedBatches Sender is requested to maybeRemoveAndDeallocateBatch abortBatches \u00b6 void abortBatches () // (1) void abortBatches ( RuntimeException reason ) Uses a KafkaException abortBatches ...FIXME abortBatches is used when: RecordAccumulator is requested to abortIncompleteBatches Sender is requested to maybeAbortBatches abortIncompleteBatches \u00b6 void abortIncompleteBatches () abortIncompleteBatches abortBatches as long as there are appendsInProgress . abortIncompleteBatches abortBatches one last time (after no thread was appending in case there was a new batch appended by the last appending thread). In the end, abortIncompleteBatches clears the batches registry. abortIncompleteBatches is used when: Sender is requested to run (and forceClose ) abortUndrainedBatches \u00b6 void abortUndrainedBatches ( RuntimeException reason ) abortUndrainedBatches ...FIXME abortUndrainedBatches is used when: Sender is requested to maybeSendAndPollTransactionalRequest Incomplete (Pending) Batches \u00b6 RecordAccumulator creates an IncompleteBatches for incomplete internal registry of pending batches when created . RecordAccumulator uses the IncompleteBatches when: append (to add a new ProducerBatch ) splitAndReenqueue (to add a new ProducerBatch ) deallocate (to remove a ProducerBatch ) awaitFlushCompletion , abortBatches and abortUndrainedBatches (to copyAll ) hasIncomplete \u00b6 boolean hasIncomplete () hasIncomplete is true when the incomplete registry is not empty. hasIncomplete is used when: Sender is requested to maybeSendAndPollTransactionalRequest and maybeAbortBatches In-Progress Batches \u00b6 ConcurrentMap < TopicPartition , Deque < ProducerBatch >> batches RecordAccumulator creates a ConcurrentMap ( Java ) for the batches internal registry of in-progress ProducerBatch es (per TopicPartition ). RecordAccumulator adds a new ArrayDeque ( Java ) when getOrCreateDeque . batches is used when: expiredBatches ready hasUndrained getDeque batches abortIncompleteBatches getOrCreateDeque \u00b6 Deque < ProducerBatch > getOrCreateDeque ( TopicPartition tp ) getOrCreateDeque ...FIXME getOrCreateDeque is used when: RecordAccumulator is requested to append , reenqueue , splitAndReenqueue","title":"RecordAccumulator"},{"location":"producer/RecordAccumulator/#recordaccumulator","text":"","title":"RecordAccumulator"},{"location":"producer/RecordAccumulator/#creating-instance","text":"RecordAccumulator takes the following to be created: LogContext Batch size CompressionType Linger retryBackoffMs deliveryTimeoutMs Metrics Name of the Metrics Group Time ApiVersions TransactionManager BufferPool RecordAccumulator is created for KafkaProducer .","title":"Creating Instance"},{"location":"producer/RecordAccumulator/#appendsinprogress-counter","text":"RecordAccumulator creates an AtomicInteger ( Java ) for appendsInProgress internal counter when created . appendsInProgress simply marks a single execution of append (and is incremented at the beginning and decremented right at the end). appendsInProgress is used when flushInProgress .","title":" appendsInProgress Counter"},{"location":"producer/RecordAccumulator/#flushinprogress","text":"boolean appendsInProgress () appendsInProgress indicates if the appendsInProgress counter is above 0 . appendsInProgress is used when abortIncompleteBatches .","title":"flushInProgress"},{"location":"producer/RecordAccumulator/#flushesinprogress-counter","text":"RecordAccumulator creates an AtomicInteger ( Java ) for flushesInProgress internal counter when created . flushesInProgress is incremented when beginFlush and decremented when awaitFlushCompletion . flushesInProgress is used when flushInProgress .","title":" flushesInProgress Counter"},{"location":"producer/RecordAccumulator/#flushinprogress_1","text":"boolean flushInProgress () flushInProgress indicates if the flushesInProgress counter is above 0 . flushInProgress is used when: RecordAccumulator is requested to ready Sender is requested to maybeSendAndPollTransactionalRequest","title":" flushInProgress"},{"location":"producer/RecordAccumulator/#appending-record","text":"RecordAppendResult append ( TopicPartition tp , long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , long maxTimeToBlock , boolean abortOnNewBatch , long nowMs ) append ...FIXME append is used when: KafkaProducer is requested to send a record (and doSend )","title":" Appending Record"},{"location":"producer/RecordAccumulator/#tryappend","text":"RecordAppendResult tryAppend ( long timestamp , byte [] key , byte [] value , Header [] headers , Callback callback , Deque < ProducerBatch > deque , long nowMs ) tryAppend ...FIXME","title":" tryAppend"},{"location":"producer/RecordAccumulator/#ready","text":"ReadyCheckResult ready ( Cluster cluster , long nowMs ) ready is a list of partitions with data ready to send. ready ...FIXME ready is used when: Sender is requested to sendProducerData","title":" ready"},{"location":"producer/RecordAccumulator/#beginflush","text":"void beginFlush () beginFlush atomically increments the flushesInProgress counter. beginFlush is used when: KafkaProducer is requested to flush Sender is requested to maybeSendAndPollTransactionalRequest","title":" beginFlush"},{"location":"producer/RecordAccumulator/#awaitflushcompletion","text":"void awaitFlushCompletion () awaitFlushCompletion ...FIXME awaitFlushCompletion is used when: KafkaProducer is requested to flush","title":" awaitFlushCompletion"},{"location":"producer/RecordAccumulator/#splitandreenqueue","text":"int splitAndReenqueue ( ProducerBatch bigBatch ) splitAndReenqueue ...FIXME splitAndReenqueue is used when: Sender is requested to completeBatch","title":" splitAndReenqueue"},{"location":"producer/RecordAccumulator/#deallocate","text":"void deallocate ( ProducerBatch batch ) deallocate ...FIXME deallocate is used when: RecordAccumulator is requested to abortBatches and abortUndrainedBatches Sender is requested to maybeRemoveAndDeallocateBatch","title":" deallocate"},{"location":"producer/RecordAccumulator/#abortbatches","text":"void abortBatches () // (1) void abortBatches ( RuntimeException reason ) Uses a KafkaException abortBatches ...FIXME abortBatches is used when: RecordAccumulator is requested to abortIncompleteBatches Sender is requested to maybeAbortBatches","title":" abortBatches"},{"location":"producer/RecordAccumulator/#abortincompletebatches","text":"void abortIncompleteBatches () abortIncompleteBatches abortBatches as long as there are appendsInProgress . abortIncompleteBatches abortBatches one last time (after no thread was appending in case there was a new batch appended by the last appending thread). In the end, abortIncompleteBatches clears the batches registry. abortIncompleteBatches is used when: Sender is requested to run (and forceClose )","title":" abortIncompleteBatches"},{"location":"producer/RecordAccumulator/#abortundrainedbatches","text":"void abortUndrainedBatches ( RuntimeException reason ) abortUndrainedBatches ...FIXME abortUndrainedBatches is used when: Sender is requested to maybeSendAndPollTransactionalRequest","title":" abortUndrainedBatches"},{"location":"producer/RecordAccumulator/#incomplete-pending-batches","text":"RecordAccumulator creates an IncompleteBatches for incomplete internal registry of pending batches when created . RecordAccumulator uses the IncompleteBatches when: append (to add a new ProducerBatch ) splitAndReenqueue (to add a new ProducerBatch ) deallocate (to remove a ProducerBatch ) awaitFlushCompletion , abortBatches and abortUndrainedBatches (to copyAll )","title":" Incomplete (Pending) Batches"},{"location":"producer/RecordAccumulator/#hasincomplete","text":"boolean hasIncomplete () hasIncomplete is true when the incomplete registry is not empty. hasIncomplete is used when: Sender is requested to maybeSendAndPollTransactionalRequest and maybeAbortBatches","title":" hasIncomplete"},{"location":"producer/RecordAccumulator/#in-progress-batches","text":"ConcurrentMap < TopicPartition , Deque < ProducerBatch >> batches RecordAccumulator creates a ConcurrentMap ( Java ) for the batches internal registry of in-progress ProducerBatch es (per TopicPartition ). RecordAccumulator adds a new ArrayDeque ( Java ) when getOrCreateDeque . batches is used when: expiredBatches ready hasUndrained getDeque batches abortIncompleteBatches","title":" In-Progress Batches"},{"location":"producer/RecordAccumulator/#getorcreatedeque","text":"Deque < ProducerBatch > getOrCreateDeque ( TopicPartition tp ) getOrCreateDeque ...FIXME getOrCreateDeque is used when: RecordAccumulator is requested to append , reenqueue , splitAndReenqueue","title":" getOrCreateDeque"},{"location":"producer/Sender/","text":"Sender \u00b6 Sender is...FIXME","title":"Sender"},{"location":"producer/Sender/#sender","text":"Sender is...FIXME","title":"Sender"}]}